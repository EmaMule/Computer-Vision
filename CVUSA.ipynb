{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaMule/Computer-Vision/blob/main/CVUSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FRSut8m-Mv"
      },
      "source": [
        "#Import and installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bMqOyt0ONvDV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title Installing dependencies\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install pytorch_lightning\n",
        "!pip install patool\n",
        "!pip install torchvision nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBFZKHodQUoQ"
      },
      "outputs": [],
      "source": [
        "# @title Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import csv\n",
        "import cv2\n",
        "import gdown\n",
        "import patoolib\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler, RandomSampler, BatchSampler\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "# pytorch lighting\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar, RichProgressBar, ModelPruning\n",
        "from pytorch_lightning import loggers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Folders Setup\n",
        "\n",
        "shutil.rmtree('/content/input', ignore_errors = True)\n",
        "os.mkdir('/content/input')\n",
        "\n",
        "shutil.rmtree('/content/output', ignore_errors = True)\n",
        "os.mkdir('/content/output')\n",
        "\n",
        "shutil.rmtree('/content/output/log', ignore_errors = True)\n",
        "os.mkdir('/content/output/log')\n",
        "\n",
        "shutil.rmtree('/content/lightning_logs', ignore_errors = True)\n",
        "os.mkdir('/content/lightning_logs')"
      ],
      "metadata": {
        "id": "XTosPQ5YcmFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Downloading Dataset\n",
        "\n",
        "# our id: 1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd\n",
        "# their id: 17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd'\n",
        "output_file = '/content/input/CVUSA_subset.rar'\n",
        "output_dir = '/content/input/data'\n",
        "\n",
        "gdown.download(url, output_file)\n",
        "patoolib.extract_archive(output_file, outdir = output_dir)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=19fD1WMGTmusYk8E7ygT6nAJTluf3a_oH'\n",
        "output_file = '/content/input/train.csv'\n",
        "gdown.download(url, output_file)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1Rt6waJ6f-kM12Q2A9mgRxAcKfZPdg9IY'\n",
        "output_file = '/content/input/val.csv'\n",
        "gdown.download(url, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "gKcEFA_tcicZ",
        "outputId": "c3594b84-b880-4450-a0b6-d3dc2ba624bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd\n",
            "From (redirected): https://drive.google.com/uc?id=1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd&confirm=t&uuid=07851613-48b5-4ee2-a20f-21b235d5223f\n",
            "To: /content/input/CVUSA_subset.rar\n",
            "100%|██████████| 4.38G/4.38G [01:39<00:00, 43.9MB/s]\n",
            "INFO patool: Extracting /content/input/CVUSA_subset.rar ...\n",
            "INFO:patool:Extracting /content/input/CVUSA_subset.rar ...\n",
            "INFO patool: ... creating output directory `/content/input/data'.\n",
            "INFO:patool:... creating output directory `/content/input/data'.\n",
            "INFO patool: running /usr/bin/unrar x -- /content/input/CVUSA_subset.rar\n",
            "INFO:patool:running /usr/bin/unrar x -- /content/input/CVUSA_subset.rar\n",
            "INFO patool:     with cwd='/content/input/data', input=''\n",
            "INFO:patool:    with cwd='/content/input/data', input=''\n",
            "INFO patool: ... /content/input/CVUSA_subset.rar extracted to `/content/input/data'.\n",
            "INFO:patool:... /content/input/CVUSA_subset.rar extracted to `/content/input/data'.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19fD1WMGTmusYk8E7ygT6nAJTluf3a_oH\n",
            "To: /content/input/train.csv\n",
            "100%|██████████| 931k/931k [00:00<00:00, 114MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Rt6waJ6f-kM12Q2A9mgRxAcKfZPdg9IY\n",
            "To: /content/input/val.csv\n",
            "100%|██████████| 310k/310k [00:00<00:00, 43.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/input/val.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Settings\n",
        "\n",
        "pl.seed_everything(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOvjHHWCdLWy",
        "outputId": "4e8639ad-f135-4f7a-dfc2-fdaab9729ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDSCF0zinL8Y"
      },
      "source": [
        "#Dataset and DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UnwPoYpJd6"
      },
      "source": [
        "We need to also possibly add polar and segmap! not done right now because there is a problem with the csv files. Also no test set, should we use validation or split the training and use the current validation as test?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT1pbNEXQknc"
      },
      "outputs": [],
      "source": [
        "# @title Dataset definition: without using polar transforms (neither segmentation)\n",
        "\n",
        "# Expected dataset structure: the input_dir contains the split cvs files and a\n",
        "# subdirectory named 'data' with the CVUSA dataset\n",
        "\n",
        "class CVUSADataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_dir, split = 'train', polar = False):\n",
        "        self.split = split\n",
        "        self.polar = polar\n",
        "        self.data = self.load_data(input_dir + f'/{split}.csv')\n",
        "\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        data = []\n",
        "        with open(csv_path, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader) #skip header\n",
        "            for row in csv_reader:\n",
        "                grd_path = row[1]\n",
        "                if self.polar: #If we want to use polar\n",
        "                   sat_path = row[3]\n",
        "                   seg_path = row[4]\n",
        "                else:\n",
        "                  sat_path = row[0]\n",
        "                  seg_path = row[2]\n",
        "                data.append({\"grd_path\": grd_path, \"sat_path\": sat_path, \"seg_path\": seg_path})\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dictionary = self.data[index]\n",
        "        grd_path = dictionary['grd_path']\n",
        "        sat_path = dictionary['sat_path']\n",
        "        seg_path = dictionary['seg_path']\n",
        "        return grd_path, sat_path, seg_path\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CVUSA-Dataset-{self.split}: {len(self.data)} samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj0ecN-1QnVq"
      },
      "outputs": [],
      "source": [
        "# @title Data module definition: without using polar transforms (neither segmentation)\n",
        "\n",
        "class CVUSADataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, input_dir, batch_size=8, grd_resize = None, sat_resize = None, seg_resize = None):\n",
        "        # Initialize the CustomDataModule\n",
        "        super(CVUSADataModule, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dir = input_dir\n",
        "        self.data_dir = input_dir + '/data'\n",
        "\n",
        "        self.resize = {'grd': grd_resize, 'sat': sat_resize, 'seg': seg_resize}\n",
        "        self.size = {'grd': None, 'sat': None, 'seg': None}\n",
        "        self.mean = {'grd': [0,0,0], 'sat': [0,0,0], 'seg': [0,0,0]}\n",
        "        self.std = {'grd': [1,1,1], 'sat': [1,1,1], 'seg': [1,1,1]}\n",
        "        self.transform = {'grd': None, 'sat': None, 'seg': None}\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        # Load the entire dataset\n",
        "        self.train_dataset = CVUSADataset(input_dir=self.input_dir, split='train')\n",
        "        print(self.train_dataset)\n",
        "\n",
        "        #self.test_dataset = CVUSADataset(input_dir=self.input_dir, split='test')\n",
        "        #print(self.test_dataset)\n",
        "\n",
        "        self.val_dataset = CVUSADataset(input_dir=self.input_dir, split='val')\n",
        "        print(self.val_dataset)\n",
        "\n",
        "        grd_sample, sat_sample, seg_sample = self.train_dataset[0]\n",
        "        grd_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, grd_sample)))\n",
        "        sat_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, sat_sample)))\n",
        "        seg_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, seg_sample)))\n",
        "\n",
        "        self.size['grd'] = grd_image.size()[1:3]\n",
        "        self.size['sat'] = sat_image.size()[1:3]\n",
        "        self.size['seg'] = seg_image.size()[1:3]\n",
        "\n",
        "        if self.resize['grd']:\n",
        "          self.size['grd'] = v2.Resize((self.resize['grd']))(grd_image).size()[1:3]\n",
        "        if self.resize['sat']:\n",
        "          self.size['sat'] = v2.Resize((self.resize['sat']))(sat_image).size()[1:3]\n",
        "        if self.resize['seg']:\n",
        "          self.size['seg'] = v2.Resize((self.resize['seg']))(seg_image).size()[1:3]\n",
        "\n",
        "        self.transform['grd'] = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.Resize(self.size['grd']),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(self.mean['grd'], self.std['grd'], inplace=False)\n",
        "        ])\n",
        "\n",
        "        self.transform['sat'] = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.Resize(self.size['sat']),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(self.mean['sat'], self.std['sat'], inplace=False)\n",
        "        ])\n",
        "\n",
        "        self.transform['seg'] = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.Resize(self.size['seg']),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(self.mean['seg'], self.std['seg'], inplace=False),\n",
        "        ])\n",
        "\n",
        "\n",
        "    #collate function is useful so we don't overuse RAM, training is a little bit slower tho...\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        grd_path, sat_path, seg_path = zip(*batch)\n",
        "\n",
        "        # Load and transform each image in the batch\n",
        "        grd_ids, grd_images = self.__compute_images(grd_path, 'grd')\n",
        "        sat_ids, sat_images = self.__compute_images(sat_path, 'sat')\n",
        "        seg_ids, seg_images = self.__compute_images(seg_path, 'seg')\n",
        "\n",
        "        grd_samples = {'imgs': grd_images, 'imgs_id': grd_ids}\n",
        "        sat_samples = {'imgs': sat_images, 'imgs_id': sat_ids}\n",
        "        seg_samples = {'imgs': seg_images, 'imgs_id': seg_ids}\n",
        "\n",
        "        return grd_samples, sat_samples, seg_samples\n",
        "\n",
        "\n",
        "    # we could add transformations (first of all normalization of the input!)\n",
        "    def __compute_images(self, paths, img_type):\n",
        "        images = []\n",
        "        ids = []\n",
        "\n",
        "        for img_path in paths:\n",
        "            img = Image.open(os.path.join(self.data_dir, img_path))\n",
        "            img = self.transform[img_type](img)\n",
        "            images.append(img)\n",
        "            ids.append(int(img_path[-11:-4]))\n",
        "\n",
        "        # Stack the image tensors along the batch dimension\n",
        "        images_tensor = torch.stack(images)\n",
        "        ids_tensor = torch.tensor(ids, dtype=int)\n",
        "        return ids_tensor, images_tensor\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=True,num_workers=2)\n",
        "\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=False,num_workers=2)\n",
        "\n",
        "\n",
        "    #def test_dataloader(self):\n",
        "    #    return DataLoader(self.test_dataset,batch_size=self.batch_size, collate_fn=self.collate_fn,shuffle=True,num_workers=2)\n",
        "\n",
        "\n",
        "    def compute_mean_std(self):\n",
        "\n",
        "        sv_mean = np.array([0., 0., 0.])\n",
        "        sv_std = np.array([0., 0., 0.])\n",
        "\n",
        "        sm_mean = np.array([0., 0., 0.])\n",
        "        sm_std = np.array([0., 0., 0.])\n",
        "\n",
        "        for i in self.train_dataset.data:\n",
        "\n",
        "            sv_path = os.path.join(self.data_dir, i['grd_path'])\n",
        "            sv_img = cv2.imread(sv_path)\n",
        "            sv_img = cv2.cvtColor(sv_img, cv2.COLOR_BGR2RGB)\n",
        "            sv_img = sv_img.astype(float) / 255.\n",
        "            sv_mean += np.mean(sv_img[:,:,:], axis = (0,1))\n",
        "\n",
        "            sm_path = os.path.join(self.data_dir, i['sat_path'])\n",
        "            sm_img = cv2.imread(sm_path)\n",
        "            sm_img = cv2.cvtColor(sm_img, cv2.COLOR_BGR2RGB)\n",
        "            sm_img = sm_img.astype(float) / 255.\n",
        "            sm_mean += np.mean(sm_img[:,:,:], axis = (0,1))\n",
        "\n",
        "        sv_mean /= len(self.train_dataset.data)\n",
        "        sm_mean /= len(self.train_dataset.data)\n",
        "\n",
        "        for i in self.train_dataset.data:\n",
        "\n",
        "            sv_path = os.path.join(self.data_dir, i['grd_path'])\n",
        "            sv_img = cv2.imread(sv_path)\n",
        "            sv_img = cv2.cvtColor(sv_img, cv2.COLOR_BGR2RGB)\n",
        "            sv_img = sv_img.astype(float) / 255.\n",
        "            sv_img_size = sv_img.shape[0] * sv_img.shape[1]\n",
        "            sv_std += ((sv_img[:,:,:] - sv_mean)**2).sum(axis = (0,1)) / sv_img_size\n",
        "\n",
        "            sm_path = os.path.join(self.data_dir, i['sat_path'])\n",
        "            sm_img = cv2.imread(sm_path)\n",
        "            sm_img = cv2.cvtColor(sm_img, cv2.COLOR_BGR2RGB)\n",
        "            sm_img = sm_img.astype(float) / 255.\n",
        "            sm_img_size = sm_img.shape[0] * sm_img.shape[1]\n",
        "            sm_std += ((sm_img[:,:,:] - sv_mean)**2).sum(axis = (0,1)) / sm_img_size\n",
        "\n",
        "        sv_std = np.sqrt(sv_std/len(self.train_dataset.data))\n",
        "        sm_std = np.sqrt(sm_std/len(self.train_dataset.data))\n",
        "\n",
        "        result = {'grd_mean': sv_mean, 'grd_std': sv_std,\n",
        "                  'sat_mean': sm_mean, 'sat_std': sm_std,\n",
        "                  'seg_mean': [0,0,0], 'seg_std': [1,1,1] # DOESN'T COMPUTE SEG MEAN AND STD\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def set_mean_std(self, mean_std):\n",
        "\n",
        "        self.mean['grd'] = mean_std['grd_mean']\n",
        "        self.mean['sat'] = mean_std['sat_mean']\n",
        "        self.mean['seg'] = mean_std['seg_mean']\n",
        "\n",
        "        self.std['grd'] = mean_std['grd_std']\n",
        "        self.std['sat'] = mean_std['sat_std']\n",
        "        self.std['seg'] = mean_std['seg_std']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkLiGNItQpOk",
        "outputId": "cb9752f4-d61c-4549-f4bb-f99489a6940c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CVUSA-Dataset-train: 6647 samples\n",
            "CVUSA-Dataset-val: 2215 samples\n"
          ]
        }
      ],
      "source": [
        "# @title Creating dataloaders: without using polar transforms (neither segmentation)\n",
        "input_dir = '/content/input'\n",
        "\n",
        "data_module = CVUSADataModule(\n",
        "    input_dir = input_dir,\n",
        "    batch_size = 64,\n",
        "    grd_resize = 64,\n",
        "    sat_resize = 128,\n",
        "    seg_resize = 128\n",
        ")\n",
        "\n",
        "# mean_std = data_module.compute_mean_std()\n",
        "mean_std = {\n",
        "    'grd_mean': [0.4691, 0.4821, 0.4603], 'grd_std': [0.2202, 0.2191, 0.2583],\n",
        "    'sat_mean': [0.3833 , 0.3964, 0.3434], 'sat_std': [0.2131, 0.2024, 0.2259],\n",
        "    'seg_mean': [0, 0, 0], 'seg_std': [1, 1, 1]\n",
        "}\n",
        "data_module.set_mean_std(mean_std)\n",
        "\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()\n",
        "#test_loader = data_module.test_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VmODtjKnRzH"
      },
      "source": [
        "#Losses and other utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TripletLoss implementation\n",
        "# class TripletLoss(pl.LightningModule):\n",
        "#     def __init__(self, margin=1.0):\n",
        "#         super(TripletLoss, self).__init__()\n",
        "#         self.margin = margin\n",
        "\n",
        "#     def forward(self, image_features1, image_features2, k = None):\n",
        "#         N = len(image_features1)\n",
        "#         if k is None:\n",
        "#           k = N-1\n",
        "#         # Calcolare le distanze Euclidee tra le features\n",
        "#         distances_per_image1 = torch.cdist(image_features1, image_features2, p=2)  # p=2 per distanza Euclidea\n",
        "#         distances_per_image2 = distances_per_image1.T  # Per simmetria\n",
        "#         #in common between the losses:\n",
        "#         mask = torch.eye(distances_per_image1.size(0), dtype=torch.bool, device = device)\n",
        "#         distances_diag = torch.masked_select(distances_per_image1, mask) @ torch.eye(n=N, device = device)\n",
        "\n",
        "#         distances_positive_matrix = distances_diag @ torch.ones(size=(N,k), device = device) #da controllare\n",
        "#         margin_matrix = torch.ones(size = (N,k), device = device) * self.margin\n",
        "\n",
        "\n",
        "#         distances_no_diag = torch.masked_select(distances_per_image1, mask.logical_not()).view(distances_per_image1.size(0), -1) #need to see if is NxN-1\n",
        "#         distances_no_diag, _ = torch.topk(distances_no_diag, k = k, dim = 1, largest = False)\n",
        "#         loss_matrix = torch.relu(distances_positive_matrix - distances_no_diag + margin_matrix)\n",
        "\n",
        "#         loss1 = torch.mean(loss_matrix, dim=(0,1))\n",
        "\n",
        "#         distances_no_diag = torch.masked_select(distances_per_image2, mask.logical_not()).view(distances_per_image2.size(0), -1) #need to see if is NxN-1\n",
        "#         distances_no_diag, _ = torch.topk(distances_no_diag, k = k, dim = 1, largest = False)\n",
        "#         loss_matrix = torch.relu(distances_positive_matrix - distances_no_diag + margin_matrix)\n",
        "\n",
        "#         loss2 = torch.mean(loss_matrix, dim=(0,1))\n",
        "\n",
        "#         return (loss1 + loss2)/2"
      ],
      "metadata": {
        "id": "7lHAEBs7rC2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation TripletLoss more stable\n",
        "\n",
        "# RIVEDI (confronto con triplet torchreid)\n",
        "\n",
        "class TripletLoss(pl.LightningModule):\n",
        "    def __init__(self, loss_weight = 1e-2):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "    def forward(self, image_features1, image_features2):\n",
        "        #image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        #image_features2 = F.normalize(image_features2, dim=-1)\n",
        "        dist_array = 2.0 - 2.0 * torch.matmul(image_features2, image_features1.T)\n",
        "        n = len(image_features1)\n",
        "        pos_dist = torch.diag(dist_array)\n",
        "        pair_n = n * (n - 1.0)\n",
        "        triplet_dist_g2s = pos_dist - dist_array\n",
        "        loss_g2s = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_g2s * self.loss_weight)))/pair_n\n",
        "        triplet_dist_s2g = torch.unsqueeze(pos_dist, 1) - dist_array\n",
        "        loss_s2g = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_s2g * self.loss_weight)))/pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "kuwQsXRGMI0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title InfoNCE implementation\n",
        "class InfoNCE(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_function):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss_function = loss_function #we can use a generic loss function!\n",
        "\n",
        "    def forward(self, image_features1, image_features2, logit_scale):\n",
        "\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "\n",
        "        logits_per_image1 = logit_scale * image_features1 @ image_features2.T #similarity matrix\n",
        "        logits_per_image2 = logits_per_image1.T\n",
        "\n",
        "        labels = torch.arange(len(logits_per_image1), dtype=torch.long, device = device)\n",
        "        loss = (self.loss_function(logits_per_image1, labels) + self.loss_function(logits_per_image2, labels))/2\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ukTOSR3IgPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u6wnToFjtlF"
      },
      "outputs": [],
      "source": [
        "# @title Top-K Rank Accuracy: takes embeddings in input\n",
        "\n",
        "def top_k_rank_accuracy(emb1, emb2, k=1):\n",
        "    if k > len(emb1) :\n",
        "      return 0.0 #might happen at the end of the dataset (batch less then the chosen one)\n",
        "    # Calculate cosine similarity\n",
        "    correct_in_top_k = 0\n",
        "    for index, elem in enumerate(emb1):\n",
        "        cosine_sim = F.cosine_similarity(elem, emb2, dim=1)\n",
        "\n",
        "        # Find the rank K similarity\n",
        "        top_k_similarities, top_k_indices = torch.topk(cosine_sim, k, largest=True)\n",
        "\n",
        "        correct_in_top_k += index in top_k_indices.tolist()\n",
        "\n",
        "    accuracy = correct_in_top_k / len(emb1)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation of Attention operator\n",
        "class Attention(pl.LightningModule):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MvHm7d1ICYme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouQUwE2nWua"
      },
      "source": [
        "# Training Architectures"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dual Model\n",
        "class DualModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_grd, model_sat):\n",
        "        super(DualModel, self).__init__()\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_sat\n",
        "\n",
        "        #self.loss1 = TripletLoss()\n",
        "        self.loss = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.branch1(x1['imgs'])\n",
        "        out2 = self.branch2(x2['imgs'])\n",
        "        return out1, out2\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        streetview, bingmap, _ = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        loss = self.loss(out1, out2, logit_scale = 3.0)\n",
        "        #loss = self.loss1(out1, out2) + self.loss2(out1, out2, logit_scale = 3.0)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "\n",
        "        self.log('train_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        streetview, bingmap, _ = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('val_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        streetview, bingmap, _ = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('test_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Sf7PT3N_ubhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abAMZR3-HJv4"
      },
      "outputs": [],
      "source": [
        "# @title Resnet\n",
        "\n",
        "class ResNet50Branch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, output_dim):\n",
        "        super(ResNet50Branch, self).__init__()\n",
        "        self.resnet50 = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n",
        "        # Modify the last layer for your specific task\n",
        "        self.resnet50.fc = torch.nn.Linear(self.resnet50.fc.in_features, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps = False):\n",
        "\n",
        "        # to print the featuremap we need to return the last conv layer output\n",
        "        if featuremaps:\n",
        "            x = self.resnet50.conv1(x)\n",
        "            x = self.resnet50.bn1(x)\n",
        "            x = self.resnet50.relu(x)\n",
        "            x = self.resnet50.maxpool(x)\n",
        "            x = self.resnet50.layer1(x)\n",
        "            x = self.resnet50.layer2(x)\n",
        "            x = self.resnet50.layer3(x)\n",
        "            x = self.resnet50.layer4(x)\n",
        "            return x\n",
        "\n",
        "        else:\n",
        "            return self.resnet50(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SAIG\n",
        "\n",
        "\n",
        "class ConvBnReluBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(ConvBnReluBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_heads,\n",
        "        qkv_bias=False,\n",
        "        qk_scale=None,\n",
        "        drop=0.,\n",
        "        attn_drop=0.,\n",
        "        dropout=0.,\n",
        "        norm_layer=nn.LayerNorm\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop\n",
        "        )\n",
        "        # check what is droppath\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class SAIGBranch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, img_size, patch_size=16, in_channels=3, embed_dim=768, num_heads = 8, depth = 4, qkv_bias = True, qk_scale = None, drop_rate=0., attn_drop_rate=0., norm_layer=None, flatten=True):\n",
        "        super(SAIGBranch, self).__init__()\n",
        "        #potremmo salvare i parametri, ha qualche senso?\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
        "\n",
        "        if img_size[0] % patch_size != 0 or img_size[1] % patch_size != 0:\n",
        "          print(\"Warning: image size is not divisible for patch size\")\n",
        "\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "        self.conv_bn_relu_blocks = nn.Sequential(\n",
        "            ConvBnReluBlock(in_channels = 3, out_channels = 64, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 64, out_channels = 128, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 128, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 256, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 256, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 512, stride = 2),\n",
        "        )\n",
        "        self.patch_block = nn.Conv2d(in_channels = 512, out_channels = embed_dim, kernel_size=1, stride=1 ,padding=0)\n",
        "        self.attn_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
        "        #self.pos_embed = nn.Parameter(torch.zeros(1, embed_dim, self.num_patches))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        #self.GAP = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        #self.logits = nn.Linear(in_features = embed_dim, out_features = 512)\n",
        "\n",
        "        self.smd = nn.Sequential(\n",
        "            nn.Linear(self.num_patches, self.num_patches*4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.num_patches*4, self.num_patches),\n",
        "            nn.Linear(self.num_patches, 8)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps = False):\n",
        "\n",
        "      # extract patch embeddings\n",
        "      x = self.conv_bn_relu_blocks(x)\n",
        "\n",
        "      if featuremaps:\n",
        "        return x\n",
        "\n",
        "      x = self.patch_block(x)\n",
        "      x = x.flatten(2).transpose(1,2)\n",
        "      #x = self.norm(x) CHECK\n",
        "\n",
        "      # add position embeddings\n",
        "      x = x + self.pos_embed\n",
        "      x = self.pos_drop(x)\n",
        "\n",
        "      # pass through sequence of attention blocks\n",
        "      for blk in self.attn_blocks:\n",
        "          x = blk(x)\n",
        "\n",
        "      x = self.norm(x)\n",
        "      # x = self.GAP(x.transpose(-1, -2)).squeeze(2)\n",
        "      # x = self.logits(x)\n",
        "\n",
        "      # if featuremaps:\n",
        "      #   return x.resize(x.shape[0], self.grid_size[0], self.grid_size[1], 384)\n",
        "\n",
        "      # x: b x 88 x 384\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = self.smd(x)\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = x.flatten(-2, -1)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "QDzQtqQOHzU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define branches\n",
        "grd_model = SAIGBranch(data_module.size['grd'], embed_dim=256, num_heads = 4, depth = 3)\n",
        "sat_model = SAIGBranch(data_module.size['sat'], embed_dim=256, num_heads = 4, depth = 3) #deals directly by itself with resizing"
      ],
      "metadata": {
        "id": "fs24fr2ZwAxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DualModel(grd_model, sat_model)"
      ],
      "metadata": {
        "id": "_FqlWcHr0qMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVD0l4tD4Byc",
        "outputId": "806d4cc6-da15-4200-e746-974ce7819a2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar()],\n",
        "    log_every_n_steps = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322,
          "referenced_widgets": [
            "f1c3653eb8cb4201a992747c62b7b0f6",
            "9951ef0d74c94297b99d44401084d067"
          ]
        },
        "id": "p57rNWjHN1hh",
        "outputId": "78f4d48b-2bcb-4f25-fb87-de1e7576c3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ branch1 │ SAIGBranch │  3.3 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ branch2 │ SAIGBranch │  3.3 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ loss    │ InfoNCE    │      0 │\n",
              "└───┴─────────┴────────────┴────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ branch1 │ SAIGBranch │  3.3 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ branch2 │ SAIGBranch │  3.3 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ loss    │ InfoNCE    │      0 │\n",
              "└───┴─────────┴────────────┴────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 6.6 M                                                                                            \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 6.6 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 26                                                                         \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 6.6 M                                                                                            \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 6.6 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 26                                                                         \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1c3653eb8cb4201a992747c62b7b0f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, \n",
              "attempting graceful shutdown...\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dubja7aKNutk"
      },
      "outputs": [],
      "source": [
        "trainer.test(\n",
        "    dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Functions"
      ],
      "metadata": {
        "id": "gJAKm9G3-Kwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize Heatmap\n",
        "\n",
        "# to visualize the images correctly, they must be de-normalized, so we must give\n",
        "# in input their mean and std\n",
        "\n",
        "GRID_SPACING = 10\n",
        "\n",
        "@torch.no_grad()\n",
        "def visactmap(\n",
        "    model,\n",
        "    data_loader,\n",
        "    save_dir,\n",
        "    use_gpu,\n",
        "    grd_mean=None,\n",
        "    grd_std=None,\n",
        "    sat_mean=None,\n",
        "    sat_std=None\n",
        "):\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    print('Visualizing activation maps')\n",
        "\n",
        "    for batch_idx, data in enumerate(data_loader):\n",
        "\n",
        "        # for now visualize only streetviews\n",
        "        streetview_imgs, streetview_ids = data[0]['imgs'], data[0]['imgs_id']\n",
        "        satmap_imgs, satmap_ids = data[1]['imgs'], data[1]['imgs_id']\n",
        "\n",
        "        if use_gpu:\n",
        "            streetview_imgs = streetview_imgs.cuda()\n",
        "            satmap_imgs = satmap_imgs.cuda()\n",
        "\n",
        "        try:\n",
        "            streetview_outputs = model.branch1(streetview_imgs, featuremaps=True)\n",
        "            satmap_outputs = model.branch2(satmap_imgs, featuremaps=True)\n",
        "        except TypeError:\n",
        "            raise TypeError('model.forward() doesn\\'t have featuremaps field')\n",
        "        if streetview_outputs.dim() != 4 or satmap_outputs.dim() != 4:\n",
        "            raise ValueError('model output is supposed to have 4 dimensions')\n",
        "\n",
        "        # compute activation maps for streetview (try adding square root?)\n",
        "        streetview_outputs = (streetview_outputs**2).sum(1)\n",
        "        b, h, w = streetview_outputs.size()\n",
        "        streetview_outputs = streetview_outputs.view(b, h * w)\n",
        "        streetview_outputs = nn.functional.normalize(streetview_outputs, p=2, dim=1)\n",
        "        streetview_outputs = streetview_outputs.view(b, h, w)\n",
        "\n",
        "        # compute activation maps for satmap\n",
        "        satmap_outputs = (satmap_outputs**2).sum(1)\n",
        "        b, h, w = satmap_outputs.size()\n",
        "        satmap_outputs = satmap_outputs.view(b, h * w)\n",
        "        satmap_outputs = nn.functional.normalize(satmap_outputs, p=2, dim=1)\n",
        "        satmap_outputs = satmap_outputs.view(b, h, w)\n",
        "\n",
        "        if use_gpu:\n",
        "            streetview_imgs, streetview_outputs = streetview_imgs.cpu(), streetview_outputs.cpu()\n",
        "            satmap_imgs, satmap_outputs = satmap_imgs.cpu(), satmap_outputs.cpu()\n",
        "\n",
        "        for j in range(streetview_outputs.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            imname = str(int(streetview_ids[j])).zfill(7)\n",
        "\n",
        "            # RGB image (from the normalized input image)\n",
        "            img = streetview_imgs[j, ...]\n",
        "            for t, m, s in zip(img, grd_mean, grd_std):\n",
        "                t.mul_(s).add_(m).clamp_(0, 1)\n",
        "            img = np.uint8(np.floor(img.numpy() * 255))\n",
        "            img = img.transpose((1, 2, 0))\n",
        "\n",
        "            height, width, _ = img.shape\n",
        "\n",
        "            # activation map (from the output image)\n",
        "            am = streetview_outputs[j, ...].numpy()\n",
        "            am = cv2.resize(am, (width, height))\n",
        "            am = 255 * (am - np.min(am)) / (np.max(am) - np.min(am) + 1e-12)\n",
        "            am = np.uint8(np.floor(am))\n",
        "            am = cv2.applyColorMap(am, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapping between the two images\n",
        "            overlapped = img*0.5 + am*0.5\n",
        "            overlapped[overlapped > 255] = 255\n",
        "            overlapped = overlapped.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            grid = 255 * np.ones((3*height + 2*GRID_SPACING, width, 3), dtype=np.uint8)\n",
        "            grid[:height, :, :] = img[:, :, ::-1]\n",
        "            grid[height + GRID_SPACING:2*height + GRID_SPACING, :, :] = am\n",
        "            grid[2*height + 2*GRID_SPACING:, :, :] = overlapped\n",
        "            cv2.imwrite(os.path.join(save_dir, imname + '_streetview.jpg'), grid)\n",
        "\n",
        "        for j in range(satmap_outputs.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            imname = str(int(satmap_ids[j])).zfill(7)\n",
        "\n",
        "            # RGB image (input image)\n",
        "            img = satmap_imgs[j, ...]\n",
        "            for t, m, s in zip(img, sat_mean, sat_std):\n",
        "                t.mul_(s).add_(m).clamp_(0, 1)\n",
        "            img = np.uint8(np.floor(img.numpy() * 255))\n",
        "            img = img.transpose((1, 2, 0))\n",
        "\n",
        "            height, width, _ = img.shape\n",
        "\n",
        "            # activation map\n",
        "            am = satmap_outputs[j, ...].numpy()\n",
        "            am = cv2.resize(am, (width, height))\n",
        "            am = 255 * (am - np.min(am)) / (np.max(am) - np.min(am) + 1e-12)\n",
        "            am = np.uint8(np.floor(am))\n",
        "            am = cv2.applyColorMap(am, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapped\n",
        "            overlapped = img*0.5 + am*0.5\n",
        "            overlapped[overlapped > 255] = 255\n",
        "            overlapped = overlapped.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            grid = 255 * np.ones((3*height + 2*GRID_SPACING, width, 3), dtype=np.uint8)\n",
        "            grid[:height, :, :] = img[:, :, ::-1]\n",
        "            grid[height + GRID_SPACING:2*height + GRID_SPACING, :, :] = am\n",
        "            grid[2*height + 2*GRID_SPACING:, :, :] = overlapped\n",
        "            cv2.imwrite(os.path.join(save_dir, imname + '_satmap.jpg'), grid)\n",
        "\n",
        "        if (batch_idx+1) % 10 == 0:\n",
        "            print('- done batch {}/{}'.format(batch_idx + 1, len(data_loader)))"
      ],
      "metadata": {
        "id": "36uMaOGy-NFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f -r '/content/output/log/visactmap'\n",
        "!mkdir '/content/output/log/visactmap'\n",
        "\n",
        "visactmap(\n",
        "    model = model,\n",
        "    data_loader = val_loader,\n",
        "    save_dir = '/content/output/log/visactmap',\n",
        "    use_gpu = True,\n",
        "    grd_mean = mean_std['grd_mean'],\n",
        "    grd_std = mean_std['grd_std'],\n",
        "    sat_mean = mean_std['sat_mean'],\n",
        "    sat_std = mean_std['sat_std']\n",
        ")"
      ],
      "metadata": {
        "id": "5dSp6VMiKNAa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ac44bd-f383-40f1-a9a5-5a987062226f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing activation maps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- done batch 10/35\n",
            "- done batch 20/35\n",
            "- done batch 30/35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combination of all features: TripleModel"
      ],
      "metadata": {
        "id": "ZGDT8x2WPpbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TripleModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_grd, model_sat, model_seg):\n",
        "        super(TripleModel, self).__init__()\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_sat\n",
        "        self.branch3 = model_seg\n",
        "\n",
        "        #self.loss1 = TripletLoss()\n",
        "        self.loss = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        out1 = self.branch1(x1['imgs'])\n",
        "        out2 = self.branch2(x2['imgs'])\n",
        "        out3 = self.branch3(x3['imgs'])\n",
        "        out2 = torch.cat((out2, out3), dim=1) #concatenation of features\n",
        "        return out1, out2\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        streetview, bingmap, segmap = batch\n",
        "        out1, out2 = self(streetview, bingmap, segmap)\n",
        "\n",
        "        loss = self.loss(out1, out2, logit_scale = 3.0)\n",
        "        #loss = self.loss1(out1, out2) + self.loss2(out1, out2, logit_scale = 3.0)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "\n",
        "        self.log('train_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        streetview, bingmap, segmap = batch\n",
        "        out1, out2 = self(streetview, bingmap, segmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('val_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        streetview, bingmap, segmap = batch\n",
        "        out1, out2 = self(streetview, bingmap, segmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('test_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "bvFAbriYPx6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grd_model = SAIGBranch(data_module.grd_size, embed_dim=512)\n",
        "sat_model = SAIGBranch(data_module.sat_size, embed_dim=256) #deals directly by itself with resizing\n",
        "seg_model = SAIGBranch(data_module.seg_size, embed_dim=256) #deals directly by itself with resizing\n",
        "print(data_module.grd_size)\n",
        "print(data_module.sat_size)\n",
        "print(data_module.seg_size)"
      ],
      "metadata": {
        "id": "gG7fYg6cSrji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TripleModel(grd_model, sat_model, seg_model)"
      ],
      "metadata": {
        "id": "MOpbzx14QeN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar()],\n",
        "    log_every_n_steps = 3\n",
        ")"
      ],
      "metadata": {
        "id": "iHcC07AUUhtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader\n",
        ")"
      ],
      "metadata": {
        "id": "OWIVZLEGUlJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(\n",
        "    dataloaders = val_loader\n",
        ")"
      ],
      "metadata": {
        "id": "7-d3ympNUoWu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "5VmODtjKnRzH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1c3653eb8cb4201a992747c62b7b0f6": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_9951ef0d74c94297b99d44401084d067",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mEpoch 0/29\u001b[0m \u001b[38;2;98;6;224m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m2/104\u001b[0m \u001b[38;5;245m0:00:07 • 0:00:48\u001b[0m \u001b[38;5;249m2.15it/s\u001b[0m \u001b[37mv_num: 8.000 train_top1_step: 0.016 \u001b[0m\n                                                                               \u001b[37mtrain_loss_step: 4.159              \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/29</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">2/104</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:07 • 0:00:48</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">2.15it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 8.000 train_top1_step: 0.016 </span>\n                                                                               <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_step: 4.159              </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "9951ef0d74c94297b99d44401084d067": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}