{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaMule/Computer-Vision/blob/main/CVUSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FRSut8m-Mv"
      },
      "source": [
        "#Import and installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bMqOyt0ONvDV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title Installing dependencies\n",
        "!pip install tqdm\n",
        "!pip install pytorch_lightning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBFZKHodQUoQ"
      },
      "outputs": [],
      "source": [
        "# @title Importing libraries\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler,SequentialSampler,RandomSampler\n",
        "from torch import nn\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar, RichProgressBar, ModelPruning\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from pytorch_lightning import loggers\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from torchvision import transforms\n",
        "import shutil\n",
        "import csv\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5fKgrevQXPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214d1f3d-68d8-4351-f342-0dbeb2922ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeSyWSOA8YeO"
      },
      "outputs": [],
      "source": [
        "#!gdown --id 17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpGlkKCU8czs"
      },
      "outputs": [],
      "source": [
        "#!unrar x -o+ -v CVUSA_subset.rar | grep -c \"^Extracting\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ESVTpqm1gq"
      },
      "source": [
        "# To be removed (later) REMEMBER: YOU NOW USE POLAR TRANSFORM!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-X6fVs1ZoIH"
      },
      "outputs": [],
      "source": [
        "# # @title Unzipping the .rar dataset: code used in creation of dataset\n",
        "# import os\n",
        "# from rarfile import RarFile\n",
        "\n",
        "# rar_file_path = 'drive/MyDrive/CV Project/CVUSA_subset.rar'\n",
        "# # Open the .rar file\n",
        "# with RarFile(rar_file_path, 'r') as rar:\n",
        "#     # Get the list of files in the archive\n",
        "#     file_list = rar.namelist()\n",
        "\n",
        "#     # Iterate through the files and extract them\n",
        "#     for file_name in tqdm(file_list, desc=\"Extracting\", unit=\"files\"):\n",
        "#         # Extract the file to the current directory\n",
        "#         rar.extract(file_name, path='drive/MyDrive/CV Project/CVUSA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxJX3qwAvTrS"
      },
      "outputs": [],
      "source": [
        "# # @title Code for transforming csv files (originally quite wrong): code used in creation of dataset\n",
        "# import pandas as pd\n",
        "\n",
        "# train_csv = \"drive/MyDrive/CV Project/CVUSA/train-19zl.csv\"\n",
        "# val_csv = \"drive/MyDrive/CV Project/CVUSA/val-19zl.csv\"\n",
        "# train_output_csv = \"drive/MyDrive/CV Project/CVUSA/train.csv\"\n",
        "# val_output_csv = \"drive/MyDrive/CV Project/CVUSA/val.csv\"\n",
        "\n",
        "# def generate_new_csv(csv_path, out_csv_path):\n",
        "#   # Read the CSV file\n",
        "#   df = pd.read_csv(csv_path, header=None)\n",
        "\n",
        "#   # Rename columns\n",
        "#   df.rename(columns={0: 'col1', 1: 'col2', 2: 'col3'}, inplace=True)\n",
        "\n",
        "#   # Change .png to .jpg in col2\n",
        "#   df['col2'] = df['col2'].str.replace('.png', '.jpg').str.replace('input', '')\n",
        "\n",
        "#   # Change 'streetview' to 'segmap' and 'input' to 'output' in col3\n",
        "#   df['col3'] = df['col3'].str.replace('streetview', 'segmap').str.replace('input', 'output')\n",
        "\n",
        "#   # Add new columns\n",
        "#   df['col4'] = 'polarmap/' + df['col1'].str.replace('bingmap', 'normal')\n",
        "#   df['col5'] = 'polarmap/' + df['col3']\n",
        "\n",
        "#   # Add header\n",
        "#   header = [\"bingmap\", \"streetview\", \"segmap\", \"polarmap/input\", \"polarmap/segmap\"]\n",
        "#   df.columns = header\n",
        "\n",
        "#   # Save the modified dataframe to a new CSV file\n",
        "#   df.to_csv(out_csv_path, index=False)\n",
        "\n",
        "# generate_new_csv(train_csv, train_output_csv)\n",
        "# generate_new_csv(val_csv, val_output_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDSCF0zinL8Y"
      },
      "source": [
        "#Dataset and DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek7SUbjQQgF3",
        "outputId": "f4b160e7-c5c5-47d8-e5bb-a232c4dd86b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "113Se08XQh30"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UnwPoYpJd6"
      },
      "source": [
        "We need to also possibly add polar and segmap! not done right now because there is a problem with the csv files. Also no test set, should we use validation or split the training and use the current validation as test?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT1pbNEXQknc"
      },
      "outputs": [],
      "source": [
        "# @title Dataset definition: without using polar transforms (neither segmentation)\n",
        "class CVUSADataset(Dataset):\n",
        "    def __init__(self, data_dir, split = 'train'):\n",
        "\n",
        "        self.split = split\n",
        "        self.data = self.load_data(data_dir + f'/{split}.csv')\n",
        "\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        data = []\n",
        "        with open(csv_path, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader) #skip header\n",
        "            for row in csv_reader:\n",
        "                bingmap = row[3]\n",
        "                streetview = row[1]\n",
        "                #segmap = row[2]\n",
        "                data.append({\"streetview\": streetview, \"bingmap\": bingmap})\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dictionary = self.data[index]\n",
        "        streetview = dictionary['streetview']\n",
        "        bingmap = dictionary['bingmap']\n",
        "        return streetview, bingmap\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CVUSA-Dataset-{self.split}: {len(self.data)} samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj0ecN-1QnVq"
      },
      "outputs": [],
      "source": [
        "# @title Data module definition: without using polar transforms (neither segmentation)\n",
        "class CVUSADataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size=8):\n",
        "\n",
        "        # Initialize the CustomDataModule\n",
        "\n",
        "        super(CVUSADataModule, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Load the entire dataset\n",
        "        self.train_dataset = CVUSADataset(data_dir=self.data_dir, split='train')\n",
        "        print(self.train_dataset)\n",
        "\n",
        "        #self.test_dataset = CVUSADataset(data_dir=self.data_dir, split='test')\n",
        "        #print(self.test_dataset)\n",
        "\n",
        "        self.val_dataset = CVUSADataset(data_dir=self.data_dir, split='val')\n",
        "        print(self.val_dataset)\n",
        "\n",
        "    #collate function is useful so we don't overuse RAM, training is a little bit slower tho...\n",
        "    def collate_fn(self,batch):\n",
        "        streetview_path, bingmap_path = zip(*batch)\n",
        "        # Load and transform each image in the batch\n",
        "        streetview_images_tensor = self.__images_to_tensor(streetview_path)\n",
        "\n",
        "        bingmap_images_tensor = self.__images_to_tensor(bingmap_path)\n",
        "\n",
        "        return streetview_images_tensor, bingmap_images_tensor\n",
        "\n",
        "    #we could add transformations (first of all normalization of the input!)\n",
        "    def __images_to_tensor(self, paths):\n",
        "        images = []\n",
        "        for img_path in paths:\n",
        "            img = Image.open(os.path.join(self.data_dir, img_path))\n",
        "            img_tensor = transforms.ToTensor()(img)\n",
        "            images.append(img_tensor)\n",
        "\n",
        "        # Stack the image tensors along the batch dimension\n",
        "        images_tensor = torch.stack(images)\n",
        "\n",
        "        return images_tensor\n",
        "\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=True,num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=False,num_workers=2)\n",
        "\n",
        "    #def test_dataloader(self):\n",
        "    #    return DataLoader(self.test_dataset,batch_size=self.batch_size, collate_fn=self.collate_fn,shuffle=True,num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkLiGNItQpOk",
        "outputId": "4c44a013-21c6-43d0-d75a-36d5cea6df62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CVUSA-Dataset-train: 6647 samples\n",
            "CVUSA-Dataset-val: 2215 samples\n"
          ]
        }
      ],
      "source": [
        "# @title Creating dataloaders: without using polar transforms (neither segmentation)\n",
        "data_dir = '/content/drive/MyDrive/CV Project/CVUSA'\n",
        "\n",
        "data_module = CVUSADataModule(data_dir = data_dir, batch_size = 16)\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()\n",
        "#test_loader = data_module.test_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VmODtjKnRzH"
      },
      "source": [
        "#Losses and other utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3LrvTpoe2-R"
      },
      "outputs": [],
      "source": [
        "# # @title TripletLoss implementation\n",
        "# class TripletLoss(pl.LightningModule):\n",
        "#     def __init__(self, margin=1.0):\n",
        "#         super(TripletLoss, self).__init__()\n",
        "#         self.margin = margin\n",
        "\n",
        "#     def calc_euclidean(self, x1, x2):\n",
        "#         return (x1 - x2).pow(2).sum(1)\n",
        "\n",
        "#     def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "#         distance_positive = self.calc_euclidean(anchor, positive)\n",
        "#         distance_negative = self.calc_euclidean(anchor, negative)\n",
        "#         losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "\n",
        "#         return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TripletLoss implementation\n",
        "class TripletLoss(pl.LightningModule):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, image_features1, image_features2, k = None):\n",
        "        N = len(image_features1)\n",
        "        if k is None:\n",
        "          k = N-1\n",
        "        # Calcolare le distanze Euclidee tra le features\n",
        "        distances_per_image1 = torch.cdist(image_features1, image_features2, p=2)  # p=2 per distanza Euclidea\n",
        "        distances_per_image2 = distances_per_image1.T  # Per simmetria\n",
        "        #in common between the losses:\n",
        "        mask = torch.eye(distances_per_image1.size(0), dtype=torch.bool, device = device)\n",
        "        distances_diag = torch.masked_select(distances_per_image1, mask) @ torch.eye(n=N, device = device)\n",
        "\n",
        "        distances_positive_matrix = distances_diag @ torch.ones(size=(N,self.k), device = device) #da controllare\n",
        "        margin_matrix = torch.ones(size = (N,self.k), device = device) * self.margin\n",
        "\n",
        "\n",
        "        distances_no_diag = torch.masked_select(distances_per_image1, mask.logical_not()).view(distances_per_image1.size(0), -1) #need to see if is NxN-1\n",
        "        distances_no_diag, _ = torch.topk(distances_no_diag, k = self.k, dim = 1, largest = False)\n",
        "        print(distances_no_diag.shape)\n",
        "        loss_matrix = torch.relu(distances_positive_matrix - distances_no_diag + margin_matrix)\n",
        "\n",
        "        loss1 = torch.mean(loss_matrix, dim=(0,1))\n",
        "\n",
        "        distances_no_diag = torch.masked_select(distances_per_image2, mask.logical_not()).view(distances_per_image2.size(0), -1) #need to see if is NxN-1\n",
        "        distances_no_diag, _ = torch.topk(distances_no_diag, k = self.k, dim = 1, largest = False)\n",
        "        loss_matrix = torch.relu(distances_positive_matrix - distances_no_diag + margin_matrix)\n",
        "\n",
        "        loss2 = torch.mean(loss_matrix, dim=(0,1))\n",
        "\n",
        "        return (loss1 + loss2)/2"
      ],
      "metadata": {
        "id": "7lHAEBs7rC2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title InfoNCE implementation\n",
        "class InfoNCE(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_function):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss_function = loss_function #we can use a generic loss function!\n",
        "\n",
        "    def forward(self, image_features1, image_features2, logit_scale):\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "\n",
        "        logits_per_image1 = logit_scale * image_features1 @ image_features2.T #similarity matrix\n",
        "\n",
        "        logits_per_image2 = logits_per_image1.T\n",
        "\n",
        "        labels = torch.arange(len(logits_per_image1), dtype=torch.long, device = device)\n",
        "\n",
        "        loss = (self.loss_function(logits_per_image1, labels) + self.loss_function(logits_per_image2, labels))/2\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ukTOSR3IgPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u6wnToFjtlF"
      },
      "outputs": [],
      "source": [
        "# @title Top-K Rank Accuracy: takes embeddings in input\n",
        "\n",
        "def top_k_rank_accuracy(emb1, emb2, k=1):\n",
        "    if k > len(emb1) :\n",
        "      return 0.0 #might happen at the end of the dataset (batch less then the chosen one)\n",
        "    # Calculate cosine similarity\n",
        "    correct_in_top_k = 0\n",
        "    for index, elem in enumerate(emb1):\n",
        "        cosine_sim = F.cosine_similarity(elem, emb2, dim=1)\n",
        "\n",
        "        # Find the rank K similarity\n",
        "        top_k_similarities, top_k_indices = torch.topk(cosine_sim, k, largest=True)\n",
        "\n",
        "        correct_in_top_k += index in top_k_indices.tolist()\n",
        "\n",
        "    accuracy = correct_in_top_k / len(emb1)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaYUTm_wqrUR"
      },
      "outputs": [],
      "source": [
        "# @title Generation of negatives: modularized so that we can experiment with a lot of versions!\n",
        "def generate_negatives(positives):\n",
        "  shuffled_indices = torch.randperm(positives.size(0))\n",
        "  negatives = positives[shuffled_indices]\n",
        "\n",
        "  return negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouQUwE2nWua"
      },
      "source": [
        "# Training Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "abAMZR3-HJv4",
        "outputId": "6830bc49-472c-4151-eec4-8d03f506f8f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "class ResNet50Branch(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(ResNet50Branch, self).__init__()\n",
        "        self.resnet50 = models.resnet50(pretrained=True)\n",
        "        # Modify the last layer for your specific task\n",
        "        self.resnet50.fc = torch.nn.Linear(self.resnet50.fc.in_features, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet50(x)\n",
        "\n",
        "class DualResNet50Model(pl.LightningModule):\n",
        "    def __init__(self, margin = 1.0):\n",
        "        super(DualResNet50Model, self).__init__()\n",
        "        self.branch1 = ResNet50Branch()\n",
        "        self.branch2 = ResNet50Branch()\n",
        "\n",
        "        self.loss = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.branch1(x1)\n",
        "        out2 = self.branch2(x2)\n",
        "        return out1, out2\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        #1 indicates necessary for loss 1, 2 for loss 2. we take the average.\n",
        "        \"\"\"anchor_embed1 = out1\n",
        "        positive_embed1 = out2\n",
        "\n",
        "        negative_embed1 = generate_negatives(positive_embed1)\n",
        "        loss1 = self.loss(anchor_embed1, positive_embed1, negative_embed1) #first loss\n",
        "\n",
        "        loss2 = 0.0\n",
        "        if self.double_loss:\n",
        "          anchor_embed2 = out2\n",
        "          positive_embed2 = out1\n",
        "\n",
        "          negative_embed2 = generate_negatives(positive_embed2)\n",
        "          loss2 = self.loss(anchor_embed2, positive_embed2, negative_embed2)\n",
        "\n",
        "        loss = loss1 + loss2\"\"\"\n",
        "\n",
        "        loss = self.loss(out1, out2, logit_scale = 1)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "\n",
        "        self.log('train_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('val_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('test_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "# Instantiate the model\n",
        "model = DualResNet50Model(margin = 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVD0l4tD4Byc",
        "outputId": "8ad106e0-b47e-486c-d962-159b2fed1571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=5,\n",
        "    devices=1,\n",
        "    callbacks=[RichProgressBar()],\n",
        "    log_every_n_steps=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "p57rNWjHN1hh",
        "outputId": "8b7af2cf-8353-4fb6-b5eb-0f5b84848082"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[37mEpoch 0/4 \u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m416/416\u001b[0m \u001b[38;5;245m1:31:00 • 0:00:00\u001b[0m \u001b[38;5;249m0.11it/s\u001b[0m \u001b[37mv_num: 4.000 train_top1_step:     \u001b[0m\n",
              "                                                                                 \u001b[37m0.429 train_loss_step: 1.553      \u001b[0m\n",
              "\u001b[37mValidation\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━\u001b[0m \u001b[37m99/139 \u001b[0m \u001b[38;5;245m0:23:32 • 0:06:02\u001b[0m \u001b[38;5;249m0.11it/s\u001b[0m                                   \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Epoch 0/4 </span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">416/416</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">1:31:00 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.11it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 4.000 train_top1_step:     </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.429 train_loss_step: 1.553      </span>\n",
              "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Validation</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">99/139 </span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:23:32 • 0:06:02</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.11it/s</span>                                   \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dubja7aKNutk"
      },
      "outputs": [],
      "source": [
        "trainer.test(dataloaders=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "jkq6qdsP7m25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth estimation"
      ],
      "metadata": {
        "id": "lD6Pwhc4Dj7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "checkpoint = \"vinvino02/glpn-nyu\"\n",
        "depth_estimator = pipeline(\"depth-estimation\", model=checkpoint)"
      ],
      "metadata": {
        "id": "ZllGbCNdwgOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://unsplash.com/photos/HwBAsSbPBDU/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MzR8fGNhciUyMGluJTIwdGhlJTIwc3RyZWV0fGVufDB8MHx8fDE2Nzg5MDEwODg&force=true&w=640\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "7wPIZ63Swhsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = depth_estimator(image)"
      ],
      "metadata": {
        "id": "88pL05K4wotg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[\"depth\"]"
      ],
      "metadata": {
        "id": "spYmg4qDwsIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
        "\n",
        "checkpoint = \"vinvino02/glpn-nyu\"\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
        "model = AutoModelForDepthEstimation.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "qiiNBc6Qx0fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"/content/drive/MyDrive/CV Project/CVUSA/streetview/0000052.jpg\")\n",
        "with torch.no_grad():\n",
        "      pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
        "      outputs = model(pixel_values)\n",
        "      predicted_depth = outputs.predicted_depth"
      ],
      "metadata": {
        "id": "sAGtPjXqyqr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# interpolate to original size\n",
        "prediction = torch.nn.functional.interpolate(\n",
        "    predicted_depth.unsqueeze(1),\n",
        "    size=image.size[::-1],\n",
        "    mode=\"bicubic\",\n",
        "    align_corners=False,\n",
        ").squeeze()\n",
        "output = prediction.numpy()\n",
        "\n",
        "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
        "depth = Image.fromarray(formatted)\n",
        "depth"
      ],
      "metadata": {
        "id": "ryjCnToU0E4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#road\n",
        "#building\n",
        "#low vegetation\n",
        "#trees\n",
        "#car\n",
        "#clutter"
      ],
      "metadata": {
        "id": "NW2V4trwH8CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#negative sampling basato sulla similarità ovvero computo la similarità di un sample con tutti gli altri e uso quelli più vicini come negativi"
      ],
      "metadata": {
        "id": "w2ubbSCwNOJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#soft margin triplet loss and weighted soft margin triplet loss"
      ],
      "metadata": {
        "id": "rTOrOUd8Nwao"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}