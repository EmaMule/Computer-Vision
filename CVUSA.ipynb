{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaMule/Computer-Vision/blob/main/CVUSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FRSut8m-Mv"
      },
      "source": [
        "#Import and installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "bMqOyt0ONvDV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title Installing dependencies\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install pytorch_lightning\n",
        "!pip install patool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JBFZKHodQUoQ"
      },
      "outputs": [],
      "source": [
        "# @title Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import csv\n",
        "import cv2\n",
        "import gdown\n",
        "import patoolib\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler, RandomSampler, BatchSampler\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "# pytorch lighting\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar, RichProgressBar, ModelPruning\n",
        "from pytorch_lightning import loggers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Folders Setup\n",
        "\n",
        "shutil.rmtree('/content/input', ignore_errors = True)\n",
        "os.mkdir('/content/input')\n",
        "\n",
        "shutil.rmtree('/content/output', ignore_errors = True)\n",
        "os.mkdir('/content/output')\n",
        "\n",
        "shutil.rmtree('/content/output/log', ignore_errors = True)\n",
        "os.mkdir('/content/output/log')\n",
        "\n",
        "shutil.rmtree('/content/lightning_logs', ignore_errors = True)\n",
        "os.mkdir('/content/lightning_logs')"
      ],
      "metadata": {
        "id": "XTosPQ5YcmFt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Downloading Dataset\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_'\n",
        "output_file = '/content/input/CVUSA_subset.rar'\n",
        "output_dir = '/content/input/data'\n",
        "\n",
        "gdown.download(url, output_file)\n",
        "patoolib.extract_archive(output_file, outdir = output_dir)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=19fD1WMGTmusYk8E7ygT6nAJTluf3a_oH'\n",
        "output_file = '/content/input/train.csv'\n",
        "gdown.download(url, output_file)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1Rt6waJ6f-kM12Q2A9mgRxAcKfZPdg9IY'\n",
        "output_file = '/content/input/val.csv'\n",
        "gdown.download(url, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gKcEFA_tcicZ",
        "outputId": "dd918633-5f3e-41e4-8429-4f1c33a9bfa5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_\n",
            "From (redirected): https://drive.google.com/uc?id=17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_&confirm=t&uuid=5e00e325-be87-49a5-a458-2b155f2356d2\n",
            "To: /content/input/CVUSA_subset.rar\n",
            "100%|██████████| 4.38G/4.38G [01:02<00:00, 70.2MB/s]\n",
            "INFO patool: Extracting /content/input/CVUSA_subset.rar ...\n",
            "INFO:patool:Extracting /content/input/CVUSA_subset.rar ...\n",
            "INFO patool: ... creating output directory `/content/input/data'.\n",
            "INFO:patool:... creating output directory `/content/input/data'.\n",
            "INFO patool: running /usr/bin/unrar x -- /content/input/CVUSA_subset.rar\n",
            "INFO:patool:running /usr/bin/unrar x -- /content/input/CVUSA_subset.rar\n",
            "INFO patool:     with cwd='/content/input/data', input=''\n",
            "INFO:patool:    with cwd='/content/input/data', input=''\n",
            "INFO patool: ... /content/input/CVUSA_subset.rar extracted to `/content/input/data'.\n",
            "INFO:patool:... /content/input/CVUSA_subset.rar extracted to `/content/input/data'.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19fD1WMGTmusYk8E7ygT6nAJTluf3a_oH\n",
            "To: /content/input/train.csv\n",
            "100%|██████████| 931k/931k [00:00<00:00, 55.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Rt6waJ6f-kM12Q2A9mgRxAcKfZPdg9IY\n",
            "To: /content/input/val.csv\n",
            "100%|██████████| 310k/310k [00:00<00:00, 13.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/input/val.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Settings\n",
        "\n",
        "pl.seed_everything(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOvjHHWCdLWy",
        "outputId": "3d14afd5-9f23-48e4-a219-4882e2e36b26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDSCF0zinL8Y"
      },
      "source": [
        "#Dataset and DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UnwPoYpJd6"
      },
      "source": [
        "We need to also possibly add polar and segmap! not done right now because there is a problem with the csv files. Also no test set, should we use validation or split the training and use the current validation as test?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "HT1pbNEXQknc"
      },
      "outputs": [],
      "source": [
        "# @title Dataset definition: without using polar transforms (neither segmentation)\n",
        "\n",
        "# Expected dataset structure: the input_dir contains the split cvs files and a\n",
        "# subdirectory named 'data' with the CVUSA dataset\n",
        "\n",
        "class CVUSADataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_dir, split = 'train'):\n",
        "        self.split = split\n",
        "        self.data = self.load_data(input_dir + f'/{split}.csv')\n",
        "\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        data = []\n",
        "        with open(csv_path, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader) #skip header\n",
        "            for row in csv_reader:\n",
        "                streetview_path = row[1]\n",
        "                satmap_path = row[0] #we are using normal images\n",
        "                #segmap = row[2]\n",
        "                data.append({\"streetview_path\": streetview_path, \"satmap_path\": satmap_path})\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dictionary = self.data[index]\n",
        "        streetview_path = dictionary['streetview_path']\n",
        "        satmap_path = dictionary['satmap_path']\n",
        "        return streetview_path, satmap_path\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CVUSA-Dataset-{self.split}: {len(self.data)} samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Vj0ecN-1QnVq"
      },
      "outputs": [],
      "source": [
        "# @title Data module definition: without using polar transforms (neither segmentation)\n",
        "\n",
        "class CVUSADataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, input_dir, batch_size=8, resize_grd = None, resize_sat = None):\n",
        "        # Initialize the CustomDataModule\n",
        "        super(CVUSADataModule, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dir = input_dir\n",
        "        self.data_dir = input_dir + '/data'\n",
        "        self.resize_grd = resize_grd\n",
        "        self.resize_sat = resize_sat\n",
        "        self.grd_size = None\n",
        "        self.sat_size = None\n",
        "        self.transform = None\n",
        "\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Load the entire dataset\n",
        "        self.train_dataset = CVUSADataset(input_dir=self.input_dir, split='train')\n",
        "        print(self.train_dataset)\n",
        "\n",
        "        #self.test_dataset = CVUSADataset(input_dir=self.input_dir, split='test')\n",
        "        #print(self.test_dataset)\n",
        "\n",
        "        self.val_dataset = CVUSADataset(input_dir=self.input_dir, split='val')\n",
        "        print(self.val_dataset)\n",
        "\n",
        "        grd_sample, sat_sample = self.train_dataset[0]\n",
        "        grd_image = Image.open(os.path.join(self.data_dir, grd_sample))\n",
        "        sat_image = Image.open(os.path.join(self.data_dir, sat_sample))\n",
        "\n",
        "        if self.resize_grd:\n",
        "          self.grd_size = transforms.Resize((self.resize_grd))(grd_image).size\n",
        "        else:\n",
        "          self.grd_size = grd_image.size\n",
        "\n",
        "        if self.resize_sat:\n",
        "          self.sat_size = transforms.Resize((self.resize_sat))(sat_image).size\n",
        "        else:\n",
        "          self.sat_size = sat_image.size\n",
        "\n",
        "\n",
        "    #collate function is useful so we don't overuse RAM, training is a little bit slower tho...\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        streetview_path, satmap_path = zip(*batch)\n",
        "\n",
        "        # Load and transform each image in the batch\n",
        "        streetview_ids, streetview_images_tensor = self.__images_to_tensor(streetview_path, 'grd')\n",
        "        satmap_ids, satmap_images_tensor = self.__images_to_tensor(satmap_path, 'sat')\n",
        "\n",
        "        streetviews = {'imgs': streetview_images_tensor, 'imgs_id': streetview_ids}\n",
        "        satmaps = {'imgs': satmap_images_tensor, 'imgs_id': satmap_ids}\n",
        "\n",
        "        return streetviews, satmaps\n",
        "\n",
        "\n",
        "    # we could add transformations (first of all normalization of the input!)\n",
        "    def __images_to_tensor(self, paths, img_type):\n",
        "        images = []\n",
        "        ids = []\n",
        "        resize = self.resize_grd if img_type == 'grd' else self.resize_sat\n",
        "        for img_path in paths:\n",
        "            img = Image.open(os.path.join(self.data_dir, img_path))\n",
        "            if resize:\n",
        "              img = transforms.Resize((resize))(img)\n",
        "            img_tensor = transforms.ToTensor()(img)\n",
        "            images.append(img_tensor)\n",
        "            ids.append(int(img_path[-11:-4]))\n",
        "\n",
        "        # Stack the image tensors along the batch dimension\n",
        "        images_tensor = torch.stack(images)\n",
        "        ids_tensor = torch.tensor(ids, dtype=int)\n",
        "        return ids_tensor, images_tensor\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=True,num_workers=2)\n",
        "\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=False,num_workers=2)\n",
        "\n",
        "\n",
        "    #def test_dataloader(self):\n",
        "    #    return DataLoader(self.test_dataset,batch_size=self.batch_size, collate_fn=self.collate_fn,shuffle=True,num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkLiGNItQpOk",
        "outputId": "4425c890-d4d6-4e07-fe17-3bc251e1d5ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CVUSA-Dataset-train: 6647 samples\n",
            "CVUSA-Dataset-val: 2215 samples\n"
          ]
        }
      ],
      "source": [
        "# @title Creating dataloaders: without using polar transforms (neither segmentation)\n",
        "input_dir = '/content/input'\n",
        "\n",
        "data_module = CVUSADataModule(input_dir = input_dir, batch_size = 64, resize_grd = 64, resize_sat = 128)\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()\n",
        "#test_loader = data_module.test_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VmODtjKnRzH"
      },
      "source": [
        "#Losses and other utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "N3LrvTpoe2-R"
      },
      "outputs": [],
      "source": [
        "# # @title TripletLoss implementation\n",
        "# class TripletLoss(pl.LightningModule):\n",
        "#     def __init__(self, margin=1.0):\n",
        "#         super(TripletLoss, self).__init__()\n",
        "#         self.margin = margin\n",
        "\n",
        "#     def calc_euclidean(self, x1, x2):\n",
        "#         return (x1 - x2).pow(2).sum(1)\n",
        "\n",
        "#     def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "#         distance_positive = self.calc_euclidean(anchor, positive)\n",
        "#         distance_negative = self.calc_euclidean(anchor, negative)\n",
        "#         losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "\n",
        "#         return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title TripletLoss implementation\n",
        "# class TripletLoss(pl.LightningModule):\n",
        "#     def __init__(self, margin=1.0):\n",
        "#         super(TripletLoss, self).__init__()\n",
        "#         self.margin = margin\n",
        "\n",
        "#     def forward(self, image_features1, image_features2, k = None):\n",
        "#         N = len(image_features1)\n",
        "#         if k is None:\n",
        "#           k = N-1\n",
        "#         # Calcolare le distanze Euclidee tra le features\n",
        "#         distances_per_image1 = torch.cdist(image_features1, image_features2, p=2)  # p=2 per distanza Euclidea\n",
        "#         distances_per_image2 = distances_per_image1.T  # Per simmetria\n",
        "#         #in common between the losses:\n",
        "#         mask = torch.eye(distances_per_image1.size(0), dtype=torch.bool, device = device)\n",
        "#         distances_diag = torch.masked_select(distances_per_image1, mask) @ torch.eye(n=N, device = device)\n",
        "\n",
        "#         distances_positive_matrix = distances_diag @ torch.ones(size=(N,k), device = device) #da controllare\n",
        "#         margin_matrix = torch.ones(size = (N,k), device = device) * self.margin\n",
        "\n",
        "\n",
        "#         distances_no_diag = torch.masked_select(distances_per_image1, mask.logical_not()).view(distances_per_image1.size(0), -1) #need to see if is NxN-1\n",
        "#         distances_no_diag, _ = torch.topk(distances_no_diag, k = k, dim = 1, largest = False)\n",
        "#         loss_matrix = torch.relu(distances_positive_matrix - distances_no_diag + margin_matrix)\n",
        "\n",
        "#         loss1 = torch.mean(loss_matrix, dim=(0,1))\n",
        "\n",
        "#         distances_no_diag = torch.masked_select(distances_per_image2, mask.logical_not()).view(distances_per_image2.size(0), -1) #need to see if is NxN-1\n",
        "#         distances_no_diag, _ = torch.topk(distances_no_diag, k = k, dim = 1, largest = False)\n",
        "#         loss_matrix = torch.relu(distances_positive_matrix - distances_no_diag + margin_matrix)\n",
        "\n",
        "#         loss2 = torch.mean(loss_matrix, dim=(0,1))\n",
        "\n",
        "#         return (loss1 + loss2)/2"
      ],
      "metadata": {
        "id": "7lHAEBs7rC2h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation TripletLoss more stable\n",
        "\n",
        "# RIVEDI (confronto con triplet torchreid)\n",
        "\n",
        "class TripletLoss(pl.LightningModule):\n",
        "    def __init__(self, loss_weight = 1e-2):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "    def forward(self, image_features1, image_features2):\n",
        "        #image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        #image_features2 = F.normalize(image_features2, dim=-1)\n",
        "        dist_array = 2.0 - 2.0 * torch.matmul(image_features2, image_features1.T)\n",
        "        n = len(image_features1)\n",
        "        pos_dist = torch.diag(dist_array)\n",
        "        pair_n = n * (n - 1.0)\n",
        "        triplet_dist_g2s = pos_dist - dist_array\n",
        "        loss_g2s = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_g2s * self.loss_weight)))/pair_n\n",
        "        triplet_dist_s2g = torch.unsqueeze(pos_dist, 1) - dist_array\n",
        "        loss_s2g = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_s2g * self.loss_weight)))/pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "kuwQsXRGMI0L"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title InfoNCE implementation\n",
        "class InfoNCE(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_function):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss_function = loss_function #we can use a generic loss function!\n",
        "\n",
        "    def forward(self, image_features1, image_features2, logit_scale):\n",
        "\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "\n",
        "        logits_per_image1 = logit_scale * image_features1 @ image_features2.T #similarity matrix\n",
        "        logits_per_image2 = logits_per_image1.T\n",
        "\n",
        "        labels = torch.arange(len(logits_per_image1), dtype=torch.long, device = device)\n",
        "        loss = (self.loss_function(logits_per_image1, labels) + self.loss_function(logits_per_image2, labels))/2\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ukTOSR3IgPf7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_u6wnToFjtlF"
      },
      "outputs": [],
      "source": [
        "# @title Top-K Rank Accuracy: takes embeddings in input\n",
        "\n",
        "def top_k_rank_accuracy(emb1, emb2, k=1):\n",
        "    if k > len(emb1) :\n",
        "      return 0.0 #might happen at the end of the dataset (batch less then the chosen one)\n",
        "    # Calculate cosine similarity\n",
        "    correct_in_top_k = 0\n",
        "    for index, elem in enumerate(emb1):\n",
        "        cosine_sim = F.cosine_similarity(elem, emb2, dim=1)\n",
        "\n",
        "        # Find the rank K similarity\n",
        "        top_k_similarities, top_k_indices = torch.topk(cosine_sim, k, largest=True)\n",
        "\n",
        "        correct_in_top_k += index in top_k_indices.tolist()\n",
        "\n",
        "    accuracy = correct_in_top_k / len(emb1)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oaYUTm_wqrUR"
      },
      "outputs": [],
      "source": [
        "# @title Generation of negatives: modularized so that we can experiment with a lot of versions!\n",
        "def generate_negatives(positives):\n",
        "  shuffled_indices = torch.randperm(positives.size(0))\n",
        "  negatives = positives[shuffled_indices]\n",
        "\n",
        "  return negatives"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(pl.LightningModule):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MvHm7d1ICYme"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouQUwE2nWua"
      },
      "source": [
        "# Training Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "abAMZR3-HJv4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Resnet\n",
        "\n",
        "class ResNet50Branch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ResNet50Branch, self).__init__()\n",
        "        self.resnet50 = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n",
        "        # Modify the last layer for your specific task\n",
        "        self.resnet50.fc = torch.nn.Linear(self.resnet50.fc.in_features, 512)\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps = False):\n",
        "\n",
        "        # to print the featuremap we need to return the last conv layer output\n",
        "        if featuremaps:\n",
        "            x = self.resnet50.conv1(x)\n",
        "            x = self.resnet50.bn1(x)\n",
        "            x = self.resnet50.relu(x)\n",
        "            x = self.resnet50.maxpool(x)\n",
        "            x = self.resnet50.layer1(x)\n",
        "            x = self.resnet50.layer2(x)\n",
        "            x = self.resnet50.layer3(x)\n",
        "            x = self.resnet50.layer4(x)\n",
        "            return x\n",
        "\n",
        "        else:\n",
        "            return self.resnet50(x)\n",
        "\n",
        "\n",
        "\n",
        "class DualResNet50Model(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DualResNet50Model, self).__init__()\n",
        "        self.branch1 = ResNet50Branch()\n",
        "        self.branch2 = ResNet50Branch()\n",
        "\n",
        "        self.loss1 = TripletLoss()\n",
        "        self.loss2 = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.branch1(x1['imgs'])\n",
        "        out2 = self.branch2(x2['imgs'])\n",
        "        return out1, out2\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        loss = self.loss2(out1, out2, logit_scale = 3.0)\n",
        "        #loss = self.loss1(out1, out2) + self.loss2(out1, out2, logit_scale = 3.0)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "\n",
        "        self.log('train_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('val_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('test_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# P nel disegno indica la dimensione di ogni patch\n",
        "# devi usare lo stride per diminuire la W and H dell'input\n",
        "# però aumentando il numero di channel (dimensione embedding)\n",
        "\n",
        "# i positional embedding sono un vettore di vettori da imparare direttamente\n",
        "# self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
        "# self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "class ConvBnReluBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(ConvBnReluBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_heads,\n",
        "        qkv_bias=False,\n",
        "        qk_scale=None,\n",
        "        drop=0.,\n",
        "        attn_drop=0.,\n",
        "        dropout=0.,\n",
        "        norm_layer=nn.LayerNorm\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop\n",
        "        )\n",
        "        # check what is droppath\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class SAIGBranch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, img_size, patch_size=16, in_channels=3, embed_dim=768, norm_layer=None, flatten=True):\n",
        "        super(SAIGBranch, self).__init__()\n",
        "        embed_dim = 384\n",
        "        num_heads = 8\n",
        "        depth = 2\n",
        "        qkv_bias = True\n",
        "        qk_scale = None\n",
        "        drop_rate = 0\n",
        "        attn_drop_rate = 0\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
        "\n",
        "        if img_size[0] % patch_size != 0 or img_size[1] % patch_size != 0:\n",
        "          print(\"Warning: image size is not divisible for patch size\")\n",
        "\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "        self.conv_bn_relu_blocks = nn.Sequential(\n",
        "            ConvBnReluBlock(in_channels = 3, out_channels = 64, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 64, out_channels = 128, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 128, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 256, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 256, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 512, stride = 2),\n",
        "        )\n",
        "        self.patch_block = nn.Conv2d(in_channels = 512, out_channels = embed_dim, kernel_size=1, stride=1 ,padding=0)\n",
        "        self.attn_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
        "        #self.pos_embed = nn.Parameter(torch.zeros(1, embed_dim, self.num_patches))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        #self.GAP = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        #self.logits = nn.Linear(in_features = embed_dim, out_features = 512)\n",
        "\n",
        "        self.smd = nn.Sequential(\n",
        "            nn.Linear(self.num_patches, self.num_patches*4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.num_patches*4, self.num_patches),\n",
        "            nn.Linear(self.num_patches, 8)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps = False):\n",
        "\n",
        "      # extract patch embeddings\n",
        "      x = self.conv_bn_relu_blocks(x)\n",
        "      x = self.patch_block(x)\n",
        "      x = x.flatten(2).transpose(1,2)\n",
        "      #x = self.norm(x) CHECK\n",
        "\n",
        "      # add position embeddings\n",
        "      x = x + self.pos_embed\n",
        "      x = self.pos_drop(x)\n",
        "\n",
        "      # pass through sequence of attention blocks\n",
        "      for blk in self.attn_blocks:\n",
        "          x = blk(x)\n",
        "\n",
        "      x = self.norm(x)\n",
        "      # x = self.GAP(x.transpose(-1, -2)).squeeze(2)\n",
        "      # x = self.logits(x)\n",
        "\n",
        "      # x: b x 88 x 384\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = self.smd(x)\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = x.flatten(-2, -1)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "class DualSAIGModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, grd_size, sat_size):\n",
        "        super(DualSAIGModel, self).__init__()\n",
        "        self.branch1 = SAIGBranch(grd_size)\n",
        "        self.branch2 = SAIGBranch(sat_size)\n",
        "\n",
        "        self.loss1 = TripletLoss()\n",
        "        self.loss2 = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.branch1(x1['imgs'])\n",
        "        out2 = self.branch2(x2['imgs'])\n",
        "        return out1, out2\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        loss = self.loss2(out1, out2, logit_scale = 3.0)\n",
        "        #loss = self.loss1(out1, out2) + self.loss2(out1, out2, logit_scale = 3.0)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "\n",
        "        self.log('train_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('val_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('test_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "QDzQtqQOHzU-"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DualSAIGModel(data_module.grd_size, data_module.sat_size)"
      ],
      "metadata": {
        "id": "_FqlWcHr0qMC"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DualResNet50Model()"
      ],
      "metadata": {
        "id": "4ujIfr0BDjGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVD0l4tD4Byc",
        "outputId": "5f431201-933f-4c16-a6fe-3074a643043e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar()],\n",
        "    log_every_n_steps = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "2f6abc8b065e4ddba5e83cb21673f0a9",
            "349ff1cd81484682937ea88e6bfe3444"
          ]
        },
        "id": "p57rNWjHN1hh",
        "outputId": "eeb3552d-4fc4-4957-e2d4-b5c5436898e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ branch1 │ SAIGBranch  │  3.8 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ branch2 │ SAIGBranch  │  3.7 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ loss1   │ TripletLoss │      0 │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss2   │ InfoNCE     │      0 │\n",
              "└───┴─────────┴─────────────┴────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ branch1 │ SAIGBranch  │  3.8 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ branch2 │ SAIGBranch  │  3.7 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ loss1   │ TripletLoss │      0 │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss2   │ InfoNCE     │      0 │\n",
              "└───┴─────────┴─────────────┴────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 7.5 M                                                                                            \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 7.5 M                                                                                                \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 30                                                                         \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 7.5 M                                                                                            \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 7.5 M                                                                                                \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 30                                                                         \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f6abc8b065e4ddba5e83cb21673f0a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is \n",
              "incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
              "  self.pid = os.fork()\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dubja7aKNutk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "dd45f0d9-064c-4da1-a2f3-8b7a7e1a405c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "`.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0a17c1993fdb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer.test(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         ckpt_path = self._checkpoint_connector._select_ckpt_path(\n\u001b[0m\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_select_ckpt_path\u001b[0;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             ckpt_path = self._parse_ckpt_path(\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0mstate_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py\u001b[0m in \u001b[0;36m_parse_ckpt_path\u001b[0;34m(self, state_fn, ckpt_path, model_provided, model_connected)\u001b[0m\n\u001b[1;32m    169\u001b[0m                         \u001b[0;34mf\" Please pass an exact checkpoint path to `.{fn}(ckpt_path=...)`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                     )\n\u001b[0;32m--> 171\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    172\u001b[0m                     \u001b[0;34mf'`.{fn}(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: `.test(ckpt_path=\"best\")` is set but `ModelCheckpoint` is not configured to save the best model."
          ]
        }
      ],
      "source": [
        "trainer.test(\n",
        "    dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth estimation"
      ],
      "metadata": {
        "id": "lD6Pwhc4Dj7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "checkpoint = \"vinvino02/glpn-nyu\"\n",
        "depth_estimator = pipeline(\"depth-estimation\", model=checkpoint)"
      ],
      "metadata": {
        "id": "ZllGbCNdwgOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://unsplash.com/photos/HwBAsSbPBDU/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MzR8fGNhciUyMGluJTIwdGhlJTIwc3RyZWV0fGVufDB8MHx8fDE2Nzg5MDEwODg&force=true&w=640\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "7wPIZ63Swhsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = depth_estimator(image)"
      ],
      "metadata": {
        "id": "88pL05K4wotg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[\"depth\"]"
      ],
      "metadata": {
        "id": "spYmg4qDwsIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
        "\n",
        "checkpoint = \"vinvino02/glpn-nyu\"\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
        "model = AutoModelForDepthEstimation.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "qiiNBc6Qx0fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"/content/drive/MyDrive/CV Project/CVUSA/streetview/0000052.jpg\")\n",
        "with torch.no_grad():\n",
        "      pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
        "      outputs = model(pixel_values)\n",
        "      predicted_depth = outputs.predicted_depth"
      ],
      "metadata": {
        "id": "sAGtPjXqyqr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# interpolate to original size\n",
        "prediction = torch.nn.functional.interpolate(\n",
        "    predicted_depth.unsqueeze(1),\n",
        "    size=image.size[::-1],\n",
        "    mode=\"bicubic\",\n",
        "    align_corners=False,\n",
        ").squeeze()\n",
        "output = prediction.numpy()\n",
        "\n",
        "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
        "depth = Image.fromarray(formatted)\n",
        "depth"
      ],
      "metadata": {
        "id": "ryjCnToU0E4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#road\n",
        "#building\n",
        "#low vegetation\n",
        "#trees\n",
        "#car\n",
        "#clutter"
      ],
      "metadata": {
        "id": "NW2V4trwH8CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#negative sampling basato sulla similarità ovvero computo la similarità di un sample con tutti gli altri e uso quelli più vicini come negativi"
      ],
      "metadata": {
        "id": "w2ubbSCwNOJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#soft margin triplet loss and weighted soft margin triplet loss"
      ],
      "metadata": {
        "id": "rTOrOUd8Nwao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Functions"
      ],
      "metadata": {
        "id": "gJAKm9G3-Kwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize Heatmap\n",
        "\n",
        "# to visualize the images correctly, they must be de-normalized, so we must give\n",
        "# in input their mean and std\n",
        "\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "GRID_SPACING = 10\n",
        "\n",
        "@torch.no_grad()\n",
        "def visactmap(\n",
        "    model,\n",
        "    data_loader,\n",
        "    save_dir,\n",
        "    use_gpu,\n",
        "    img_mean=None,\n",
        "    img_std=None\n",
        "):\n",
        "\n",
        "    if img_mean is None or img_std is None:\n",
        "        img_mean = IMAGENET_MEAN\n",
        "        img_std = IMAGENET_STD\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    print('Visualizing activation maps')\n",
        "\n",
        "    for batch_idx, data in enumerate(data_loader):\n",
        "\n",
        "        # for now visualize only streetviews\n",
        "        streetview_imgs, streetview_ids = data[0]['imgs'], data[0]['imgs_id']\n",
        "        satmap_imgs, satmap_ids = data[1]['imgs'], data[1]['imgs_id']\n",
        "\n",
        "        if use_gpu:\n",
        "            streetview_imgs = streetview_imgs.cuda()\n",
        "            satmap_imgs = satmap_imgs.cuda()\n",
        "\n",
        "        try:\n",
        "            streetview_outputs = model.branch1(streetview_imgs, featuremaps=True)\n",
        "            satmap_outputs = model.branch2(satmap_imgs, featuremaps=True)\n",
        "        except TypeError:\n",
        "            raise TypeError('model.forward() doesn\\'t have featuremaps field')\n",
        "        if streetview_outputs.dim() != 4 or satmap_outputs.dim() != 4:\n",
        "            raise ValueError('model output is supposed to have 4 dimensions')\n",
        "\n",
        "        # compute activation maps for streetview (try adding square root?)\n",
        "        streetview_outputs = (streetview_outputs**2).sum(1)\n",
        "        b, h, w = streetview_outputs.size()\n",
        "        streetview_outputs = streetview_outputs.view(b, h * w)\n",
        "        streetview_outputs = nn.functional.normalize(streetview_outputs, p=2, dim=1)\n",
        "        streetview_outputs = streetview_outputs.view(b, h, w)\n",
        "\n",
        "        # compute activation maps for satmap\n",
        "        satmap_outputs = (satmap_outputs**2).sum(1)\n",
        "        b, h, w = satmap_outputs.size()\n",
        "        satmap_outputs = satmap_outputs.view(b, h * w)\n",
        "        satmap_outputs = nn.functional.normalize(satmap_outputs, p=2, dim=1)\n",
        "        satmap_outputs = satmap_outputs.view(b, h, w)\n",
        "\n",
        "        if use_gpu:\n",
        "            streetview_imgs, streetview_outputs = streetview_imgs.cpu(), streetview_outputs.cpu()\n",
        "            satmap_imgs, satmap_outputs = satmap_imgs.cpu(), satmap_outputs.cpu()\n",
        "\n",
        "        for j in range(streetview_outputs.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            imname = str(int(streetview_ids[j])).zfill(7)\n",
        "\n",
        "            # RGB image (from the normalized input image)\n",
        "            img = streetview_imgs[j, ...]\n",
        "            for t, m, s in zip(img, img_mean, img_std):\n",
        "                t.mul_(s).add_(m).clamp_(0, 1)\n",
        "            img = np.uint8(np.floor(img.numpy() * 255))\n",
        "            img = img.transpose((1, 2, 0))\n",
        "\n",
        "            height, width, _ = img.shape\n",
        "\n",
        "            # activation map (from the output image)\n",
        "            am = streetview_outputs[j, ...].numpy()\n",
        "            am = cv2.resize(am, (width, height))\n",
        "            am = 255 * (am - np.min(am)) / (np.max(am) - np.min(am) + 1e-12)\n",
        "            am = np.uint8(np.floor(am))\n",
        "            am = cv2.applyColorMap(am, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapping between the two images\n",
        "            overlapped = img*0.5 + am*0.5\n",
        "            overlapped[overlapped > 255] = 255\n",
        "            overlapped = overlapped.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            grid = 255 * np.ones((3*height + 2*GRID_SPACING, width, 3), dtype=np.uint8)\n",
        "            grid[:height, :, :] = img[:, :, ::-1]\n",
        "            grid[height + GRID_SPACING:2*height + GRID_SPACING, :, :] = am\n",
        "            grid[2*height + 2*GRID_SPACING:, :, :] = overlapped\n",
        "            cv2.imwrite(os.path.join(save_dir, imname + '_streetview.jpg'), grid)\n",
        "\n",
        "        for j in range(satmap_outputs.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            imname = str(int(satmap_ids[j])).zfill(7)\n",
        "\n",
        "            # RGB image (input image)\n",
        "            img = satmap_imgs[j, ...]\n",
        "            for t, m, s in zip(img, img_mean, img_std):\n",
        "                t.mul_(s).add_(m).clamp_(0, 1)\n",
        "            img = np.uint8(np.floor(img.numpy() * 255))\n",
        "            img = img.transpose((1, 2, 0))\n",
        "\n",
        "            height, width, _ = img.shape\n",
        "\n",
        "            # activation map\n",
        "            am = satmap_outputs[j, ...].numpy()\n",
        "            am = cv2.resize(am, (width, height))\n",
        "            am = 255 * (am - np.min(am)) / (np.max(am) - np.min(am) + 1e-12)\n",
        "            am = np.uint8(np.floor(am))\n",
        "            am = cv2.applyColorMap(am, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapped\n",
        "            overlapped = img*0.5 + am*0.5\n",
        "            overlapped[overlapped > 255] = 255\n",
        "            overlapped = overlapped.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            grid = 255 * np.ones((3*height + 2*GRID_SPACING, width, 3), dtype=np.uint8)\n",
        "            grid[:height, :, :] = img[:, :, ::-1]\n",
        "            grid[height + GRID_SPACING:2*height + GRID_SPACING, :, :] = am\n",
        "            grid[2*height + 2*GRID_SPACING:, :, :] = overlapped\n",
        "            cv2.imwrite(os.path.join(save_dir, imname + '_satmap.jpg'), grid)\n",
        "\n",
        "        if (batch_idx+1) % 10 == 0:\n",
        "            print('- done batch {}/{}'.format(batch_idx + 1, len(data_loader)))"
      ],
      "metadata": {
        "id": "36uMaOGy-NFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f -r '/content/output/log/visactmap'\n",
        "!mkdir '/content/output/log/visactmap'\n",
        "\n",
        "visactmap(\n",
        "    model = model,\n",
        "    data_loader = val_loader,\n",
        "    save_dir = '/content/output/log/visactmap',\n",
        "    use_gpu = True,\n",
        "    img_mean = [0, 0, 0],\n",
        "    img_std = [1, 1, 1]\n",
        ")"
      ],
      "metadata": {
        "id": "5dSp6VMiKNAa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "y1FRSut8m-Mv",
        "pDSCF0zinL8Y",
        "lD6Pwhc4Dj7l"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f6abc8b065e4ddba5e83cb21673f0a9": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_349ff1cd81484682937ea88e6bfe3444",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Epoch 11/29 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;98;6;224m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━\u001b[0m \u001b[37m70/104\u001b[0m \u001b[38;5;245m0:00:46 • 0:00:22\u001b[0m \u001b[38;5;249m1.55it/s\u001b[0m \u001b[37mv_num: 12.000 train_top1_step:    \u001b[0m\n                                                                                 \u001b[37m0.750 train_loss_step: 2.159      \u001b[0m\n                                                                                 \u001b[37mval_top1_step: 0.641              \u001b[0m\n                                                                                 \u001b[37mval_top3_step: 0.897              \u001b[0m\n                                                                                 \u001b[37mval_top10_step: 1.000             \u001b[0m\n                                                                                 \u001b[37mval_top1_epoch: 0.643             \u001b[0m\n                                                                                 \u001b[37mval_top3_epoch: 0.879             \u001b[0m\n                                                                                 \u001b[37mval_top10_epoch: 0.983            \u001b[0m\n                                                                                 \u001b[37mtrain_top1_epoch: 0.748           \u001b[0m\n                                                                                 \u001b[37mtrain_loss_epoch: 2.158           \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 11/29 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">70/104</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:46 • 0:00:22</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.55it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 12.000 train_top1_step:    </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.750 train_loss_step: 2.159      </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top1_step: 0.641              </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top3_step: 0.897              </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top10_step: 1.000             </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top1_epoch: 0.643             </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top3_epoch: 0.879             </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top10_epoch: 0.983            </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_top1_epoch: 0.748           </span>\n                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_epoch: 2.158           </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "349ff1cd81484682937ea88e6bfe3444": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}