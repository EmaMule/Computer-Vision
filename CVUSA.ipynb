{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaMule/Computer-Vision/blob/main/CVUSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FRSut8m-Mv"
      },
      "source": [
        "#Import and installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bMqOyt0ONvDV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title Installing dependencies\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install pytorch_lightning\n",
        "!pip install patool\n",
        "!pip install torchvision nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBFZKHodQUoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395e4825-1f18-453d-f17b-e927844ea106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ],
      "source": [
        "# @title Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import csv\n",
        "import cv2\n",
        "import gdown\n",
        "import patoolib\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler, RandomSampler, BatchSampler\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
        "\n",
        "# pytorch lighting\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar, RichProgressBar, ModelPruning\n",
        "from pytorch_lightning import loggers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Folders Setup\n",
        "\n",
        "shutil.rmtree('/content/input', ignore_errors = True)\n",
        "os.mkdir('/content/input')\n",
        "\n",
        "shutil.rmtree('/content/output', ignore_errors = True)\n",
        "os.mkdir('/content/output')\n",
        "\n",
        "shutil.rmtree('/content/output/log', ignore_errors = True)\n",
        "os.mkdir('/content/output/log')\n",
        "\n",
        "shutil.rmtree('/content/lightning_logs', ignore_errors = True)\n",
        "os.mkdir('/content/lightning_logs')"
      ],
      "metadata": {
        "id": "XTosPQ5YcmFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Downloading Dataset\n",
        "\n",
        "# our id: 1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd\n",
        "# their id: 17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd'\n",
        "output_file = '/content/input/CVUSA_subset.rar'\n",
        "output_dir = '/content/input/data'\n",
        "\n",
        "gdown.download(url, output_file)\n",
        "patoolib.extract_archive(output_file, outdir = output_dir)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=19fD1WMGTmusYk8E7ygT6nAJTluf3a_oH'\n",
        "output_file = '/content/input/train.csv'\n",
        "gdown.download(url, output_file)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1Rt6waJ6f-kM12Q2A9mgRxAcKfZPdg9IY'\n",
        "output_file = '/content/input/val.csv'\n",
        "gdown.download(url, output_file)"
      ],
      "metadata": {
        "id": "gKcEFA_tcicZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547094b1-fea9-48bf-a555-a1d9bf8e0a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd\n",
            "From (redirected): https://drive.google.com/uc?id=1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd&confirm=t&uuid=813aa56a-360a-4677-a502-6625b9ebe1f2\n",
            "To: /content/input/CVUSA_subset.rar\n",
            "100%|██████████| 4.38G/4.38G [03:16<00:00, 22.3MB/s]\n",
            "INFO patool: Extracting /content/input/CVUSA_subset.rar ...\n",
            "INFO:patool:Extracting /content/input/CVUSA_subset.rar ...\n",
            "INFO patool: ... creating output directory `/content/input/data'.\n",
            "INFO:patool:... creating output directory `/content/input/data'.\n",
            "INFO patool: running /usr/bin/unrar x -- /content/input/CVUSA_subset.rar\n",
            "INFO:patool:running /usr/bin/unrar x -- /content/input/CVUSA_subset.rar\n",
            "INFO patool:     with cwd='/content/input/data', input=''\n",
            "INFO:patool:    with cwd='/content/input/data', input=''\n",
            "INFO patool: ... /content/input/CVUSA_subset.rar extracted to `/content/input/data'.\n",
            "INFO:patool:... /content/input/CVUSA_subset.rar extracted to `/content/input/data'.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19fD1WMGTmusYk8E7ygT6nAJTluf3a_oH\n",
            "To: /content/input/train.csv\n",
            "100%|██████████| 931k/931k [00:00<00:00, 135MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Rt6waJ6f-kM12Q2A9mgRxAcKfZPdg9IY\n",
            "To: /content/input/val.csv\n",
            "100%|██████████| 310k/310k [00:00<00:00, 69.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/input/val.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Settings\n",
        "\n",
        "pl.seed_everything(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "uOvjHHWCdLWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57313aa4-ee24-4d85-8e25-115125de0388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDSCF0zinL8Y"
      },
      "source": [
        "#Dataset and DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UnwPoYpJd6"
      },
      "source": [
        "We need to also possibly add polar and segmap! not done right now because there is a problem with the csv files. Also no test set, should we use validation or split the training and use the current validation as test?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT1pbNEXQknc"
      },
      "outputs": [],
      "source": [
        "# @title Dataset definition: without using polar transforms (neither segmentation)\n",
        "\n",
        "# Expected dataset structure: the input_dir contains the split cvs files and a\n",
        "# subdirectory named 'data' with the CVUSA dataset\n",
        "\n",
        "class CVUSADataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_dir, split = 'train', polar = False):\n",
        "        self.split = split\n",
        "        self.polar = polar\n",
        "        self.data = self.load_data(input_dir + f'/{split}.csv')\n",
        "\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        data = []\n",
        "        with open(csv_path, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader) #skip header\n",
        "            for row in csv_reader:\n",
        "                grd_path = row[1]\n",
        "                if self.polar: #If we want to use polar\n",
        "                   sat_path = row[3]\n",
        "                   seg_path = row[4]\n",
        "                else:\n",
        "                  sat_path = row[0]\n",
        "                  seg_path = row[2]\n",
        "                data.append({\"grd_path\": grd_path, \"sat_path\": sat_path, \"seg_path\": seg_path})\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dictionary = self.data[index]\n",
        "        grd_path = dictionary['grd_path']\n",
        "        sat_path = dictionary['sat_path']\n",
        "        seg_path = dictionary['seg_path']\n",
        "        return grd_path, sat_path, seg_path\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CVUSA-Dataset-{self.split}: {len(self.data)} samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj0ecN-1QnVq"
      },
      "outputs": [],
      "source": [
        "# @title Data module definition: without using polar transforms (neither segmentation)\n",
        "\n",
        "class CVUSADataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, input_dir, batch_size=8, grd_resize = None, sat_resize = None, seg_resize = None):\n",
        "\n",
        "        super(CVUSADataModule, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dir = input_dir\n",
        "        self.data_dir = input_dir + '/data'\n",
        "\n",
        "        self.original_size = {'grd': None, 'sat': None, 'seg': None}\n",
        "        self.resize = {'grd': grd_resize, 'sat': sat_resize, 'seg': seg_resize}\n",
        "        self.size = {'grd': None, 'sat': None, 'seg': None}\n",
        "        self.mean = {'grd': [0,0,0], 'sat': [0,0,0], 'seg': [0,0,0]}\n",
        "        self.std = {'grd': [1,1,1], 'sat': [1,1,1], 'seg': [1,1,1]}\n",
        "        self.transform = {'grd': None, 'sat': None, 'seg': None}\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        # load the datasets\n",
        "        self.train_dataset = CVUSADataset(input_dir=self.input_dir, split='train')\n",
        "        self.val_dataset = CVUSADataset(input_dir=self.input_dir, split='val')\n",
        "        #self.test_dataset = CVUSADataset(input_dir=self.input_dir, split='test')\n",
        "\n",
        "        print(self.train_dataset)\n",
        "        print(self.val_dataset)\n",
        "        #print(self.test_dataset)\n",
        "\n",
        "        # find image sizes\n",
        "        grd_sample, sat_sample, seg_sample = self.train_dataset[0]\n",
        "        grd_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, grd_sample)))\n",
        "        sat_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, sat_sample)))\n",
        "        seg_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, seg_sample)))\n",
        "\n",
        "        self.original_size['grd'] = grd_image.size()[1:3]\n",
        "        self.original_size['sat'] = sat_image.size()[1:3]\n",
        "        self.original_size['seg'] = seg_image.size()[1:3]\n",
        "\n",
        "        self.size['grd'] = grd_image.size()[1:3]\n",
        "        self.size['sat'] = sat_image.size()[1:3]\n",
        "        self.size['seg'] = seg_image.size()[1:3]\n",
        "\n",
        "        # compute image new sizes\n",
        "        if self.resize['grd']:\n",
        "          self.size['grd'] = v2.Resize((self.resize['grd']))(grd_image).size()[1:3]\n",
        "        if self.resize['sat']:\n",
        "          self.size['sat'] = v2.Resize((self.resize['sat']))(sat_image).size()[1:3]\n",
        "        if self.resize['seg']:\n",
        "          self.size['seg'] = v2.Resize((self.resize['seg']))(seg_image).size()[1:3]\n",
        "\n",
        "        # compute transforms\n",
        "        self.transform['grd'] = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.Resize(self.size['grd']),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(self.mean['grd'], self.std['grd'], inplace=False)\n",
        "        ])\n",
        "\n",
        "        self.transform['sat'] = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.Resize(self.size['sat']),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(self.mean['sat'], self.std['sat'], inplace=False)\n",
        "        ])\n",
        "\n",
        "        self.transform['seg'] = v2.Compose([\n",
        "            v2.ToImage(),\n",
        "            v2.Resize(self.size['seg']),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(self.mean['seg'], self.std['seg'], inplace=False),\n",
        "        ])\n",
        "\n",
        "\n",
        "    #collate function is useful so we don't overuse RAM, training is a little bit slower tho...\n",
        "    def collate_fn(self,batch):\n",
        "\n",
        "        grd_path, sat_path, seg_path = zip(*batch)\n",
        "\n",
        "        # load and transform each image in the batch\n",
        "        grd_ids, grd_images = self.__compute_images(grd_path, 'grd')\n",
        "        sat_ids, sat_images = self.__compute_images(sat_path, 'sat')\n",
        "        seg_ids, seg_images = self.__compute_images(seg_path, 'seg')\n",
        "\n",
        "        grd_samples = {'imgs': grd_images, 'imgs_id': grd_ids}\n",
        "        sat_samples = {'imgs': sat_images, 'imgs_id': sat_ids}\n",
        "        seg_samples = {'imgs': seg_images, 'imgs_id': seg_ids}\n",
        "\n",
        "        return grd_samples, sat_samples, seg_samples\n",
        "\n",
        "\n",
        "    # we could add transformations (first of all normalization of the input!)\n",
        "    def __compute_images(self, paths, img_type):\n",
        "        images = []\n",
        "        ids = []\n",
        "\n",
        "        for img_path in paths:\n",
        "            img = Image.open(os.path.join(self.data_dir, img_path))\n",
        "            img = self.transform[img_type](img)\n",
        "            images.append(img)\n",
        "            ids.append(int(img_path[-11:-4]))\n",
        "\n",
        "        # Stack the image tensors along the batch dimension\n",
        "        images_tensor = torch.stack(images)\n",
        "        ids_tensor = torch.tensor(ids, dtype=int)\n",
        "        return ids_tensor, images_tensor\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def combine_segmap(image, segmap, sigma):\n",
        "        assert image.shape == segmap.shape\n",
        "        combined = sigma * image + (1-sigma) * segmap\n",
        "        return combined\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=True,num_workers=2)\n",
        "\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=False,num_workers=2)\n",
        "\n",
        "\n",
        "    #def test_dataloader(self):\n",
        "    #    return DataLoader(self.test_dataset,batch_size=self.batch_size, collate_fn=self.collate_fn,shuffle=True,num_workers=2)\n",
        "\n",
        "\n",
        "    def compute_mean_std(self):\n",
        "\n",
        "        sv_mean = np.array([0., 0., 0.])\n",
        "        sv_std = np.array([0., 0., 0.])\n",
        "\n",
        "        sm_mean = np.array([0., 0., 0.])\n",
        "        sm_std = np.array([0., 0., 0.])\n",
        "\n",
        "        # compute mean\n",
        "        for i in self.train_dataset.data:\n",
        "\n",
        "            sv_path = os.path.join(self.data_dir, i['grd_path'])\n",
        "            sv_img = cv2.imread(sv_path)\n",
        "            sv_img = cv2.cvtColor(sv_img, cv2.COLOR_BGR2RGB)\n",
        "            sv_img = sv_img.astype(float) / 255.\n",
        "            sv_mean += np.mean(sv_img[:,:,:], axis = (0,1))\n",
        "\n",
        "            sm_path = os.path.join(self.data_dir, i['sat_path'])\n",
        "            sm_img = cv2.imread(sm_path)\n",
        "            sm_img = cv2.cvtColor(sm_img, cv2.COLOR_BGR2RGB)\n",
        "            sm_img = sm_img.astype(float) / 255.\n",
        "            sm_mean += np.mean(sm_img[:,:,:], axis = (0,1))\n",
        "\n",
        "        sv_mean /= len(self.train_dataset.data)\n",
        "        sm_mean /= len(self.train_dataset.data)\n",
        "\n",
        "        # compute std\n",
        "        for i in self.train_dataset.data:\n",
        "\n",
        "            sv_path = os.path.join(self.data_dir, i['grd_path'])\n",
        "            sv_img = cv2.imread(sv_path)\n",
        "            sv_img = cv2.cvtColor(sv_img, cv2.COLOR_BGR2RGB)\n",
        "            sv_img = sv_img.astype(float) / 255.\n",
        "            sv_img_size = sv_img.shape[0] * sv_img.shape[1]\n",
        "            sv_std += ((sv_img[:,:,:] - sv_mean)**2).sum(axis = (0,1)) / sv_img_size\n",
        "\n",
        "            sm_path = os.path.join(self.data_dir, i['sat_path'])\n",
        "            sm_img = cv2.imread(sm_path)\n",
        "            sm_img = cv2.cvtColor(sm_img, cv2.COLOR_BGR2RGB)\n",
        "            sm_img = sm_img.astype(float) / 255.\n",
        "            sm_img_size = sm_img.shape[0] * sm_img.shape[1]\n",
        "            sm_std += ((sm_img[:,:,:] - sv_mean)**2).sum(axis = (0,1)) / sm_img_size\n",
        "\n",
        "        sv_std = np.sqrt(sv_std/len(self.train_dataset.data))\n",
        "        sm_std = np.sqrt(sm_std/len(self.train_dataset.data))\n",
        "\n",
        "        # does it make sense to normalize the segmap?\n",
        "\n",
        "        result = {'grd_mean': sv_mean, 'grd_std': sv_std,\n",
        "                  'sat_mean': sm_mean, 'sat_std': sm_std,\n",
        "                  'seg_mean': [0,0,0], 'seg_std': [1,1,1]\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def set_mean_std(self, mean_std):\n",
        "        self.mean['grd'] = mean_std['grd_mean']\n",
        "        self.mean['sat'] = mean_std['sat_mean']\n",
        "        self.mean['seg'] = mean_std['seg_mean']\n",
        "        self.std['grd'] = mean_std['grd_std']\n",
        "        self.std['sat'] = mean_std['sat_std']\n",
        "        self.std['seg'] = mean_std['seg_std']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkLiGNItQpOk",
        "outputId": "c0c6677c-3c4e-43fb-c764-3ec5422371e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CVUSA-Dataset-train: 6647 samples\n",
            "CVUSA-Dataset-val: 2215 samples\n"
          ]
        }
      ],
      "source": [
        "# @title Creating dataloaders\n",
        "\n",
        "input_dir = '/content/input'\n",
        "\n",
        "data_module = CVUSADataModule(\n",
        "    input_dir = input_dir,\n",
        "    batch_size = 64,\n",
        "    grd_resize = 64,\n",
        "    sat_resize = 128,\n",
        "    seg_resize = 128\n",
        ")\n",
        "\n",
        "# mean_std = data_module.compute_mean_std()\n",
        "mean_std = {\n",
        "    'grd_mean': [0.4691, 0.4821, 0.4603], 'grd_std': [0.2202, 0.2191, 0.2583],\n",
        "    'sat_mean': [0.3833 , 0.3964, 0.3434], 'sat_std': [0.2131, 0.2024, 0.2259],\n",
        "    'seg_mean': [0, 0, 0], 'seg_std': [1, 1, 1]\n",
        "}\n",
        "data_module.set_mean_std(mean_std)\n",
        "\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()\n",
        "#test_loader = data_module.test_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VmODtjKnRzH"
      },
      "source": [
        "#Losses and other utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation TripletLoss more stable\n",
        "\n",
        "# RIVEDI (confronto con triplet torchreid)\n",
        "\n",
        "class TripletLoss(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_weight = 0.3):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "\n",
        "    def forward(self, image_features1, image_features2):\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "        dist_array = 2.0 - 2.0 * torch.matmul(image_features2, image_features1.T)\n",
        "        n = len(image_features1)\n",
        "        pos_dist = torch.diag(dist_array)\n",
        "        pair_n = n * (n - 1.0)\n",
        "        triplet_dist_g2s = pos_dist - dist_array\n",
        "        loss_g2s = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_g2s * self.loss_weight)))/pair_n\n",
        "        triplet_dist_s2g = torch.unsqueeze(pos_dist, 1) - dist_array\n",
        "        loss_s2g = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_s2g * self.loss_weight)))/pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "kuwQsXRGMI0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title InfoNCE implementation\n",
        "class InfoNCE(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_function, logit_scale=3.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss_function = loss_function #we can use a generic loss function!\n",
        "        self.logit_scale = logit_scale\n",
        "\n",
        "\n",
        "    def forward(self, image_features1, image_features2):\n",
        "\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "\n",
        "        # use pairwise_cosine_similarity instead? it's the same?\n",
        "        logits_per_image1 = self.logit_scale * image_features1 @ image_features2.T\n",
        "        logits_per_image2 = logits_per_image1.T\n",
        "\n",
        "        labels = torch.arange(len(logits_per_image1), dtype=torch.long, device = device)\n",
        "        loss = (self.loss_function(logits_per_image1, labels) + self.loss_function(logits_per_image2, labels))/2\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ukTOSR3IgPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u6wnToFjtlF"
      },
      "outputs": [],
      "source": [
        "# @title Top-K Rank Accuracy: takes embeddings in input\n",
        "\n",
        "def top_k_rank_accuracy(emb1, emb2, k=1):\n",
        "\n",
        "    num_samples = len(emb1)\n",
        "\n",
        "    if k > num_samples :\n",
        "      return 0.0 # might happen at the end of the dataset (batch less then the chosen one)\n",
        "\n",
        "    # replace with pairwise_cosine?\n",
        "    emb1 = F.normalize(emb1, dim=-1)\n",
        "    emb2 = F.normalize(emb2, dim=-1)\n",
        "    dist_matrix = 1 - (emb1 @ emb2.T)\n",
        "\n",
        "    _, topk_indices = torch.topk(dist_matrix, k = k, dim = 1, largest = False)\n",
        "\n",
        "    correct_in_topk = sum([i in topk_indices[i, :] for i in range(num_samples)])\n",
        "\n",
        "    accuracy = correct_in_topk / num_samples\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation of Attention operator\n",
        "class Attention(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MvHm7d1ICYme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Branches"
      ],
      "metadata": {
        "id": "rXEBXiWchDAq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abAMZR3-HJv4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Resnet\n",
        "\n",
        "class ResNet50Branch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, output_dim):\n",
        "        super(ResNet50Branch, self).__init__()\n",
        "        self.output_dim = output_dim #so we can access from super-models\n",
        "        self.resnet50 = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n",
        "        # Modify the last layer for your specific task\n",
        "        self.resnet50.fc = torch.nn.Linear(self.resnet50.fc.in_features, self.output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps = False):\n",
        "\n",
        "        # to print the featuremap we need to return the last conv layer output\n",
        "        if featuremaps:\n",
        "            x = self.resnet50.conv1(x)\n",
        "            x = self.resnet50.bn1(x)\n",
        "            x = self.resnet50.relu(x)\n",
        "            x = self.resnet50.maxpool(x)\n",
        "            x = self.resnet50.layer1(x)\n",
        "            x = self.resnet50.layer2(x)\n",
        "            x = self.resnet50.layer3(x)\n",
        "            x = self.resnet50.layer4(x)\n",
        "            return x\n",
        "\n",
        "        else:\n",
        "            return self.resnet50(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SAIG\n",
        "\n",
        "\n",
        "class ConvBnReluBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(ConvBnReluBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "###\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_heads,\n",
        "        qkv_bias=False,\n",
        "        qk_scale=None,\n",
        "        drop=0.,\n",
        "        attn_drop=0.,\n",
        "        dropout=0.,\n",
        "        norm_layer=nn.LayerNorm\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop\n",
        "        )\n",
        "        # check what is droppath\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
        "        return x\n",
        "\n",
        "###\n",
        "\n",
        "class SAIGBranch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, img_size, patch_size=16, in_channels=3, embed_dim=768, num_heads = 8, depth = 4, smd_dim = 8, qkv_bias = True, qk_scale = None, drop_rate=0., attn_drop_rate=0., norm_layer=None, flatten=True):\n",
        "        super(SAIGBranch, self).__init__()\n",
        "        #potremmo salvare i parametri, ha qualche senso?\n",
        "        self.output_dim = embed_dim * smd_dim\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
        "\n",
        "        if img_size[0] % patch_size != 0 or img_size[1] % patch_size != 0:\n",
        "          print(\"Warning: image size is not divisible for patch size\")\n",
        "\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "        self.conv_bn_relu_blocks = nn.Sequential(\n",
        "            ConvBnReluBlock(in_channels = 3, out_channels = 64, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 64, out_channels = 128, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 128, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 256, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 256, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 512, stride = 2),\n",
        "        )\n",
        "        self.patch_block = nn.Conv2d(in_channels = 512, out_channels = embed_dim, kernel_size=1, stride=1 ,padding=0)\n",
        "        self.attn_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
        "        #self.pos_embed = nn.Parameter(torch.zeros(1, embed_dim, self.num_patches))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        #self.GAP = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        #self.logits = nn.Linear(in_features = embed_dim, out_features = 512)\n",
        "\n",
        "        self.smd = nn.Sequential(\n",
        "            nn.Linear(self.num_patches, self.num_patches*4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.num_patches*4, self.num_patches),\n",
        "            nn.Linear(self.num_patches, smd_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps = False):\n",
        "\n",
        "      # extract patch embeddings\n",
        "      x = self.conv_bn_relu_blocks(x)\n",
        "\n",
        "      if featuremaps:\n",
        "        return x\n",
        "\n",
        "      x = self.patch_block(x)\n",
        "      x = x.flatten(2).transpose(1,2)\n",
        "      #x = self.norm(x) CHECK\n",
        "\n",
        "      # add position embeddings\n",
        "      x = x + self.pos_embed\n",
        "      x = self.pos_drop(x)\n",
        "\n",
        "      # pass through sequence of attention blocks\n",
        "      for blk in self.attn_blocks:\n",
        "          x = blk(x)\n",
        "\n",
        "      x = self.norm(x)\n",
        "      # x = self.GAP(x.transpose(-1, -2)).squeeze(2)\n",
        "      # x = self.logits(x)\n",
        "\n",
        "      # if featuremaps:\n",
        "      #   return x.resize(x.shape[0], self.grid_size[0], self.grid_size[1], 384)\n",
        "\n",
        "      # x: b x 88 x 384\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = self.smd(x)\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = x.flatten(-2, -1)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "QDzQtqQOHzU-",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title VGG16\n",
        "\n",
        "class VGG_net(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, img_size, architecture, num_max_pooling, in_channels=3, output_dim=1000):\n",
        "        super(VGG_net, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.output_dim = output_dim\n",
        "        self.division = 2 ** num_max_pooling\n",
        "\n",
        "        self.conv_layers = self.create_conv_layers(architecture)\n",
        "\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(128 * img_size[0]//self.division * img_size[1]//self.division, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, output_dim),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps=False):\n",
        "        for layer in self.conv_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "            if featuremaps and x.shape[2] == 32:\n",
        "                return x\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fcs(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == int:\n",
        "                out_channels = x\n",
        "\n",
        "                layers += [\n",
        "                    nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "                in_channels = x\n",
        "            elif x == \"M\":\n",
        "                layers += [nn.MaxPool2d(2, 2)]\n",
        "\n",
        "        return nn.ModuleList(layers)"
      ],
      "metadata": {
        "id": "mTFXxK0cb51w",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title VGG16 - pretrained\n",
        "class VGGBranch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, output_dim):\n",
        "        super(VGGBranch, self).__init__()\n",
        "        self.output_dim = output_dim #so we can access from super-models\n",
        "        self.vgg = models.vgg16(pretrained=True)\n",
        "        # Modify the last layer for your specific task\n",
        "        num_features = self.vgg.classifier[-1].in_features\n",
        "        self.vgg.classifier[-1] = nn.Linear(num_features, self.output_dim)\n",
        "\n",
        "    def forward(self, x, featuremaps=False):\n",
        "\n",
        "        # to print the featuremap we need to return the last conv layer output\n",
        "        if featuremaps:\n",
        "            features = self.vgg.features(x)\n",
        "            return features\n",
        "\n",
        "        else:\n",
        "            return self.vgg(x)\n"
      ],
      "metadata": {
        "id": "ARF98Rb2USX6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouQUwE2nWua"
      },
      "source": [
        "# Dual Model (RGB Grd | RGB Sat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dual Model\n",
        "\n",
        "class DualModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_grd, model_sat, loss = InfoNCE(loss_function=nn.CrossEntropyLoss())):\n",
        "        super(DualModel, self).__init__()\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_sat\n",
        "\n",
        "        if self.branch1.output_dim != self.branch2.output_dim:\n",
        "          raise ValueError(\"ATTENTION, MISMATCHING OUTPUT DIMENSIONS FOR THE BRANCHES!\")\n",
        "\n",
        "        self.output_dim = self.branch1.output_dim\n",
        "\n",
        "        self.loss = loss\n",
        "\n",
        "        #train\n",
        "        #self.grd_features_train =torch.empty((0, self.output_dim), device = device)\n",
        "        #self.sat_features_train =torch.empty((0, self.output_dim), device = device)\n",
        "\n",
        "        #validation\n",
        "        self.grd_features_val =  []\n",
        "        self.sat_features_val =  []\n",
        "        #test\n",
        "        self.grd_features_test = []\n",
        "        self.sat_features_test = []\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.branch1(x1['imgs'])\n",
        "        out2 = self.branch2(x2['imgs'])\n",
        "        return out1, out2\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        grd_img, sat_img, _ = batch\n",
        "        out1, out2 = self(grd_img, sat_img)\n",
        "\n",
        "        loss = self.loss(out1, out2)\n",
        "\n",
        "        top_1 = top_k_rank_accuracy(out1, out2, k=1)\n",
        "\n",
        "        self.log('train_top1', top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        grd_img, sat_img, _ = batch\n",
        "        out1, out2 = self(grd_img, sat_img)\n",
        "\n",
        "        self.grd_features_val.append(out1)\n",
        "        self.sat_features_val.append(out2)\n",
        "\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "\n",
        "      grd_features_val = torch.cat(self.grd_features_val, dim=0)\n",
        "      sat_features_val = torch.cat(self.sat_features_val, dim=0)\n",
        "\n",
        "      num_samples = grd_features_val.shape[0]\n",
        "      percent1 = int(0.01*num_samples)\n",
        "\n",
        "      top_1 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=1)\n",
        "      top_3 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=3)\n",
        "      top_10 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=10)\n",
        "      top_percent1 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=percent1)\n",
        "\n",
        "      self.log('val_top1', top_1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top3', top_3, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top10', top_10, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top1%', top_percent1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "      #we deallocate the memory!\n",
        "\n",
        "      #validation\n",
        "      self.grd_features_val.clear()\n",
        "      self.sat_features_val.clear()\n",
        "\n",
        "      del grd_features_val, sat_features_val\n",
        "\n",
        "      return top_1, top_3, top_10, top_percent1\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        grd_img, sat_img, _ = batch\n",
        "        out1, out2 = self(grd_img, sat_img)\n",
        "\n",
        "        self.grd_features_test.append(out1)\n",
        "        self.sat_features_test.append(out2)\n",
        "\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "\n",
        "      grd_features_test = torch.cat(self.grd_features_test, dim = 0)\n",
        "      sat_features_test = torch.cat(self.sat_features_test, dim = 0)\n",
        "\n",
        "      num_samples = grd_features_test.shape[0]\n",
        "      percent1 = int(0.01*num_samples)\n",
        "\n",
        "      top_1 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=1)\n",
        "      top_3 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=3)\n",
        "      top_10 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=10)\n",
        "      top_percent1 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=percent1)\n",
        "\n",
        "      self.log('test_top1', top_1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top3', top_3, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top10', top_10, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top1%', top_percent1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "      #we deallocate the memory!\n",
        "\n",
        "      self.grd_features_test.clear()\n",
        "      self.sat_features_test.clear()\n",
        "\n",
        "      del grd_features_test, sat_features_test\n",
        "\n",
        "      return top_1, top_3, top_10, top_percent1\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Sf7PT3N_ubhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Model\n",
        "\n",
        "branch_type = 'resnet'\n",
        "\n",
        "###\n",
        "\n",
        "if branch_type == 'resnet':\n",
        "\n",
        "    grd_model = ResNet50Branch(128)\n",
        "    sat_model = ResNet50Branch(128)\n",
        "\n",
        "\n",
        "elif branch_type == 'saig':\n",
        "\n",
        "    grd_model = SAIGBranch(\n",
        "        data_module.size['grd'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8\n",
        "    )\n",
        "\n",
        "    sat_model = SAIGBranch(\n",
        "        data_module.size['sat'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8\n",
        "    )\n",
        "\n",
        "\n",
        "elif branch_type == 'vgg':\n",
        "\n",
        "    # Output channel of each layer in the convolution layers\n",
        "    # \"M\" stands for maxpooling layer\n",
        "    #VGG16 = [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"]\n",
        "    #VGG16 = [32, 32, \"M\", 64, 64, \"M\", 128, 128, 128, \"M\", 256, 256, 256, \"M\", 256, 256, 256, \"M\"]\n",
        "    VGG16 = [16, 16, \"M\", 32, 32, \"M\", 64, 64, 64, \"M\", 128, 128, 128, \"M\", 128, 128, 128, \"M\"]\n",
        "\n",
        "    grd_model = VGG_net(data_module.size['grd'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "    sat_model = VGG_net(data_module.size['sat'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "\n",
        "###\n",
        "\n",
        "model = DualModel(grd_model, sat_model)"
      ],
      "metadata": {
        "id": "fs24fr2ZwAxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e174ed-14a0-4420-97d1-3058179cc9fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 166MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVD0l4tD4Byc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5dae57d-56eb-4d4f-88a9-306d07614da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "# @title Create Trainer\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar()],\n",
        "    log_every_n_steps = 3,\n",
        "    default_root_dir = \"/content/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p57rNWjHN1hh"
      },
      "outputs": [],
      "source": [
        "# @title Train\n",
        "\n",
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dubja7aKNutk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316,
          "referenced_widgets": [
            "3fa12981c53f445ba00fe714f02d8fea",
            "d5b5a808b920452194628ce76a0f7870"
          ]
        },
        "outputId": "81b93ecb-000c-40c2-e227-572bc99be5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fa12981c53f445ba00fe714f02d8fea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "35\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">35\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torch.Size([2215, 128])\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.Size([2215, 128])\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "35\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">35\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top1        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.007223476190119982   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top1%        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_top10        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.05462753772735596   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_top3        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.016704289242625237   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top1         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.007223476190119982    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top1%         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_top10         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.05462753772735596    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_top3         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.016704289242625237    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test_top1': 0.007223476190119982,\n",
              "  'test_top3': 0.016704289242625237,\n",
              "  'test_top10': 0.05462753772735596,\n",
              "  'test_top1%': 0.0}]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "# @title Test\n",
        "\n",
        "trainer.test(\n",
        "    model = model,\n",
        "    dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triple Model (RGB Grd | RGB + SEG Sat)"
      ],
      "metadata": {
        "id": "ZGDT8x2WPpbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TripleModel\n",
        "\n",
        "class TripleModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, model_grd, model_sat, model_seg, loss = InfoNCE(loss_function=nn.CrossEntropyLoss())):\n",
        "        super(TripleModel, self).__init__()\n",
        "        self.branch1 = model_grd\n",
        "        self.branch2 = model_sat\n",
        "        self.branch3 = model_seg\n",
        "\n",
        "        #verify that grd output dim is coherent with the sum of the other two models\n",
        "        if self.branch1.output_dim != (self.branch2.output_dim + self.branch3.output_dim):\n",
        "          raise ValueError(\"ATTENTION, MISMATCHING OUTPUT DIMENSIONS FOR THE BRANCHES! Must have output_dim1 = output_dim2+output_dim3\")\n",
        "\n",
        "        self.output_dim = self.branch1.output_dim\n",
        "\n",
        "        self.loss = loss\n",
        "\n",
        "        #train\n",
        "        #self.grd_features_train =torch.empty((0, self.output_dim), device = device)\n",
        "        #self.sat_features_train =torch.empty((0, self.output_dim), device = device)\n",
        "        #self.seg_features_train =torch.empty((0, self.output_dim), device = device)\n",
        "\n",
        "        #validation\n",
        "        self.grd_features_val = []\n",
        "        self.sat_features_val = [] #IMPORTANTE: SAT é SIA SAT CHE SEG!!!! LE COMBINIAMO!\n",
        "\n",
        "        #test\n",
        "        self.grd_features_test = []\n",
        "        self.sat_features_test = []\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2, x3):\n",
        "        out1 = self.branch1(x1['imgs'])\n",
        "        out2 = self.branch2(x2['imgs'])\n",
        "        out3 = self.branch3(x3['imgs'])\n",
        "        out2 = torch.cat((out2, out3), dim=1) #concatenation of features\n",
        "        return out1, out2\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        grd_img, sat_img, seg_img = batch\n",
        "        out1, out2 = self(grd_img, sat_img, seg_img)\n",
        "\n",
        "        loss = self.loss(out1, out2)\n",
        "\n",
        "        top_1 = top_k_rank_accuracy(out1, out2, k=1)\n",
        "\n",
        "        self.log('train_top1', top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        grd_img, sat_img, seg_img = batch\n",
        "        out1, out2 = self(grd_img, sat_img, seg_img)\n",
        "\n",
        "        self.grd_features_val.append(out1)\n",
        "        self.sat_features_val.append(out2)\n",
        "\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "\n",
        "      grd_features_val = torch.cat(self.grd_features_val, dim=0)\n",
        "      sat_features_val = torch.cat(self.sat_features_val, dim=0)\n",
        "\n",
        "      num_samples = grd_features_val.shape[0]\n",
        "      percent1 = int(0.01*num_samples)\n",
        "\n",
        "      top_1 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=1)\n",
        "      top_3 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=3)\n",
        "      top_10 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=10)\n",
        "      top_percent1 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=percent1)\n",
        "\n",
        "      self.log('val_top1', top_1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top3', top_3, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top10', top_10, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top1%', top_percent1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "      #we deallocate the memory!\n",
        "\n",
        "      #validation\n",
        "      self.grd_features_val.clear()\n",
        "      self.sat_features_val.clear()\n",
        "\n",
        "      del grd_features_val, sat_features_val\n",
        "\n",
        "      return top_1, top_3, top_10, top_percent1\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        grd_img, sat_img, seg_img = batch\n",
        "        out1, out2 = self(grd_img, sat_img, seg_img)\n",
        "\n",
        "        self.grd_features_test.append(out1)\n",
        "        self.sat_features_test.append(out2)\n",
        "\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "\n",
        "      grd_features_test = torch.cat(self.grd_features_test, dim = 0)\n",
        "      sat_features_test = torch.cat(self.sat_features_test, dim = 0)\n",
        "\n",
        "      num_samples = grd_features_test.shape[0]\n",
        "      percent1 = int(0.01*num_samples)\n",
        "\n",
        "      top_1 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=1)\n",
        "      top_3 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=3)\n",
        "      top_10 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=10)\n",
        "      top_percent1 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=percent1)\n",
        "\n",
        "      self.log('test_top1', top_1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top3', top_3, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top10', top_10, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top1%', top_percent1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "      #we deallocate the memory!\n",
        "\n",
        "      self.grd_features_test.clear()\n",
        "      self.sat_features_test.clear()\n",
        "\n",
        "      del grd_features_test, sat_features_test\n",
        "\n",
        "      return top_1, top_3, top_10, top_percent1\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "UWIiTSLyBc8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create Model\n",
        "\n",
        "grd_model = SAIGBranch(data_module.size['grd'], embed_dim=512)\n",
        "sat_model = SAIGBranch(data_module.size['sat'], embed_dim=256) #deals directly by itself with resizing\n",
        "seg_model = SAIGBranch(data_module.size['seg'], embed_dim=256) #deals directly by itself with resizing\n",
        "\n",
        "\n",
        "model = TripleModel(grd_model, sat_model, seg_model)"
      ],
      "metadata": {
        "id": "gG7fYg6cSrji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar()],\n",
        "    log_every_n_steps = 3\n",
        ")"
      ],
      "metadata": {
        "id": "iHcC07AUUhtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce86107-9c1d-496e-995f-1b8d94d4af71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader\n",
        ")"
      ],
      "metadata": {
        "id": "OWIVZLEGUlJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406,
          "referenced_widgets": [
            "2e708a80d995466ca260c87737672724",
            "ca25f57475164f97b659c09fd477dc9d"
          ]
        },
        "outputId": "33ae5041-e130-454a-e816-7288e87088f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ branch1 │ SAIGBranch │  6.9 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ branch2 │ SAIGBranch │  3.5 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ branch3 │ SAIGBranch │  3.5 M │\n",
              "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ loss    │ InfoNCE    │      0 │\n",
              "└───┴─────────┴────────────┴────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
              "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
              "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ branch1 │ SAIGBranch │  6.9 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ branch2 │ SAIGBranch │  3.5 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ branch3 │ SAIGBranch │  3.5 M │\n",
              "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ loss    │ InfoNCE    │      0 │\n",
              "└───┴─────────┴────────────┴────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 13.9 M                                                                                           \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 13.9 M                                                                                               \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 55                                                                         \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 13.9 M                                                                                           \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 13.9 M                                                                                               \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 55                                                                         \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e708a80d995466ca260c87737672724"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a \n",
              "cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: \n",
              "CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
              "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a \n",
              "cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: \n",
              "CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
              "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.test(\n",
        "    dataloaders = val_loader\n",
        ")"
      ],
      "metadata": {
        "id": "7-d3ympNUoWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization Functions"
      ],
      "metadata": {
        "id": "gJAKm9G3-Kwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize Heatmap\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def create_activation_maps(\n",
        "    model,\n",
        "    data_module,\n",
        "    split,\n",
        "    save_dir,\n",
        "    use_gpu = True\n",
        "):\n",
        "\n",
        "    model.eval()\n",
        "    spacing = 10\n",
        "    fading = 0.5\n",
        "\n",
        "    if split == 'train':\n",
        "        data_loader = data_module.train_dataloader()\n",
        "    elif split == 'test':\n",
        "        data_loader = data_module.test_dataloader()\n",
        "    elif split == 'val':\n",
        "        data_loader = data_module.val_dataloader()\n",
        "    else:\n",
        "        raise ValueError('split should be \"train\", \"test\" or \"val\"')\n",
        "\n",
        "    grd_mean = data_module.mean['grd']\n",
        "    grd_std = data_module.std['grd']\n",
        "    sat_mean = data_module.mean['sat']\n",
        "    sat_std = data_module.std['sat']\n",
        "\n",
        "    grd_height, grd_width = data_module.size['grd']\n",
        "    sat_height, sat_width = data_module.size['sat']\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        grd_imgs, grd_ids = batch[0]['imgs'], batch[0]['imgs_id']\n",
        "        sat_imgs, sat_ids = batch[1]['imgs'], batch[1]['imgs_id']\n",
        "\n",
        "        if use_gpu:\n",
        "            grd_imgs = grd_imgs.cuda()\n",
        "            sat_imgs = sat_imgs.cuda()\n",
        "\n",
        "        grd_output = model.branch1(grd_imgs, featuremaps=True)\n",
        "        sat_output = model.branch2(sat_imgs, featuremaps=True)\n",
        "\n",
        "        # compute activation maps for streetview (try adding square root?)\n",
        "        grd_output = (grd_output**2).sum(1)\n",
        "        b, h, w = grd_output.size()\n",
        "        grd_output = grd_output.view(b, h * w)\n",
        "        grd_output = nn.functional.normalize(grd_output, p=2, dim=1)\n",
        "        grd_output = grd_output.view(b, h, w)\n",
        "\n",
        "        # compute activation maps for satmap\n",
        "        sat_output = (sat_output**2).sum(1)\n",
        "        b, h, w = sat_output.size()\n",
        "        sat_output = sat_output.view(b, h * w)\n",
        "        sat_output = nn.functional.normalize(sat_output, p=2, dim=1)\n",
        "        sat_output = sat_output.view(b, h, w)\n",
        "\n",
        "        if use_gpu:\n",
        "            grd_imgs, grd_output = grd_imgs.cpu(), grd_output.cpu()\n",
        "            sat_imgs, sat_output = sat_imgs.cpu(), sat_output.cpu()\n",
        "\n",
        "        for index in range(grd_output.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            img_id = str(int(grd_ids[index])).zfill(7)\n",
        "\n",
        "            # RGB image (from the normalized input image)\n",
        "            input_img = grd_imgs[index, ...]\n",
        "            for img, mean, std in zip(input_img, grd_mean, grd_std):\n",
        "                img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "            input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "            input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "            # activation map (from the output image)\n",
        "            act_map = grd_output[index, ...].numpy()\n",
        "            act_map = cv2.resize(act_map, (grd_width, grd_height))\n",
        "            act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "            act_map = np.uint8(np.floor(act_map))\n",
        "            act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapping between the two images\n",
        "            overlapped_img = input_img*(1-fading) + act_map*fading\n",
        "            overlapped_img[overlapped_img > 255] = 255\n",
        "            overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            output_img = 255 * np.ones((3*grd_height + 2*spacing, grd_width, 3), dtype=np.uint8)\n",
        "            output_img[:grd_height, ...] = input_img[..., ::-1]\n",
        "            output_img[grd_height + spacing:2*grd_height + spacing, ...] = act_map\n",
        "            output_img[2*grd_height + 2*spacing:, ...] = overlapped_img\n",
        "            cv2.imwrite(os.path.join(save_dir, img_id + '_streetview.jpg'), output_img)\n",
        "\n",
        "        for index in range(sat_output.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            img_id = str(int(sat_ids[index])).zfill(7)\n",
        "\n",
        "            # RGB image (input image)\n",
        "            input_img = sat_imgs[index, ...]\n",
        "            for img, mean, std in zip(input_img, sat_mean, sat_std):\n",
        "                img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "            input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "            input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "            # activation map\n",
        "            act_map = sat_output[index, ...].numpy()\n",
        "            act_map = cv2.resize(act_map, (sat_width, sat_height))\n",
        "            act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "            act_map = np.uint8(np.floor(act_map))\n",
        "            act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapped image\n",
        "            overlapped_img = input_img*(1-fading) + act_map*(fading)\n",
        "            overlapped_img[overlapped_img > 255] = 255\n",
        "            overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            output_img = 255 * np.ones((3*sat_height + 2*spacing, sat_width, 3), dtype=np.uint8)\n",
        "            output_img[:sat_height, ...] = input_img[..., ::-1]\n",
        "            output_img[sat_height + spacing:2*sat_height + spacing, ...] = act_map\n",
        "            output_img[2*sat_height + 2*spacing:, ...] = overlapped_img\n",
        "            cv2.imwrite(os.path.join(save_dir, img_id + '_satmap.jpg'), output_img)"
      ],
      "metadata": {
        "id": "36uMaOGy-NFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Visualize Ranked Results\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_ranked_results(\n",
        "    model,\n",
        "    data_module,\n",
        "    split,\n",
        "    save_dir,\n",
        "    top_k = 5,\n",
        "    use_gpu = True\n",
        "):\n",
        "\n",
        "    spacing = 10\n",
        "    query_spacing = 30\n",
        "    border = 5\n",
        "    text_space = 30\n",
        "\n",
        "    # select dataloader\n",
        "    if split == 'train':\n",
        "        data_loader = data_module.train_dataloader()\n",
        "    elif split == 'test':\n",
        "        data_loader = data_module.test_dataloader()\n",
        "    elif split == 'val':\n",
        "        data_loader = data_module.val_dataloader()\n",
        "    else:\n",
        "        raise ValueError('split should be \"train\", \"test\" or \"val\"')\n",
        "\n",
        "    # (using data module dimensions)\n",
        "    grd_height, grd_width = data_module.original_size['grd']\n",
        "    sat_height, sat_width = data_module.original_size['sat']\n",
        "\n",
        "    grd_ids = np.empty((0))\n",
        "    sat_ids = np.empty((0))\n",
        "\n",
        "    model.grd_features_test = []\n",
        "    model.sat_features_test = []\n",
        "\n",
        "    # compute features for each image\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        grd_ids = np.concatenate((grd_ids, batch[0]['imgs_id']))\n",
        "        sat_ids = np.concatenate((sat_ids, batch[0]['imgs_id']))\n",
        "\n",
        "        if use_gpu:\n",
        "            batch[0]['imgs'] = batch[0]['imgs'].cuda()\n",
        "            batch[1]['imgs'] = batch[1]['imgs'].cuda()\n",
        "            batch[2]['imgs'] = batch[2]['imgs'].cuda()\n",
        "\n",
        "        model.test_step(batch, batch_idx)\n",
        "\n",
        "        if use_gpu:\n",
        "            batch[0]['imgs'] = batch[0]['imgs'].cpu()\n",
        "            batch[1]['imgs'] = batch[1]['imgs'].cpu()\n",
        "            batch[2]['imgs'] = batch[2]['imgs'].cpu()\n",
        "\n",
        "    grd_features = torch.cat(model.grd_features_test, dim = 0)\n",
        "    sat_features = torch.cat(model.sat_features_test, dim = 0)\n",
        "\n",
        "    if use_gpu:\n",
        "        grd_features = grd_features.cpu()\n",
        "        sat_features = sat_features.cpu()\n",
        "\n",
        "    num_samples = grd_features.shape[0]\n",
        "\n",
        "    model.grd_features = []\n",
        "    model.sat_features = []\n",
        "\n",
        "    # compute distance matrix\n",
        "    grd_features = F.normalize(grd_features, dim=-1)\n",
        "    sat_features = F.normalize(sat_features, dim=-1)\n",
        "    dist_matrix = 1 - grd_features @ sat_features.T\n",
        "    indices = np.argsort(dist_matrix, axis=1)\n",
        "\n",
        "    for grd_index in range(num_samples):\n",
        "\n",
        "        # create empty output image\n",
        "        output_height = grd_height + query_spacing + sat_height + text_space\n",
        "        output_width = max(grd_width, top_k*sat_width + (top_k-1)*spacing)\n",
        "        output_img = 255 * np.ones((output_height, output_width, 3), dtype=np.uint8)\n",
        "\n",
        "        # create query image with black border\n",
        "        grd_id = str(int(grd_ids[grd_index])).zfill(7)\n",
        "        grd_path = data_module.data_dir + '/streetview/' + grd_id + '.jpg'\n",
        "        color = (0,0,0)\n",
        "        grd_img = cv2.imread(grd_path)\n",
        "        grd_img = cv2.resize(grd_img, (grd_width, grd_height))\n",
        "        grd_img = cv2.copyMakeBorder(grd_img, border, border, border, border, cv2.BORDER_CONSTANT, value = color)\n",
        "        grd_img = cv2.resize(grd_img, (grd_width, grd_height))\n",
        "\n",
        "        # add query image to the output\n",
        "        start_width = (output_width - grd_width) // 2\n",
        "        end_width = start_width + grd_width\n",
        "        output_img[:grd_height, start_width:end_width, :] = grd_img\n",
        "\n",
        "        rank = 1\n",
        "        for sat_index in indices[grd_index, :]:\n",
        "\n",
        "            # create ranked image with red or green border\n",
        "            sat_id = str(int(sat_ids[sat_index])).zfill(7)\n",
        "            sat_path = data_module.data_dir + '/bingmap/input' + sat_id + '.png'\n",
        "            color = (0, 255, 0) if (grd_id == sat_id) else (0, 0, 255)\n",
        "            sat_img = cv2.imread(sat_path)\n",
        "            sat_img = cv2.resize(sat_img, (sat_width, sat_height))\n",
        "            sat_img = cv2.copyMakeBorder(sat_img, border, border, border, border, cv2.BORDER_CONSTANT, value = color)\n",
        "            sat_img = cv2.resize(sat_img, (sat_width, sat_height))\n",
        "\n",
        "            # add ranked image to the output\n",
        "            start_height = grd_height+query_spacing\n",
        "            end_height = start_height + sat_height\n",
        "            start_width = (rank-1) * (sat_width + spacing)\n",
        "            end_width = start_width + sat_width\n",
        "            output_img[start_height:end_height, start_width:end_width, :] = sat_img\n",
        "\n",
        "            # add text about the ranked image\n",
        "            text = \"Rank: {} Distance:{:.4f}\".format(rank, dist_matrix[grd_index, sat_index])\n",
        "            bottom_left = (start_width + 10, output_height - 5)\n",
        "            cv2.putText(output_img, text, bottom_left, cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 1, 2)\n",
        "\n",
        "            rank += 1\n",
        "            if rank > top_k:\n",
        "                break\n",
        "\n",
        "        # create output image file\n",
        "        cv2.imwrite(os.path.join(save_dir, grd_id + '_visrank.jpg'), output_img)"
      ],
      "metadata": {
        "id": "xRMabKiQQffN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f -r '/content/output/log/activation_maps'\n",
        "!mkdir '/content/output/log/activation_maps'\n",
        "\n",
        "create_activation_maps(\n",
        "    model = model.to(device),\n",
        "    data_module = data_module,\n",
        "    split = 'val',\n",
        "    save_dir = '/content/output/log/activation_maps',\n",
        "    use_gpu = True\n",
        ")"
      ],
      "metadata": {
        "id": "5dSp6VMiKNAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -f -r '/content/output/log/ranked_results'\n",
        "!mkdir '/content/output/log/ranked_results'\n",
        "\n",
        "visualize_ranked_results(\n",
        "    model = model.to(device),\n",
        "    data_module = data_module,\n",
        "    split = 'val',\n",
        "    save_dir = '/content/output/log/ranked_results',\n",
        "    top_k = 5,\n",
        "    use_gpu = True\n",
        ")"
      ],
      "metadata": {
        "id": "buh9sdrgGU-B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "y1FRSut8m-Mv",
        "5VmODtjKnRzH",
        "RouQUwE2nWua"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3fa12981c53f445ba00fe714f02d8fea": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d5b5a808b920452194628ce76a0f7870",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m35/35\u001b[0m \u001b[38;5;245m0:00:33 • 0:00:00\u001b[0m \u001b[38;5;249m1.01it/s\u001b[0m  \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">35/35</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:33 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.01it/s</span>  \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "d5b5a808b920452194628ce76a0f7870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e708a80d995466ca260c87737672724": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ca25f57475164f97b659c09fd477dc9d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Epoch 2/29 \u001b[38;2;98;6;224m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m10/104\u001b[0m \u001b[38;5;245m0:00:17 • 0:02:10\u001b[0m \u001b[38;5;249m0.73it/s\u001b[0m \u001b[37mv_num: 11.000 train_top1_step:     \u001b[0m\n                                                                                \u001b[37m0.172 train_loss_step: 3.030       \u001b[0m\n                                                                                \u001b[37mval_top1: 0.011 val_top3: 0.032    \u001b[0m\n                                                                                \u001b[37mval_top10: 0.086 val_top1%: 0.151  \u001b[0m\n                                                                                \u001b[37mtrain_top1_epoch: 0.176            \u001b[0m\n                                                                                \u001b[37mtrain_loss_epoch: 3.265            \u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 2/29 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">10/104</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:17 • 0:02:10</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.73it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 11.000 train_top1_step:     </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.172 train_loss_step: 3.030       </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top1: 0.011 val_top3: 0.032    </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top10: 0.086 val_top1%: 0.151  </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_top1_epoch: 0.176            </span>\n                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_epoch: 3.265            </span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "ca25f57475164f97b659c09fd477dc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}