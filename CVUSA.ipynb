{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaMule/Computer-Vision/blob/main/CVUSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FRSut8m-Mv"
      },
      "source": [
        "#Import and installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bMqOyt0ONvDV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title Installing dependencies\n",
        "!pip install tqdm\n",
        "!pip install pytorch_lightning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBFZKHodQUoQ"
      },
      "outputs": [],
      "source": [
        "# @title Importing libraries\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler,SequentialSampler,RandomSampler\n",
        "from torch import nn\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar, RichProgressBar, ModelPruning\n",
        "import torch.nn.functional as F\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from pytorch_lightning import loggers\n",
        "from torch.utils.data.sampler import BatchSampler\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from torchvision import transforms\n",
        "import shutil\n",
        "import csv\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5fKgrevQXPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc6753cc-46ac-484a-b8cb-6bc31e78faf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeSyWSOA8YeO"
      },
      "outputs": [],
      "source": [
        "#!gdown --id 17W9VEPMneRlb6igtSxa--Xh4fSZs3RS_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpGlkKCU8czs"
      },
      "outputs": [],
      "source": [
        "#!unrar x -o+ -v CVUSA_subset.rar | grep -c \"^Extracting\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ESVTpqm1gq"
      },
      "source": [
        "# To be removed (later) REMEMBER: YOU NOW USE POLAR TRANSFORM!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-X6fVs1ZoIH"
      },
      "outputs": [],
      "source": [
        "# # @title Unzipping the .rar dataset: code used in creation of dataset\n",
        "# import os\n",
        "# from rarfile import RarFile\n",
        "\n",
        "# rar_file_path = 'drive/MyDrive/CV Project/CVUSA_subset.rar'\n",
        "# # Open the .rar file\n",
        "# with RarFile(rar_file_path, 'r') as rar:\n",
        "#     # Get the list of files in the archive\n",
        "#     file_list = rar.namelist()\n",
        "\n",
        "#     # Iterate through the files and extract them\n",
        "#     for file_name in tqdm(file_list, desc=\"Extracting\", unit=\"files\"):\n",
        "#         # Extract the file to the current directory\n",
        "#         rar.extract(file_name, path='drive/MyDrive/CV Project/CVUSA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxJX3qwAvTrS"
      },
      "outputs": [],
      "source": [
        "# # @title Code for transforming csv files (originally quite wrong): code used in creation of dataset\n",
        "# import pandas as pd\n",
        "\n",
        "# train_csv = \"drive/MyDrive/CV Project/CVUSA/train-19zl.csv\"\n",
        "# val_csv = \"drive/MyDrive/CV Project/CVUSA/val-19zl.csv\"\n",
        "# train_output_csv = \"drive/MyDrive/CV Project/CVUSA/train.csv\"\n",
        "# val_output_csv = \"drive/MyDrive/CV Project/CVUSA/val.csv\"\n",
        "\n",
        "# def generate_new_csv(csv_path, out_csv_path):\n",
        "#   # Read the CSV file\n",
        "#   df = pd.read_csv(csv_path, header=None)\n",
        "\n",
        "#   # Rename columns\n",
        "#   df.rename(columns={0: 'col1', 1: 'col2', 2: 'col3'}, inplace=True)\n",
        "\n",
        "#   # Change .png to .jpg in col2\n",
        "#   df['col2'] = df['col2'].str.replace('.png', '.jpg').str.replace('input', '')\n",
        "\n",
        "#   # Change 'streetview' to 'segmap' and 'input' to 'output' in col3\n",
        "#   df['col3'] = df['col3'].str.replace('streetview', 'segmap').str.replace('input', 'output')\n",
        "\n",
        "#   # Add new columns\n",
        "#   df['col4'] = 'polarmap/' + df['col1'].str.replace('bingmap', 'normal')\n",
        "#   df['col5'] = 'polarmap/' + df['col3']\n",
        "\n",
        "#   # Add header\n",
        "#   header = [\"bingmap\", \"streetview\", \"segmap\", \"polarmap/input\", \"polarmap/segmap\"]\n",
        "#   df.columns = header\n",
        "\n",
        "#   # Save the modified dataframe to a new CSV file\n",
        "#   df.to_csv(out_csv_path, index=False)\n",
        "\n",
        "# generate_new_csv(train_csv, train_output_csv)\n",
        "# generate_new_csv(val_csv, val_output_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDSCF0zinL8Y"
      },
      "source": [
        "#Dataset and DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek7SUbjQQgF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e9177f-fb6b-41be-85ba-a4358775a0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "113Se08XQh30"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UnwPoYpJd6"
      },
      "source": [
        "We need to also possibly add polar and segmap! not done right now because there is a problem with the csv files. Also no test set, should we use validation or split the training and use the current validation as test?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT1pbNEXQknc"
      },
      "outputs": [],
      "source": [
        "# @title Dataset definition: without using polar transforms (neither segmentation)\n",
        "class CVUSADataset(Dataset):\n",
        "    def __init__(self, data_dir, split = 'train'):\n",
        "\n",
        "        self.split = split\n",
        "        self.data = self.load_data(data_dir + f'/{split}.csv')\n",
        "\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        data = []\n",
        "        with open(csv_path, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader) #skip header\n",
        "            for row in csv_reader:\n",
        "                bingmap = row[3]\n",
        "                streetview = row[1]\n",
        "                #segmap = row[2]\n",
        "                data.append({\"streetview\": streetview, \"bingmap\": bingmap})\n",
        "\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dictionary = self.data[index]\n",
        "        streetview = dictionary['streetview']\n",
        "        bingmap = dictionary['bingmap']\n",
        "        return streetview, bingmap\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CVUSA-Dataset-{self.split}: {len(self.data)} samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj0ecN-1QnVq"
      },
      "outputs": [],
      "source": [
        "# @title Data module definition: without using polar transforms (neither segmentation)\n",
        "class CVUSADataModule(pl.LightningDataModule):\n",
        "    def __init__(self, data_dir, batch_size=8, resize = True):\n",
        "\n",
        "        # Initialize the CustomDataModule\n",
        "\n",
        "        super(CVUSADataModule, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.data_dir = data_dir\n",
        "        self.resize = resize\n",
        "    def setup(self, stage=None):\n",
        "\n",
        "        # Load the entire dataset\n",
        "        self.train_dataset = CVUSADataset(data_dir=self.data_dir, split='train')\n",
        "        print(self.train_dataset)\n",
        "\n",
        "        #self.test_dataset = CVUSADataset(data_dir=self.data_dir, split='test')\n",
        "        #print(self.test_dataset)\n",
        "\n",
        "        self.val_dataset = CVUSADataset(data_dir=self.data_dir, split='val')\n",
        "        print(self.val_dataset)\n",
        "\n",
        "    #collate function is useful so we don't overuse RAM, training is a little bit slower tho...\n",
        "    def collate_fn(self,batch):\n",
        "        streetview_path, bingmap_path = zip(*batch)\n",
        "        # Load and transform each image in the batch\n",
        "        streetview_images_tensor = self.__images_to_tensor(streetview_path)\n",
        "\n",
        "        bingmap_images_tensor = self.__images_to_tensor(bingmap_path)\n",
        "\n",
        "        return streetview_images_tensor, bingmap_images_tensor\n",
        "\n",
        "    #we could add transformations (first of all normalization of the input!)\n",
        "    def __images_to_tensor(self, paths):\n",
        "        images = []\n",
        "        for img_path in paths:\n",
        "            img = Image.open(os.path.join(self.data_dir, img_path))\n",
        "            if self.resize:\n",
        "              img = transforms.Resize((64))(img)\n",
        "\n",
        "            img_tensor = transforms.ToTensor()(img)\n",
        "            images.append(img_tensor)\n",
        "\n",
        "        # Stack the image tensors along the batch dimension\n",
        "        images_tensor = torch.stack(images)\n",
        "\n",
        "        return images_tensor\n",
        "\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=True,num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset,batch_size=self.batch_size,collate_fn=self.collate_fn,shuffle=False,num_workers=2)\n",
        "\n",
        "    #def test_dataloader(self):\n",
        "    #    return DataLoader(self.test_dataset,batch_size=self.batch_size, collate_fn=self.collate_fn,shuffle=True,num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkLiGNItQpOk",
        "outputId": "b492985c-9fe2-4724-d467-053420b0e300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CVUSA-Dataset-train: 6647 samples\n",
            "CVUSA-Dataset-val: 2215 samples\n"
          ]
        }
      ],
      "source": [
        "# @title Creating dataloaders: without using polar transforms (neither segmentation)\n",
        "data_dir = '/content/drive/MyDrive/CV Project/CVUSA'\n",
        "\n",
        "data_module = CVUSADataModule(data_dir = data_dir, batch_size = 64)\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()\n",
        "#test_loader = data_module.test_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VmODtjKnRzH"
      },
      "source": [
        "#Losses and other utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3LrvTpoe2-R"
      },
      "outputs": [],
      "source": [
        "# # @title TripletLoss implementation\n",
        "# class TripletLoss(pl.LightningModule):\n",
        "#     def __init__(self, margin=1.0):\n",
        "#         super(TripletLoss, self).__init__()\n",
        "#         self.margin = margin\n",
        "\n",
        "#     def calc_euclidean(self, x1, x2):\n",
        "#         return (x1 - x2).pow(2).sum(1)\n",
        "\n",
        "#     def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:\n",
        "#         distance_positive = self.calc_euclidean(anchor, positive)\n",
        "#         distance_negative = self.calc_euclidean(anchor, negative)\n",
        "#         losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "\n",
        "#         return losses.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title TripletLoss implementation\n",
        "# class TripletLoss(pl.LightningModule):\n",
        "#     def __init__(self, margin=1.0):\n",
        "#         super(TripletLoss, self).__init__()\n",
        "#         self.margin = margin\n",
        "\n",
        "#     def forward(self, image_features1, image_features2, k = None):\n",
        "#         N = len(image_features1)\n",
        "#         if k is None:\n",
        "#           k = N-1\n",
        "#         # Calcolare le distanze Euclidee tra le features\n",
        "#         distances_per_image1 = torch.cdist(image_features1, image_features2, p=2)  # p=2 per distanza Euclidea\n",
        "#         distances_per_image2 = distances_per_image1.T  # Per simmetria\n",
        "#         #in common between the losses:\n",
        "#         mask = torch.eye(distances_per_image1.size(0), dtype=torch.bool, device = device)\n",
        "#         distances_diag = torch.masked_select(distances_per_image1, mask) @ torch.eye(n=N, device = device)\n",
        "\n",
        "#         distances_positive_matrix = distances_diag @ torch.ones(size=(N,k), device = device) #da controllare\n",
        "#         margin_matrix = torch.ones(size = (N,k), device = device) * self.margin\n",
        "\n",
        "\n",
        "#         distances_no_diag = torch.masked_select(distances_per_image1, mask.logical_not()).view(distances_per_image1.size(0), -1) #need to see if is NxN-1\n",
        "#         distances_no_diag, _ = torch.topk(distances_no_diag, k = k, dim = 1, largest = False)\n",
        "#         loss_matrix = torch.relu(distances_positive_matrix - distances_no_diag + margin_matrix)\n",
        "\n",
        "#         loss1 = torch.mean(loss_matrix, dim=(0,1))\n",
        "\n",
        "#         distances_no_diag = torch.masked_select(distances_per_image2, mask.logical_not()).view(distances_per_image2.size(0), -1) #need to see if is NxN-1\n",
        "#         distances_no_diag, _ = torch.topk(distances_no_diag, k = k, dim = 1, largest = False)\n",
        "#         loss_matrix = torch.relu(distances_positive_matrix - distances_no_diag + margin_matrix)\n",
        "\n",
        "#         loss2 = torch.mean(loss_matrix, dim=(0,1))\n",
        "\n",
        "#         return (loss1 + loss2)/2"
      ],
      "metadata": {
        "id": "7lHAEBs7rC2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation TripletLoss more stable\n",
        "class TripletLoss(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, grd_global, sat_global, args):\n",
        "        dist_array = 2.0 - 2.0 * torch.matmul(sat_global, grd_global.T)\n",
        "\n",
        "        pos_dist = torch.diag(dist_array)\n",
        "        pair_n = args.train_batch_size * (args.train_batch_size - 1.0)\n",
        "        triplet_dist_g2s = pos_dist - dist_array\n",
        "        loss_g2s = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_g2s * args.loss_weight)))/pair_n\n",
        "        triplet_dist_s2g = torch.unsqueeze(pos_dist, 1) - dist_array\n",
        "        loss_s2g = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_s2g * args.loss_weight)))/pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "kuwQsXRGMI0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title InfoNCE implementation\n",
        "class InfoNCE(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_function):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss_function = loss_function #we can use a generic loss function!\n",
        "\n",
        "    def forward(self, image_features1, image_features2, logit_scale):\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "\n",
        "        logits_per_image1 = logit_scale * image_features1 @ image_features2.T #similarity matrix\n",
        "\n",
        "        logits_per_image2 = logits_per_image1.T\n",
        "\n",
        "        labels = torch.arange(len(logits_per_image1), dtype=torch.long, device = device)\n",
        "\n",
        "        loss = (self.loss_function(logits_per_image1, labels) + self.loss_function(logits_per_image2, labels))/2\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "ukTOSR3IgPf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u6wnToFjtlF"
      },
      "outputs": [],
      "source": [
        "# @title Top-K Rank Accuracy: takes embeddings in input\n",
        "\n",
        "def top_k_rank_accuracy(emb1, emb2, k=1):\n",
        "    if k > len(emb1) :\n",
        "      return 0.0 #might happen at the end of the dataset (batch less then the chosen one)\n",
        "    # Calculate cosine similarity\n",
        "    correct_in_top_k = 0\n",
        "    for index, elem in enumerate(emb1):\n",
        "        cosine_sim = F.cosine_similarity(elem, emb2, dim=1)\n",
        "\n",
        "        # Find the rank K similarity\n",
        "        top_k_similarities, top_k_indices = torch.topk(cosine_sim, k, largest=True)\n",
        "\n",
        "        correct_in_top_k += index in top_k_indices.tolist()\n",
        "\n",
        "    accuracy = correct_in_top_k / len(emb1)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaYUTm_wqrUR"
      },
      "outputs": [],
      "source": [
        "# @title Generation of negatives: modularized so that we can experiment with a lot of versions!\n",
        "def generate_negatives(positives):\n",
        "  shuffled_indices = torch.randperm(positives.size(0))\n",
        "  negatives = positives[shuffled_indices]\n",
        "\n",
        "  return negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouQUwE2nWua"
      },
      "source": [
        "# Training Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "abAMZR3-HJv4",
        "outputId": "7f077aaf-c2f2-4067-99d7-26b16a4644be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "class ResNet50Branch(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(ResNet50Branch, self).__init__()\n",
        "        self.resnet50 = models.resnet50(pretrained=True)\n",
        "        # Modify the last layer for your specific task\n",
        "        self.resnet50.fc = torch.nn.Linear(self.resnet50.fc.in_features, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet50(x)\n",
        "\n",
        "class DualResNet50Model(pl.LightningModule):\n",
        "    def __init__(self, margin = 1.0):\n",
        "        super(DualResNet50Model, self).__init__()\n",
        "        self.branch1 = ResNet50Branch()\n",
        "        self.branch2 = ResNet50Branch()\n",
        "\n",
        "        self.loss = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "        #self.loss = TripletLoss(margin = 0.5)\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.branch1(x1)\n",
        "        out2 = self.branch2(x2)\n",
        "        return out1, out2\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        #1 indicates necessary for loss 1, 2 for loss 2. we take the average.\n",
        "        \"\"\"anchor_embed1 = out1\n",
        "        positive_embed1 = out2\n",
        "\n",
        "        negative_embed1 = generate_negatives(positive_embed1)\n",
        "        loss1 = self.loss(anchor_embed1, positive_embed1, negative_embed1) #first loss\n",
        "\n",
        "        loss2 = 0.0\n",
        "        if self.double_loss:\n",
        "          anchor_embed2 = out2\n",
        "          positive_embed2 = out1\n",
        "\n",
        "          negative_embed2 = generate_negatives(positive_embed2)\n",
        "          loss2 = self.loss(anchor_embed2, positive_embed2, negative_embed2)\n",
        "\n",
        "        loss = loss1 + loss2\"\"\"\n",
        "\n",
        "        loss = self.loss(out1, out2, logit_scale = 3.0)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "\n",
        "        self.log('train_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('val_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        streetview, bingmap = batch\n",
        "        out1, out2 = self(streetview, bingmap)\n",
        "\n",
        "        accuracy_top_1 = top_k_rank_accuracy(out1, out2, k = 1)\n",
        "        accuracy_top_3 = top_k_rank_accuracy(out1, out2, k = 3)\n",
        "        accuracy_top_10 = top_k_rank_accuracy(out1, out2, k = 10)\n",
        "\n",
        "        self.log('test_top1', accuracy_top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top3', accuracy_top_3, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('test_top10', accuracy_top_10, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return accuracy_top_1, accuracy_top_3, accuracy_top_10\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "# Instantiate the model\n",
        "model = DualResNet50Model(margin = 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVD0l4tD4Byc",
        "outputId": "96c1ec0b-72a8-49ac-976c-67b9b78400a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=30,\n",
        "    devices=1,\n",
        "    callbacks=[RichProgressBar()],\n",
        "    log_every_n_steps=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "p57rNWjHN1hh",
        "outputId": "22861fdd-de69-46dc-d4c8-f765cc67159a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 29/29 \u001b[38;2;98;6;224m━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m32/104\u001b[0m \u001b[38;5;245m0:00:28 • 0:01:02\u001b[0m \u001b[38;5;249m1.16it/s\u001b[0m \u001b[37mv_num: 12.000 train_top1_step:    \u001b[0m\n",
              "                                                                                 \u001b[37m0.953 train_loss_step: 1.692      \u001b[0m\n",
              "                                                                                 \u001b[37mval_top1_step: 0.718              \u001b[0m\n",
              "                                                                                 \u001b[37mval_top3_step: 1.000              \u001b[0m\n",
              "                                                                                 \u001b[37mval_top10_step: 1.000             \u001b[0m\n",
              "                                                                                 \u001b[37mval_top1_epoch: 0.637             \u001b[0m\n",
              "                                                                                 \u001b[37mval_top3_epoch: 0.904             \u001b[0m\n",
              "                                                                                 \u001b[37mval_top10_epoch: 0.986            \u001b[0m\n",
              "                                                                                 \u001b[37mtrain_top1_epoch: 0.931           \u001b[0m\n",
              "                                                                                 \u001b[37mtrain_loss_epoch: 1.685           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 29/29 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">32/104</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:28 • 0:01:02</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">1.16it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">v_num: 12.000 train_top1_step:    </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">0.953 train_loss_step: 1.692      </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top1_step: 0.718              </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top3_step: 1.000              </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top10_step: 1.000             </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top1_epoch: 0.637             </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top3_epoch: 0.904             </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">val_top10_epoch: 0.986            </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_top1_epoch: 0.931           </span>\n",
              "                                                                                 <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_loss_epoch: 1.685           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dubja7aKNutk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376,
          "referenced_widgets": [
            "aab4e1b2e0f045ee98c223b27dc2990e",
            "c86f025107874ca99492c94e8e7ddcc0"
          ]
        },
        "outputId": "7df147ae-312c-4e64-cdc0-35e41d887790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/lightning_logs/version_9/checkpoints/epoch=0-step=52.ckpt\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Loaded model weights from the checkpoint at /content/lightning_logs/version_9/checkpoints/epoch=0-step=52.ckpt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aab4e1b2e0f045ee98c223b27dc2990e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0009ad6a8247>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         return call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_provided\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         )\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;31m# remove the tensors from the test results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_tensors_to_scalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;31m# run step hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;31m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"on_test_batch_end\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"on_validation_batch_end\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mhook_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mhook_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Callback]{callback.state_key}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/progress/rich_progress.py\u001b[0m in \u001b[0;36mon_test_batch_end\u001b[0;34m(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_disabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_progress_bar_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_progress_bar_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.test(dataloaders=val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "jkq6qdsP7m25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Depth estimation"
      ],
      "metadata": {
        "id": "lD6Pwhc4Dj7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "checkpoint = \"vinvino02/glpn-nyu\"\n",
        "depth_estimator = pipeline(\"depth-estimation\", model=checkpoint)"
      ],
      "metadata": {
        "id": "ZllGbCNdwgOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://unsplash.com/photos/HwBAsSbPBDU/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MzR8fGNhciUyMGluJTIwdGhlJTIwc3RyZWV0fGVufDB8MHx8fDE2Nzg5MDEwODg&force=true&w=640\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ],
      "metadata": {
        "id": "7wPIZ63Swhsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = depth_estimator(image)"
      ],
      "metadata": {
        "id": "88pL05K4wotg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[\"depth\"]"
      ],
      "metadata": {
        "id": "spYmg4qDwsIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
        "\n",
        "checkpoint = \"vinvino02/glpn-nyu\"\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
        "model = AutoModelForDepthEstimation.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "qiiNBc6Qx0fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open(\"/content/drive/MyDrive/CV Project/CVUSA/streetview/0000052.jpg\")\n",
        "with torch.no_grad():\n",
        "      pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n",
        "      outputs = model(pixel_values)\n",
        "      predicted_depth = outputs.predicted_depth"
      ],
      "metadata": {
        "id": "sAGtPjXqyqr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# interpolate to original size\n",
        "prediction = torch.nn.functional.interpolate(\n",
        "    predicted_depth.unsqueeze(1),\n",
        "    size=image.size[::-1],\n",
        "    mode=\"bicubic\",\n",
        "    align_corners=False,\n",
        ").squeeze()\n",
        "output = prediction.numpy()\n",
        "\n",
        "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
        "depth = Image.fromarray(formatted)\n",
        "depth"
      ],
      "metadata": {
        "id": "ryjCnToU0E4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#road\n",
        "#building\n",
        "#low vegetation\n",
        "#trees\n",
        "#car\n",
        "#clutter"
      ],
      "metadata": {
        "id": "NW2V4trwH8CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#negative sampling basato sulla similarità ovvero computo la similarità di un sample con tutti gli altri e uso quelli più vicini come negativi"
      ],
      "metadata": {
        "id": "w2ubbSCwNOJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#soft margin triplet loss and weighted soft margin triplet loss"
      ],
      "metadata": {
        "id": "rTOrOUd8Nwao"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aab4e1b2e0f045ee98c223b27dc2990e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c86f025107874ca99492c94e8e7ddcc0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c86f025107874ca99492c94e8e7ddcc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}