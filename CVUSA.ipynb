{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmaMule/Computer-Vision/blob/main/CVUSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi5yzhh_RW7B"
      },
      "source": [
        "#Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GFbuy6OTiUw"
      },
      "source": [
        "**IMPORTANT**: this Python Notebook is intended to be used on Google Colab\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ozCeAZd5--khzPANWxLOWVXU4qGOLL6U?authuser=1#scrollTo=2GFbuy6OTiUw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLOgjr_hRkmL"
      },
      "source": [
        "The task of **Ground-to-Aerial matching** caught our attention for several reasons.\n",
        "\n",
        "Primarily, we were intrigued to delve into our first work in the field of Information Retrieval and multi-view and cross-domain image analysis. The prospect of addressing a novel challenge within this domain was exciting and offered the opportunity to contribute meaningfully to the development of new methodologies and technologies.\n",
        "\n",
        "Additionally, the challenge of matching a ground image to the correct satellite image without any spatial information (such as coordinates where the picture was taken) presented a fascinating problem. The complexity of this problem lies in the significant differences in scale, angle, and appearance between ground-level and aerial images, necessitating advanced algorithms and innovative approaches to bridge the gap.\n",
        "\n",
        "Following the **feature-enrichment approach** seen in the [paper](https://arxiv.org/html/2404.11302v1) \"A Semantic Segmentation-guided Approach for Ground-to-Aerial Image Matching\", we present an enriched dataset, starting from a subset of the CVUSA dataset, and new models combining brand new features (such as Ground Depth estimation and Ground Semantic Segmentation), with the ones presented in the paper.\n",
        "\n",
        "As **backbone models** for our \"branches\" we used many state of the art models, in particular VGG16, ResNet (the 50, 101 and 152 versions) and SAIG, a transformer-based model for dealing that has already been proved to be [quite effective](https://arxiv.org/abs/2302.01572) on this task.\n",
        "\n",
        "The generation of the new features was done in two different Colab notebooks:\n",
        "\n",
        "\n",
        "*   [Ground Depth Estimation](https://colab.research.google.com/drive/1FzAKRESCVgZdi3U21maYJardy_BZ2Qx6?usp=drive_link)\n",
        "*   [Ground Semantic Segmentation](https://colab.research.google.com/drive/1Mu0SlsxwE-nl9W5rOCQCxvP7UQ-AITN6?usp=drive_link)\n",
        "\n",
        "Everything was done using **Pytorch Lightning**.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1BFNMRqB_wGC_cYWSbWoNL1jvsERyNGjk' />\n",
        "<figcaption>Example of matching</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1FRSut8m-Mv"
      },
      "source": [
        "#Import and installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "bMqOyt0ONvDV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# @title Installing dependencies\n",
        "\n",
        "!pip install tqdm\n",
        "!pip install pytorch_lightning\n",
        "!pip install patool\n",
        "!pip install torchvision nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBFZKHodQUoQ"
      },
      "outputs": [],
      "source": [
        "# @title Importing libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import csv\n",
        "import cv2\n",
        "import gdown\n",
        "import patoolib\n",
        "import inspect\n",
        "from typing_extensions import override\n",
        "from sys import version\n",
        "import abc\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler, random_split\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler, RandomSampler, BatchSampler\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
        "\n",
        "\n",
        "# pytorch lighting\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar, RichProgressBar, ModelPruning\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.loggers.logger import Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XTosPQ5YcmFt"
      },
      "outputs": [],
      "source": [
        "# @title Folders Setup\n",
        "\n",
        "shutil.rmtree('/content/input', ignore_errors = True)\n",
        "os.mkdir('/content/input')\n",
        "\n",
        "shutil.rmtree('/content/output', ignore_errors = True)\n",
        "os.mkdir('/content/output')\n",
        "\n",
        "shutil.rmtree('/content/output/log', ignore_errors = True)\n",
        "os.mkdir('/content/output/log')\n",
        "\n",
        "shutil.rmtree('/content/lightning_logs', ignore_errors = True)\n",
        "os.mkdir('/content/lightning_logs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gKcEFA_tcicZ"
      },
      "outputs": [],
      "source": [
        "# @title Downloading Dataset from our Google Drive\n",
        "\n",
        "# subset: 1-FHbO02_KtJcStojzf1JRKwJkc0hMCLd\n",
        "# subset 2.0 : 11DR7zhd6wchdyt8DSkTY2JGgf_jrtf1D\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=11DR7zhd6wchdyt8DSkTY2JGgf_jrtf1D'\n",
        "output_file = '/content/input/CVUSA_subset_2_0.rar'\n",
        "output_dir = '/content/input/data'\n",
        "\n",
        "gdown.download(url, output_file)\n",
        "patoolib.extract_archive(output_file, outdir = output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOvjHHWCdLWy"
      },
      "outputs": [],
      "source": [
        "# @title Settings\n",
        "\n",
        "pl.seed_everything(42)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJVcYcx3gMql"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDSCF0zinL8Y"
      },
      "source": [
        "#Dataset and DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0UnwPoYpJd6"
      },
      "source": [
        "In this section we define the **CVUSADataset** class and the **CVUSADataModule** class.\n",
        "\n",
        "The CVUSADataset class deals with the *train_updated.csv* and *val_updated.csv* files containing the **paths to the images** and associating images belonging to the same sample.\n",
        "\n",
        "The CVUSADataModule class builds the two splits of the CVUSADataset and enables to load batches *on-the-fly* making possible to use more images than could actually fit in the main memory at our disposal. It also deals with:\n",
        "\n",
        "1.   The **resizing** of the images\n",
        "2.   The **Normalization** of the images\n",
        "3.   Other preprocessing and **augmentation** steps\n",
        "3.   The definition of the **Dataloaders**\n",
        "4.   Computation of **mean and standard deviation**\n",
        "\n",
        "Each sample of our dataset is composed of 5 images:\n",
        "\n",
        "1.   The original RGB ground image\n",
        "2.   The segmentation of the ground image we produced\n",
        "3.   The depth estimation of the ground image we produced\n",
        "4.   The original RGB satellite image\n",
        "5.   The segmentation of the satellite image from [this paper](https://arxiv.org/html/2404.11302v1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1ekFGoQvb1jwnwu41noJddDuVzqEM7SYX' />\n",
        "<figcaption>Example of ground segmentation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1gTYYsRrwns3taHfZ5YZQAGtSz0EQhQqF' />\n",
        "<figcaption>Example of ground depth estimation</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HT1pbNEXQknc"
      },
      "outputs": [],
      "source": [
        "# @title CVUSADataset class definition\n",
        "\n",
        "# Expected dataset structure: the input_dir contains the split cvs files and a\n",
        "# subdirectory named 'data' with the CVUSA dataset\n",
        "\n",
        "class CVUSADataset(Dataset):\n",
        "\n",
        "    def __init__(self, input_dir, split = 'train', polar = False):\n",
        "        self.split = split\n",
        "        self.polar = polar\n",
        "        self.data = self.load_data(input_dir + f'/{split}.csv')\n",
        "\n",
        "\n",
        "    def load_data(self, csv_path):\n",
        "        data = []\n",
        "        with open(csv_path, 'r') as file:\n",
        "            csv_reader = csv.reader(file)\n",
        "            next(csv_reader) #skip header\n",
        "            for row in csv_reader:\n",
        "                grd_path = row[1]\n",
        "                grd_seg_path = row[5]\n",
        "                grd_dep_path = row[6]\n",
        "                if self.polar: #If we want to use polar\n",
        "                   sat_path = row[3]\n",
        "                   sat_seg_path = row[4]\n",
        "                else:\n",
        "                  sat_path = row[0]\n",
        "                  sat_seg_path = row[2]\n",
        "                data.append({\"grd_path\": grd_path, \"grd_seg_path\": grd_seg_path, \"grd_dep_path\": grd_dep_path, \"sat_path\": sat_path, \"sat_seg_path\": sat_seg_path})\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        dictionary = self.data[index]\n",
        "        grd_path = dictionary['grd_path']\n",
        "        grd_seg_path = dictionary['grd_seg_path']\n",
        "        grd_dep_path = dictionary['grd_dep_path']\n",
        "        sat_path = dictionary['sat_path']\n",
        "        sat_seg_path = dictionary['sat_seg_path']\n",
        "        return grd_path, grd_seg_path, grd_dep_path, sat_path, sat_seg_path\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"CVUSA-Dataset-{self.split}: {len(self.data)} samples\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj0ecN-1QnVq"
      },
      "outputs": [],
      "source": [
        "# @title CVUSADataModule class definition\n",
        "\n",
        "class CVUSADataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, input_dir, polar=False, batch_size=8, grd_resize=None, grd_seg_resize=None, grd_dep_resize=None, sat_resize=None, sat_seg_resize=None, augmentations=False):\n",
        "        super(CVUSADataModule, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dir = input_dir\n",
        "        self.data_dir = input_dir  # + '/data' I made this choice for semplicity!!! \"\"\"IMPORTANT!!!\"\n",
        "        self.polar = polar\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "        self.original_size = {'grd': None, 'grd_seg': None, 'grd_dep': None, 'sat': None, 'sat_seg': None}\n",
        "        self.resize = {'grd': grd_resize, 'grd_seg': grd_seg_resize, 'grd_dep': grd_dep_resize, 'sat': sat_resize, 'sat_seg': sat_seg_resize}\n",
        "        self.size = {'grd': None, 'grd_seg': None, 'grd_dep': None, 'sat': None, 'sat_seg': None}\n",
        "        self.mean = {'grd': [0, 0, 0], 'grd_seg': [0, 0, 0], 'grd_dep': [0, 0, 0], 'sat': [0, 0, 0], 'sat_seg': [0, 0, 0]}\n",
        "        self.std = {'grd': [1, 1, 1], 'grd_seg': [1, 1, 1], 'grd_dep': [1, 1, 1], 'sat': [1, 1, 1], 'sat_seg': [1, 1, 1]}\n",
        "        self.transform = {'grd': None, 'grd_seg': None, 'grd_dep': None, 'sat': None, 'sat_seg': None}\n",
        "        self.train_transform = {'grd': None, 'grd_seg': None, 'grd_dep': None, 'sat': None, 'sat_seg': None}\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "        # load the datasets\n",
        "        self.train_dataset = CVUSADataset(input_dir=self.input_dir, split='train_updated', polar=self.polar)\n",
        "        self.val_dataset = CVUSADataset(input_dir=self.input_dir, split='val_updated', polar=self.polar)\n",
        "\n",
        "        print(self.train_dataset)\n",
        "        print(self.val_dataset)\n",
        "\n",
        "        # find image sizes\n",
        "        self.__compute_image_sizes()\n",
        "\n",
        "        # compute transforms\n",
        "        self.__compute_transforms()\n",
        "\n",
        "\n",
        "    def __compute_image_sizes(self):\n",
        "        grd_sample, grd_seg_sample, grd_dep_sample, sat_sample, sat_seg_sample = self.train_dataset[0]\n",
        "\n",
        "        grd_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, grd_sample)))\n",
        "        grd_seg_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, grd_seg_sample)))\n",
        "        grd_dep_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, grd_dep_sample)))\n",
        "        sat_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, sat_sample)))\n",
        "        sat_seg_image = v2.ToImage()(Image.open(os.path.join(self.data_dir, sat_seg_sample)))\n",
        "\n",
        "        self.original_size['grd'] = grd_image.size()[1:3]\n",
        "        self.original_size['grd_seg'] = grd_seg_image.size()[1:3]\n",
        "        self.original_size['grd_dep'] = grd_dep_image.size()[1:3]\n",
        "        self.original_size['sat'] = sat_image.size()[1:3]\n",
        "        self.original_size['sat_seg'] = sat_seg_image.size()[1:3]\n",
        "\n",
        "        self.size['grd'] = grd_image.size()[1:3]\n",
        "        self.size['grd_seg'] = grd_seg_image.size()[1:3]\n",
        "        self.size['grd_dep'] = grd_dep_image.size()[1:3]\n",
        "        self.size['sat'] = sat_image.size()[1:3]\n",
        "        self.size['sat_seg'] = sat_seg_image.size()[1:3]\n",
        "\n",
        "        # compute image new sizes\n",
        "        for key in self.resize:\n",
        "            if self.resize[key]:\n",
        "                image = v2.ToImage()(Image.open(os.path.join(self.data_dir, locals()[f\"{key}_sample\"])))\n",
        "                self.size[key] = v2.Resize((self.resize[key]))(image).size()[1:3]\n",
        "\n",
        "\n",
        "    def __compute_transforms(self):\n",
        "        for key in self.transform:\n",
        "            self.transform[key] = A.Compose([\n",
        "                A.Resize(self.size[key][0], self.size[key][1], interpolation=cv2.INTER_LINEAR_EXACT, p=1.0),\n",
        "                A.Normalize(self.mean[key], self.std[key]),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "        self.train_transform = self.transform.copy()\n",
        "\n",
        "        if self.augmentations:\n",
        "\n",
        "            self.train_transform['grd'] = A.Compose([\n",
        "                A.Resize(self.size['grd'][0], self.size['grd'][1], interpolation=cv2.INTER_LINEAR_EXACT, p=1.0),\n",
        "                # New Transforms\n",
        "                A.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.15, always_apply=False, p=0.5),\n",
        "                A.OneOf([\n",
        "                    A.AdvancedBlur(p=1.0),\n",
        "                    A.Sharpen(p=1.0),\n",
        "                ], p=0.3),\n",
        "                A.OneOf([\n",
        "                    A.GridDropout(ratio=0.5, p=1.0),\n",
        "                    A.CoarseDropout(max_holes=25,\n",
        "                        max_height=int(0.2*self.size['grd'][0]),\n",
        "                        max_width=int(0.2*self.size['grd'][0]),\n",
        "                        min_holes=10,\n",
        "                        min_height=int(0.1*self.size['grd'][0]),\n",
        "                        min_width=int(0.1*self.size['grd'][0]),\n",
        "                        p=1.0),\n",
        "                    ], p=0.3),\n",
        "                ###\n",
        "                A.Normalize(self.mean['grd'], self.std['grd']),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "            self.train_transform['sat'] = A.Compose([\n",
        "                A.Resize(self.size['sat'][0], self.size['sat'][1], interpolation=cv2.INTER_LINEAR_EXACT, p=1.0),\n",
        "                # New Transforms\n",
        "                A.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15, hue=0.15, always_apply=False, p=0.5),\n",
        "                A.OneOf([\n",
        "                    A.AdvancedBlur(p=1.0),\n",
        "                    A.Sharpen(p=1.0),\n",
        "                ], p=0.3),\n",
        "                A.OneOf([\n",
        "                    A.GridDropout(ratio=0.4, p=1.0),\n",
        "                    A.CoarseDropout(max_holes=25,\n",
        "                        max_height=int(0.2*self.size['sat'][0]),\n",
        "                        max_width=int(0.2*self.size['sat'][0]),\n",
        "                        min_holes=10,\n",
        "                        min_height=int(0.1*self.size['sat'][0]),\n",
        "                        min_width=int(0.1*self.size['sat'][0]),\n",
        "                    p=1.0),\n",
        "                ], p=0.3),\n",
        "                ###\n",
        "                A.Normalize(self.mean['sat'], self.std['sat']),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "            # Still needed for depth and segmaps\n",
        "\n",
        "\n",
        "    def train_collate_fn(self, batch):\n",
        "        return self.collate_fn(batch, 'train')\n",
        "\n",
        "\n",
        "    def val_collate_fn(self, batch):\n",
        "        return self.collate_fn(batch, 'val')\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch, dataset):\n",
        "        grd_path, grd_seg_path, grd_dep_path, sat_path, sat_seg_path = zip(*batch)\n",
        "\n",
        "        # load and transform each image in the batch\n",
        "        grd_ids, grd_images = self.__compute_images(grd_path, 'grd', dataset)\n",
        "        grd_seg_ids, grd_seg_images = self.__compute_images(grd_seg_path, 'grd_seg', dataset)\n",
        "        grd_dep_ids, grd_dep_images = self.__compute_images(grd_dep_path, 'grd_dep', dataset)\n",
        "        sat_ids, sat_images = self.__compute_images(sat_path, 'sat', dataset)\n",
        "        sat_seg_ids, sat_seg_images = self.__compute_images(sat_seg_path, 'sat_seg', dataset)\n",
        "\n",
        "        grd_samples = {'imgs': grd_images, 'imgs_id': grd_ids}\n",
        "        grd_seg_samples = {'imgs': grd_seg_images, 'imgs_id': grd_seg_ids}\n",
        "        grd_dep_samples = {'imgs': grd_dep_images, 'imgs_id': grd_dep_ids}\n",
        "        sat_samples = {'imgs': sat_images, 'imgs_id': sat_ids}\n",
        "        sat_seg_samples = {'imgs': sat_seg_images, 'imgs_id': sat_seg_ids}\n",
        "\n",
        "        return grd_samples, grd_seg_samples, grd_dep_samples, sat_samples, sat_seg_samples\n",
        "\n",
        "\n",
        "    def __compute_images(self, paths, img_type, dataset):\n",
        "        images = []\n",
        "        ids = []\n",
        "\n",
        "        for img_path in paths:\n",
        "            img = cv2.imread(os.path.join(self.data_dir, img_path))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            if dataset == 'train':\n",
        "                img = self.train_transform[img_type](image = img)['image']\n",
        "            else:\n",
        "                img = self.transform[img_type](image = img)['image']\n",
        "\n",
        "            images.append(img)\n",
        "\n",
        "            ids.append(int(img_path[-11:-4]))\n",
        "\n",
        "        # Stack the image tensors along the batch dimension\n",
        "        images_tensor = torch.stack(images)\n",
        "        ids_tensor = torch.tensor(ids, dtype=int)\n",
        "        return ids_tensor, images_tensor\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.train_collate_fn, shuffle=True, num_workers=2)\n",
        "\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.val_collate_fn, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    def compute_mean_std(self):\n",
        "        mean_std = {}\n",
        "\n",
        "        for key in self.mean:\n",
        "            mean, std = self.__compute_mean_std_for_key(key)\n",
        "            mean_std[key + '_mean'] = mean\n",
        "            mean_std[key + '_std'] = std\n",
        "\n",
        "        return mean_std\n",
        "\n",
        "\n",
        "    def __compute_mean_std_for_key(self, key):\n",
        "        mean = np.array([0., 0., 0.])\n",
        "        std = np.array([0., 0., 0.])\n",
        "\n",
        "        for i in self.train_dataset.data:\n",
        "            img_path = os.path.join(self.data_dir, i[f'{key}_path'])\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = img.astype(float) / 255.\n",
        "            img_size = img.shape[0] * img.shape[1]\n",
        "            mean += np.mean(img[:, :, :], axis=(0, 1))\n",
        "            std += ((img[:, :, :] - mean) ** 2).sum(axis=(0, 1)) / img_size\n",
        "\n",
        "        mean /= len(self.train_dataset.data)\n",
        "        std = np.sqrt(std / len(self.train_dataset.data))\n",
        "\n",
        "        return mean, std\n",
        "\n",
        "\n",
        "    def set_mean_std(self, mean_std):\n",
        "        for key in self.mean:\n",
        "            self.mean[key] = mean_std[f'{key}_mean']\n",
        "            self.std[key] = mean_std[f'{key}_std']\n",
        "\n",
        "        self.__compute_transforms()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkLiGNItQpOk"
      },
      "outputs": [],
      "source": [
        "# @title Creating dataloaders\n",
        "\n",
        "input_dir = '/content/input/data/CVUSA'\n",
        "polar = False\n",
        "\n",
        "data_module = CVUSADataModule(\n",
        "    input_dir = input_dir,\n",
        "    polar = polar,\n",
        "    augmentations = True,\n",
        "    batch_size = 64,\n",
        "    grd_resize = 64,\n",
        "    grd_seg_resize = 64,\n",
        "    grd_dep_resize = 64,\n",
        "    sat_resize = 128,\n",
        "    sat_seg_resize = 128\n",
        ")\n",
        "\n",
        "data_module.setup()\n",
        "\n",
        "#mean_std = data_module.compute_mean_std()\n",
        "\n",
        "# pre-computed mean and std\n",
        "if not polar:\n",
        "    mean_std = {'grd_mean': [0.4691, 0.4821, 0.4603],'grd_std': [0.2202, 0.2191, 0.2583],\n",
        "                'grd_seg_mean': [0.2976, 0.7013, 0.3604],'grd_seg_std': [0.2777, 0.3306, 0.4343],\n",
        "                'grd_dep_mean': [0.3874, 0.166 , 0.1971],'grd_dep_std': [0.3763, 0.2308, 0.171 ],\n",
        "                'sat_mean': [0.3833, 0.3964, 0.3434],'sat_std': [0.1951, 0.1833, 0.1934],\n",
        "                'sat_seg_mean': [0.2861, 0.8014, 0.8299],'sat_seg_std': [0.4468, 0.3955, 0.3707]\n",
        "                }\n",
        "else:\n",
        "# polar = True\n",
        "    mean_std = {'grd_mean': [0.4691, 0.4821, 0.4603],'grd_std': [0.2202, 0.2191, 0.2583],\n",
        "                'grd_seg_mean': [0.2976, 0.7013, 0.3604],'grd_seg_std': [0.2777, 0.3306, 0.4343],\n",
        "                'grd_dep_mean': [0.3874, 0.166 , 0.1971],'grd_dep_std': [0.3763, 0.2308, 0.171 ],\n",
        "                'sat_mean': [0.4   , 0.4128, 0.3647],'sat_std': [0.1966, 0.1862, 0.1993],\n",
        "                'sat_seg_mean': [0.3349, 0.7968, 0.8518],'sat_seg_std': [0.4638, 0.3969, 0.3478]\n",
        "                }\n",
        "\n",
        "data_module.set_mean_std(mean_std)\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VmODtjKnRzH"
      },
      "source": [
        "#Losses and other utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-_ElWjjXIfI"
      },
      "source": [
        "Our models were trained using two loss functions commonly adopted in Information Retrieval: the **InfoNCE Loss** and **Triplet Loss**.\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_j)/\\tau)}{\\sum_{k=1}^N \\exp(\\text{sim}(\\mathbf{z}_i, \\mathbf{z}_k)/\\tau)}\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathcal{L}_{\\text{triplet}} = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\log \\left(1 + \\exp\\left((\\text{pos_dist}_i - \\text{dist_array}_{ij}) \\cdot w\\right)\\right)\n",
        "\\end{align*}\n",
        "\n",
        "In particular, we consider as negative samples not only all the **incorrect satellite images for a given ground image** but also all the other **incorrect ground images for a given satellite**. This is done for multiple reasons:\n",
        "\n",
        "1. The model will also learn to match a certain satellite image to a ground image, making the model **multi-modal**.\n",
        "2. The model can better shape the embeddings produced in a way that each of them is ideally far from all the other **embeddings that are not the correct ones**.\n",
        "\n",
        "These two components of the loss are then averaged (possibly with a weight)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuwQsXRGMI0L"
      },
      "outputs": [],
      "source": [
        "#@title Implementation SoftMarginTripletLoss\n",
        "\n",
        "class TripletLoss(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_weight = 1.0):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "\n",
        "\n",
        "    def forward(self, image_features1, image_features2):\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "        dist_array = 2.0 - 2.0 * torch.matmul(image_features2, image_features1.T)\n",
        "        n = len(image_features1)\n",
        "        pos_dist = torch.diag(dist_array)\n",
        "        pair_n = n * (n - 1.0)\n",
        "        triplet_dist_g2s = pos_dist - dist_array\n",
        "        loss_g2s = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_g2s * self.loss_weight)))/pair_n\n",
        "        triplet_dist_s2g = torch.unsqueeze(pos_dist, 1) - dist_array\n",
        "        loss_s2g = torch.sum(torch.log(1.0 + torch.exp(triplet_dist_s2g * self.loss_weight)))/pair_n\n",
        "        loss = (loss_g2s + loss_s2g) / 2.0\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukTOSR3IgPf7"
      },
      "outputs": [],
      "source": [
        "#@title Implementation  InfoNCE Loss\n",
        "class InfoNCE(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss_function, logit_scale=3.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.loss_function = loss_function # we can use any loss function!\n",
        "        self.logit_scale = logit_scale\n",
        "\n",
        "\n",
        "    def forward(self, image_features1, image_features2):\n",
        "\n",
        "        image_features1 = F.normalize(image_features1, dim=-1)\n",
        "        image_features2 = F.normalize(image_features2, dim=-1)\n",
        "\n",
        "        # use pairwise_cosine_similarity instead? it's the same?\n",
        "        logits_per_image1 = self.logit_scale * image_features1 @ image_features2.T\n",
        "        logits_per_image2 = logits_per_image1.T\n",
        "\n",
        "        labels = torch.arange(len(logits_per_image1), dtype=torch.long, device = device)\n",
        "        loss = (self.loss_function(logits_per_image1, labels) + self.loss_function(logits_per_image2, labels))/2\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u6wnToFjtlF"
      },
      "outputs": [],
      "source": [
        "# @title Top-K Rank Accuracy: takes embeddings in input\n",
        "\n",
        "def top_k_rank_accuracy(emb1, emb2, k=1, inverse=False):\n",
        "\n",
        "    num_samples = len(emb1)\n",
        "\n",
        "    if k > num_samples :\n",
        "      return 0.0 # might happen at the end of the dataset (batch less then the chosen one)\n",
        "\n",
        "    if inverse:\n",
        "      emb1, emb2 = emb2, emb1\n",
        "\n",
        "    emb1 = F.normalize(emb1, dim=-1)\n",
        "    emb2 = F.normalize(emb2, dim=-1)\n",
        "    dist_matrix = 1 - (emb1 @ emb2.T)\n",
        "\n",
        "    _, topk_indices = torch.topk(dist_matrix, k = k, dim = 1, largest = False)\n",
        "\n",
        "    correct_in_topk = sum([i in topk_indices[i, :] for i in range(num_samples)])\n",
        "\n",
        "    accuracy = correct_in_topk / num_samples\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MvHm7d1ICYme"
      },
      "outputs": [],
      "source": [
        "#@title Implementation of Attention operator\n",
        "class Attention(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "viL8vSAwncJX"
      },
      "outputs": [],
      "source": [
        "# @title Implementation ModelCheckpoint\n",
        "class MyModelCheckpoint(ModelCheckpoint):\n",
        "    def __init__(self, config, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.config = config\n",
        "\n",
        "    def on_train_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n",
        "        if not self.dirpath:\n",
        "            raise ValueError(\"dirpath must be specified in ModelCheckpoint\")\n",
        "        file_path = os.path.join(os.path.dirname(self.dirpath), \"config.txt\")\n",
        "        with open(file_path, 'w') as f:\n",
        "            f.write(self.config)\n",
        "            f.write('\\n')\n",
        "\n",
        "        return super().on_train_start(trainer, pl_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXEBXiWchDAq"
      },
      "source": [
        "# Branches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF2JabV5ia4S"
      },
      "source": [
        "In this section, we defined the backbone models that will be employed in the creation of our models.\n",
        "\n",
        "We implemented:\n",
        "\n",
        "1. **VGG16**: a well-known Convolutional Neural Network (CNN) that was introduced by the Visual Geometry Group (VGG) at the University of Oxford. It is known for its simplicity and depth, consisting of 16 layers with a fixed architecture using small convolution filters (3x3) throughout the network. VGG16 is widely used for image classification tasks and serves as a strong baseline model due to its straightforward design and effectiveness.\n",
        "\n",
        "2. **ResNet**: ResNet addresses the vanishing gradient problem by using residual blocks, which allows the model to learn residual functions with reference to the layer inputs, rather than learning unreferenced functions.\n",
        "We worked with versions 50, 101 and 152.\n",
        "\n",
        "3. **SAIG**: SAIG model integrates self-attention mechanisms into CNNs to capture long-range dependencies and contextual information more effectively than traditional convolutional operations alone. This approach combines the strengths of CNNs in extracting local features with the ability of self-attention to model global relationships within the data, making it suitable for tasks requiring a high level of detail and contextual understanding.\n",
        "\n",
        "The models are properly adapted to be used for our structure, in particular to produce adequate output for our visualization functions and to get as input different concatenated images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTFXxK0cb51w",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title VGG16\n",
        "\n",
        "class VGGBranch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, img_size, architecture, num_max_pooling, in_channels=3, output_dim=1000):\n",
        "        super(VGGBranch, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.architecture = architecture\n",
        "        self.num_max_pooling = num_max_pooling\n",
        "        self.in_channels = in_channels\n",
        "        self.output_dim = output_dim\n",
        "        self.division = 2 ** num_max_pooling\n",
        "\n",
        "        self.conv_layers = self.create_conv_layers(architecture)\n",
        "\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(128 * img_size[0]//self.division * img_size[1]//self.division, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, output_dim),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps=False):\n",
        "        for layer in self.conv_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "            if featuremaps and x.shape[2]*x.shape[1] == 48*48:\n",
        "                return x\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fcs(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == int:\n",
        "                out_channels = x\n",
        "\n",
        "                layers += [\n",
        "                    nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "                    nn.BatchNorm2d(out_channels),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "                in_channels = x\n",
        "            elif x == \"M\":\n",
        "                layers += [nn.MaxPool2d(2, 2)]\n",
        "\n",
        "        return nn.ModuleList(layers)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"VGGBranch(img_size={self.img_size}, architecture={self.architecture}, num_max_pooling={self.num_max_pooling}, in_channels={self.in_channels}, output_dim={self.output_dim})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abAMZR3-HJv4"
      },
      "outputs": [],
      "source": [
        "# @title Resnet\n",
        "\n",
        "class ResNetBranch(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        output_dim,\n",
        "        resnet_version=50,\n",
        "        input_images = 1,\n",
        "        finetuning='free',\n",
        "        conv_only=False\n",
        "    ):\n",
        "        super(ResNetBranch, self).__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.resnet_version = resnet_version\n",
        "        self.finetuning = finetuning\n",
        "        self.conv_only = conv_only\n",
        "\n",
        "\n",
        "        if resnet_version == 50:\n",
        "            self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        elif resnet_version == 101:\n",
        "            self.resnet = models.resnet101(weights=models.ResNet101_Weights.DEFAULT)\n",
        "        elif resnet_version == 152:\n",
        "            self.resnet = models.resnet152(weights=models.ResNet152_Weights.DEFAULT)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported ResNet version. Choose between 50, 101, or 152.\")\n",
        "\n",
        "\n",
        "        # Generalization to accept multiple input images\n",
        "        if input_images > 1:\n",
        "            weight = self.resnet.conv1.weight\n",
        "            weight = list(weight for i in range(input_images))\n",
        "            weight = torch.cat(weight, dim = 1)\n",
        "            weight = torch.nn.Parameter(weight)\n",
        "            self.resnet.conv1.weight = weight\n",
        "\n",
        "\n",
        "        if not self.conv_only:\n",
        "            self.resnet.fc = nn.Linear(self.resnet.fc.in_features, self.output_dim)\n",
        "        elif self.conv_only:\n",
        "            # could remove last layers of resnet?\n",
        "            pass\n",
        "\n",
        "\n",
        "        if self.finetuning == 'free':\n",
        "            pass\n",
        "        elif self.finetuning == 'freeze':\n",
        "            ct = 0\n",
        "            for child in self.resnet.children():\n",
        "                ct += 1\n",
        "                if ct < 7:\n",
        "                    for param in child.parameters():\n",
        "                        param.requires_grad = False\n",
        "        elif self.finetuning == 'reset':\n",
        "            ct = 0\n",
        "            for child in self.resnet.children():\n",
        "                ct += 1\n",
        "                if ct < 7:\n",
        "                    for param in child.parameters():\n",
        "                        param.requires_grad = False\n",
        "                elif ct >= 7:\n",
        "                    for param in child.parameters():\n",
        "                        torch.nn.init.xavier_uniform(param.data)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported fine-tuning. Choose between 'free', 'freeze' and 'reset'.\")\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps=False):\n",
        "        # To print the featuremap we need to return the last conv layer output\n",
        "        if featuremaps:\n",
        "            for name, layer in list(self.resnet.named_children())[:-2]:\n",
        "                x = layer(x)\n",
        "                if x.shape[2]*x.shape[1]<128*128: #magic number: dimension 16 for visualization\n",
        "                  break\n",
        "            return x\n",
        "        else:\n",
        "            return self.resnet(x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"ResNetBranch(output_dim={self.output_dim}, resnet_version={self.resnet_version})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDzQtqQOHzU-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title SAIG\n",
        "\n",
        "\n",
        "class ConvBnReluBlock(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super(ConvBnReluBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "###\n",
        "\n",
        "class Block(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_heads,\n",
        "        qkv_bias=False,\n",
        "        qk_scale=None,\n",
        "        drop=0.,\n",
        "        attn_drop=0.,\n",
        "        dropout=0.,\n",
        "        norm_layer=nn.LayerNorm\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop\n",
        "        )\n",
        "        # check what is droppath\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0. else nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
        "        return x\n",
        "\n",
        "###\n",
        "\n",
        "class SAIGBranch(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, img_size, patch_size=16, in_channels=3, embed_dim=768, num_heads = 8, depth = 4, smd_dim = 8, qkv_bias = True, qk_scale = None, drop_rate=0., attn_drop_rate=0., norm_layer=None, flatten=True):\n",
        "        super(SAIGBranch, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = depth\n",
        "        self.smd_dim = smd_dim\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.qk_scale = qk_scale\n",
        "        self.drop_rate = drop_rate\n",
        "        self.attn_drop_rate = attn_drop_rate\n",
        "        self.norm_layer = norm_layer\n",
        "        self.flatten = flatten\n",
        "\n",
        "        #potremmo salvare i parametri, ha qualche senso?\n",
        "        self.output_dim = embed_dim * smd_dim\n",
        "\n",
        "        self.grid_size = (img_size[0] // patch_size, img_size[1] // patch_size)\n",
        "\n",
        "        if img_size[0] % patch_size != 0 or img_size[1] % patch_size != 0:\n",
        "          print(\"Warning: image size is not divisible for patch size\")\n",
        "\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "        self.conv_bn_relu_blocks = nn.Sequential(\n",
        "            ConvBnReluBlock(in_channels = self.in_channels, out_channels = 64, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 64, out_channels = 128, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 128, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 128, out_channels = 256, stride = 2),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 256, stride = 1),\n",
        "            ConvBnReluBlock(in_channels = 256, out_channels = 512, stride = 2),\n",
        "        )\n",
        "        self.patch_block = nn.Conv2d(in_channels = 512, out_channels = embed_dim, kernel_size=1, stride=1 ,padding=0)\n",
        "        self.attn_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                drop=drop_rate, attn_drop=attn_drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim))\n",
        "        #self.pos_embed = nn.Parameter(torch.zeros(1, embed_dim, self.num_patches))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        #self.GAP = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        #self.logits = nn.Linear(in_features = embed_dim, out_features = 512)\n",
        "\n",
        "        self.smd = nn.Sequential(\n",
        "            nn.Linear(self.num_patches, self.num_patches*4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(self.num_patches*4, self.num_patches),\n",
        "            nn.Linear(self.num_patches, smd_dim)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, featuremaps = False):\n",
        "\n",
        "      # extract patch embeddings\n",
        "      x = self.conv_bn_relu_blocks(x)\n",
        "\n",
        "      if featuremaps:\n",
        "        return x\n",
        "\n",
        "      x = self.patch_block(x)\n",
        "      x = x.flatten(2).transpose(1,2)\n",
        "      #x = self.norm(x) CHECK\n",
        "\n",
        "      # add position embeddings\n",
        "      x = x + self.pos_embed\n",
        "      x = self.pos_drop(x)\n",
        "\n",
        "      # pass through sequence of attention blocks\n",
        "      for blk in self.attn_blocks:\n",
        "          x = blk(x)\n",
        "\n",
        "      x = self.norm(x)\n",
        "      # x = self.GAP(x.transpose(-1, -2)).squeeze(2)\n",
        "      # x = self.logits(x)\n",
        "\n",
        "      # if featuremaps:\n",
        "      #   return x.resize(x.shape[0], self.grid_size[0], self.grid_size[1], 384)\n",
        "\n",
        "      # x: b x 88 x 384\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = self.smd(x)\n",
        "      x = x.transpose(-1, -2)\n",
        "      x = x.flatten(-2, -1)\n",
        "\n",
        "      return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SAIGBranch(img_size={self.img_size}, patch_size={self.patch_size}, in_channels={self.in_channels}, embed_dim={self.embed_dim}, num_heads={self.num_heads}, depth={self.depth}, smd_dim={self.smd_dim}, qkv_bias={self.qkv_bias}, qk_scale={self.qk_scale}, drop_rate={self.drop_rate}, attn_drop_rate={self.attn_drop_rate}, norm_layer={self.norm_layer}, flatten={self.flatten})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNh1Gez5-j8h"
      },
      "source": [
        "# Generic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuDuS7IUhBpr"
      },
      "source": [
        "In this section we implemented the **generic model** class, which will be inherited by every model we will employ later.\n",
        "\n",
        "Some methods are left abstract and should be implemented following certain rules:\n",
        "\n",
        "* the **forward** function takes the whole batch as input, it has to divide it,\n",
        "apply the branches and return 3 elements: a tuple with all the output of all\n",
        "the embeddings, the total ground embedding and the total satellite embedding\n",
        "\n",
        "* the **compute_loss** function takes the tuple with all the embeddings and the\n",
        "two total embeddings as input, compute the loss and return it\n",
        "\n",
        "* the __repr__ function returns a string representing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap-bDZT2-mpU"
      },
      "outputs": [],
      "source": [
        "# @title Multi-Branch Model\n",
        "\n",
        "class MultiBranchModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, loss):\n",
        "        super(MultiBranchModel, self).__init__()\n",
        "\n",
        "        self.loss = loss\n",
        "\n",
        "        self.branch_grd = None\n",
        "        self.branch_grd_seg = None\n",
        "        self.branch_grd_dep = None\n",
        "        self.branch_sat = None\n",
        "        self.branch_sat_seg = None\n",
        "\n",
        "        self.grd_features_train = []\n",
        "        self.sat_features_train = []\n",
        "\n",
        "        self.grd_features_val =  []\n",
        "        self.sat_features_val =  []\n",
        "\n",
        "        self.grd_features_test = []\n",
        "        self.sat_features_test = []\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, batch):\n",
        "        return\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        return\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        embeddings, grd_emb, sat_emb = self(batch)\n",
        "\n",
        "        loss = self.compute_loss(embeddings, grd_emb, sat_emb)\n",
        "        top_1 = top_k_rank_accuracy(grd_emb, sat_emb, k=1)\n",
        "\n",
        "        self.log('train_top1', top_1, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, grd_emb, sat_emb = self(batch)\n",
        "\n",
        "        self.grd_features_val.append(grd_emb)\n",
        "        self.sat_features_val.append(sat_emb)\n",
        "\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "\n",
        "      grd_features_val = torch.cat(self.grd_features_val, dim=0)\n",
        "      sat_features_val = torch.cat(self.sat_features_val, dim=0)\n",
        "\n",
        "      num_samples = grd_features_val.shape[0]\n",
        "      percent1 = int(0.01*num_samples)\n",
        "\n",
        "      top_1 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=1)\n",
        "      top_3 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=3)\n",
        "      top_10 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=10)\n",
        "      top_percent1 = top_k_rank_accuracy(grd_features_val, sat_features_val, k=percent1)\n",
        "\n",
        "      self.log('val_top1', top_1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top3', top_3, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top10', top_10, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('val_top1%', top_percent1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "      self.grd_features_val.clear()\n",
        "      self.sat_features_val.clear()\n",
        "      del grd_features_val, sat_features_val\n",
        "\n",
        "      return top_1, top_3, top_10, top_percent1\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, grd_emb, sat_emb = self(batch)\n",
        "\n",
        "        self.grd_features_test.append(grd_emb)\n",
        "        self.sat_features_test.append(sat_emb)\n",
        "\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "\n",
        "      grd_features_test = torch.cat(self.grd_features_test, dim = 0)\n",
        "      sat_features_test = torch.cat(self.sat_features_test, dim = 0)\n",
        "\n",
        "      num_samples = grd_features_test.shape[0]\n",
        "      percent1 = int(0.01*num_samples)\n",
        "\n",
        "      top_1 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=1)\n",
        "      top_3 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=3)\n",
        "      top_10 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=10)\n",
        "      top_percent1 = top_k_rank_accuracy(grd_features_test, sat_features_test, k=percent1)\n",
        "\n",
        "      self.log('test_top1', top_1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top3', top_3, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top10', top_10, on_step=False, on_epoch=True, prog_bar=True)\n",
        "      self.log('test_top1%', top_percent1, on_step=False, on_epoch=True, prog_bar=True)\n",
        "\n",
        "      self.grd_features_test.clear()\n",
        "      self.sat_features_test.clear()\n",
        "      del grd_features_test, sat_features_test\n",
        "\n",
        "      return top_1, top_3, top_10, top_percent1\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def __repr__(self):\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RouQUwE2nWua"
      },
      "source": [
        "# Dual Model (RGB Grd | RGB Sat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2--P6cntwrB"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1GiA7Bi3xAkRg5j_NiimIqaVc4v5XujXZ' />\n",
        "<figcaption>Dual Model architecture</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "The architecture is proposed as the baseline model. It uses two branches to extract features of the ground and of the satellite images, and projects them into a latent space, describing them as vectors.\n",
        "\n",
        "To match the correct images, we compute the distances between the descriptors of the satellite images and the vectors of the ground images. Specifically, we create a distance matrix where each element represents the distance between a descriptor from a satellite image and a vector from a ground image. By calculating these distances, we can identify pairs of satellite and ground images that are closest to each other in the latent space, thereby matching the images accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IYilREkEAmgW"
      },
      "outputs": [],
      "source": [
        "# @title Dual Model\n",
        "\n",
        "class DualModel(MultiBranchModel):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_sat,\n",
        "        loss = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "    ):\n",
        "        super(DualModel, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch_grd = model_grd\n",
        "        self.branch_sat = model_sat\n",
        "\n",
        "        # check output dimension\n",
        "        if self.branch_grd.output_dim != self.branch_sat.output_dim:\n",
        "          raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch_grd.output_dim\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_imgs, _, _, sat_imgs, _ = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_emb = self.branch_grd(grd_imgs['imgs'])\n",
        "        sat_emb = self.branch_sat(sat_imgs['imgs'])\n",
        "\n",
        "        return (grd_emb, sat_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DualModel(model_grd={self.branch_grd}, model_sat={self.branch_sat}, loss={self.loss})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs24fr2ZwAxD"
      },
      "outputs": [],
      "source": [
        "#@title Create Model\n",
        "\n",
        "branch_type = 'resnet'\n",
        "\n",
        "###\n",
        "\n",
        "if branch_type == 'resnet':\n",
        "\n",
        "    grd_model = ResNetBranch(output_dim=128, resnet_version = 50)\n",
        "    sat_model = ResNetBranch(output_dim=128,  resnet_version = 50)\n",
        "\n",
        "\n",
        "elif branch_type == 'saig':\n",
        "\n",
        "    grd_model = SAIGBranch(\n",
        "        data_module.size['grd'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8\n",
        "    )\n",
        "\n",
        "    sat_model = SAIGBranch(\n",
        "        data_module.size['sat'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8\n",
        "    )\n",
        "\n",
        "\n",
        "elif branch_type == 'vgg':\n",
        "\n",
        "    # Output channel of each layer in the convolution layers\n",
        "    # \"M\" stands for maxpooling layer\n",
        "    #VGG16 = [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"]\n",
        "    #VGG16 = [32, 32, \"M\", 64, 64, \"M\", 128, 128, 128, \"M\", 256, 256, 256, \"M\", 256, 256, 256, \"M\"]\n",
        "    VGG16 = [16, 16, \"M\", 32, 32, \"M\", 64, 64, 64, \"M\", 128, 128, 128, \"M\", 128, 128, 128, \"M\"]\n",
        "\n",
        "    grd_model = VGGBranch(data_module.size['grd'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "    sat_model = VGGBranch(data_module.size['sat'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "else:\n",
        "\n",
        "  raise ValueError(\"Should specify some model between the chosable ones\")\n",
        "\n",
        "###\n",
        "\n",
        "model = DualModel(grd_model, sat_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "fVD0l4tD4Byc"
      },
      "outputs": [],
      "source": [
        "# @title Create Trainer\n",
        "\n",
        "logger = TensorBoardLogger(\"/content/drive/MyDrive/CV Project/tb_logs\", name=\"dual_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='dual_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=3, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps = 3,\n",
        "    default_root_dir = \"/content/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p57rNWjHN1hh"
      },
      "outputs": [],
      "source": [
        "# @title Train\n",
        "\n",
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dubja7aKNutk"
      },
      "outputs": [],
      "source": [
        "# @title Test\n",
        "\n",
        "trainer.test(\n",
        "    model = model,\n",
        "    dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eQpKOJZarI6"
      },
      "source": [
        "# \"Enriched\" Dual Model (ALL Grd | ALL Sat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1_Se6LbYgS-9fHi47wd37i7cr8KXpUzUJ'/>\n",
        "<figcaption>Enriched Dual Model architecture</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This architecture follows the same structure of the simple Dual Model, with the difference that each branch takes as input more images (concatenated along the channel dimension) in order to make it able to combine them together optimally. The branch models had to be appropiately modified to be able to accept this kind of input.\n",
        "\n"
      ],
      "metadata": {
        "id": "yuPU1tLSVq4F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iDykf1JZdeIE"
      },
      "outputs": [],
      "source": [
        "# @title Dual Model with all features\n",
        "\n",
        "class DualModel_all(MultiBranchModel):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_sat,\n",
        "        loss = InfoNCE(loss_function=nn.CrossEntropyLoss())\n",
        "    ):\n",
        "        super(DualModel_all, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch_grd = model_grd\n",
        "        self.branch_sat = model_sat\n",
        "\n",
        "        # check output dimension\n",
        "        if self.branch_grd.output_dim != self.branch_sat.output_dim:\n",
        "          raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch_grd.output_dim\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_img, grd_seg, grd_depth, sat_img, sat_seg = batch\n",
        "        grd_tot = torch.cat((grd_img['imgs'], grd_seg['imgs'], grd_depth['imgs']), dim = 1)\n",
        "        sat_tot = torch.cat((sat_img['imgs'], sat_seg['imgs']), dim = 1)\n",
        "\n",
        "        # apply the models\n",
        "        grd_emb = self.branch_grd(grd_tot)\n",
        "        sat_emb = self.branch_sat(sat_tot)\n",
        "\n",
        "        return (grd_emb, sat_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DualModel(model_grd={self.branch_grd}, model_sat={self.branch_sat}, loss={self.loss})\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XHHxUnbarI6",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create Model\n",
        "\n",
        "branch_type = 'resnet'\n",
        "\n",
        "###\n",
        "\n",
        "if branch_type == 'resnet':\n",
        "\n",
        "    grd_model = ResNetBranch(output_dim = 512, resnet_version = 152, input_images = 3)\n",
        "    sat_model = ResNetBranch(output_dim = 512,  resnet_version = 152, input_images = 2)\n",
        "\n",
        "\n",
        "elif branch_type == 'saig':\n",
        "\n",
        "    grd_model = SAIGBranch(\n",
        "        data_module.size['grd'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8,\n",
        "        in_channels = 9\n",
        "    )\n",
        "\n",
        "    sat_model = SAIGBranch(\n",
        "        data_module.size['sat'],\n",
        "        embed_dim= 256,\n",
        "        num_heads = 4,\n",
        "        depth = 4,\n",
        "        smd_dim = 8,\n",
        "        in_channels = 6\n",
        "    )\n",
        "\n",
        "\n",
        "elif branch_type == 'vgg':\n",
        "\n",
        "    # Output channel of each layer in the convolution layers\n",
        "    # \"M\" stands for maxpooling layer\n",
        "    #VGG16 = [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"]\n",
        "    #VGG16 = [32, 32, \"M\", 64, 64, \"M\", 128, 128, 128, \"M\", 256, 256, 256, \"M\", 256, 256, 256, \"M\"]\n",
        "    VGG16 = [16, 16, \"M\", 32, 32, \"M\", 64, 64, 64, \"M\", 128, 128, 128, \"M\", 128, 128, 128, \"M\"]\n",
        "\n",
        "    grd_model = VGGBranch(data_module.size['grd'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "    sat_model = VGGBranch(data_module.size['sat'], VGG16, VGG16.count(\"M\"), output_dim = 100)\n",
        "\n",
        "\n",
        "else:\n",
        "\n",
        "  raise ValueError(\"Should specify some model between the chosable ones\")\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "model = DualModel_all(grd_model, sat_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIaRk-C6arI6"
      },
      "outputs": [],
      "source": [
        "# @title Create Trainer\n",
        "\n",
        "logger = TensorBoardLogger(\"/content/drive/MyDrive/CV Project/tb_logs\", name=\"dual_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='dual_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs = 30,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=44, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps = 3,\n",
        "    default_root_dir = \"/content/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMjC0JgUarI7"
      },
      "outputs": [],
      "source": [
        "# @title Train\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=val_loader,\n",
        "    ckpt_path=\"/content/drive/MyDrive/CV Project/tb_logs/dual_model/version_12/checkpoints/dual_model-epoch=14-val_top1=0.16.ckpt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "267LCO1WarI7"
      },
      "outputs": [],
      "source": [
        "# @title Test\n",
        "\n",
        "trainer.test(\n",
        "    model = model,\n",
        "    dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDT8x2WPpbC"
      },
      "source": [
        "# Triple Model (RGB Grd | RGB + SEG Sat) and (RGB Sat | RGB + SEG Grd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RqMbboA0RLs"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=17ql5CPDt4zYVDweuADsO5brW-ikqGzXK' />\n",
        "<figcaption>Triple Model architecture using satellite segmentation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "The architecture is proposed as a slight modification of the architecture used in the original paper. It uses three branches to extract features of the ground, satellite images and satellite segmentation images, and projects them into a latent space, describing them as vectors.\n",
        "\n",
        "Differently from the Dual Model, we first concatenate (using also a fully connected layer to make such an operation differentiable) the embeddings of the two satellite images, then like the dual model we evaluate the distance matrix over such vectors.\n",
        "\n",
        "Another difference from the previous model is the possibility of defining an auxilary loss that works on the embeddings produced by the two sub-branches related to satellite images. By minimizing such a loss the idea is that the model is able to learn a good representation of such embeddings in a more direct way making them less dependent on layers of the network that appear later. During our tentatives, the models using such auxilary losses exhibited better performances in early epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ARdskEYANcL"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1bEYjAd5i-B5YQ1GHsbDkZ12a0dYx28--' />\n",
        "<figcaption>Triple Model architecture using ground segmentation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "An alternative approach with the triple model was tried. The proposed alternative architecture uses three branches to extract features of the ground, ground segmentation images, satellite images, and projects them into a latent space, describing them as vectors.\n",
        "\n",
        "Like the previous model, we concatenate the embeddings to then compare the vectors describing the input images to match the right images.\n",
        "\n",
        "Even in this case, we tried to employ an auxilary loss (in this case defined on the embeddings resulting from the ground sub-branches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GJBtgoPJUyyY"
      },
      "outputs": [],
      "source": [
        "#@title Triple Model satellite using multiple losses\n",
        "\n",
        "class TripleModel_sat(MultiBranchModel):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_sat,\n",
        "        model_sat_seg,\n",
        "        loss=InfoNCE(loss_function=nn.CrossEntropyLoss()),\n",
        "        fully_concat=True,\n",
        "        multiple_losses=True,\n",
        "        auxilary_loss_weight = 0.5\n",
        "    ):\n",
        "        super(TripleModel_sat, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch_grd = model_grd\n",
        "        self.branch_sat = model_sat\n",
        "        self.branch_sat_seg = model_sat_seg\n",
        "\n",
        "        # check output dimension\n",
        "        sat_output_dim = self.branch_sat.output_dim + self.branch_sat_seg.output_dim\n",
        "        if self.branch_grd.output_dim != sat_output_dim:\n",
        "            raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch_grd.output_dim\n",
        "\n",
        "        self.fully_concat = fully_concat\n",
        "        self.multiple_losses = multiple_losses\n",
        "\n",
        "        # To make concatenation learnable\n",
        "        if fully_concat:\n",
        "            self.fc = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        self.auxilary_loss_weight = auxilary_loss_weight #used for multiple losses only\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_imgs, _, _, sat_imgs, sat_seg = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_emb = self.branch_grd(grd_imgs['imgs'])\n",
        "        sat_rgb_emb = self.branch_sat(sat_imgs['imgs'])\n",
        "        sat_seg_emb = self.branch_sat_seg(sat_seg['imgs'])\n",
        "\n",
        "        # compute the total embeddings\n",
        "        sat_emb = torch.cat((sat_rgb_emb, sat_seg_emb), dim=1)\n",
        "        if self.fully_concat:\n",
        "            sat_emb = self.fc(sat_emb)\n",
        "\n",
        "        return (grd_emb, sat_rgb_emb, sat_seg_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        if self.multiple_losses:\n",
        "            loss += self.auxilary_loss_weight*self.loss(embeddings[1], embeddings[2])\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"TripleModel_sat(model_grd={self.branch_grd}, model_sat={self.branch_sat},\n",
        "        model_sat_seg={self.branch_sat_seg}, loss={self.loss}, fully_concat={self.fully_concat},\n",
        "        multiple_losses={self.multiple_losses}, auxilary_loss_weight = {self.auxilary_loss_weight})\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NVkdN6KHiMaS"
      },
      "outputs": [],
      "source": [
        "#@title Triple Model ground with multiple losses\n",
        "class TripleModel_grd(MultiBranchModel):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_sat,\n",
        "        model_grd_seg,\n",
        "        loss=InfoNCE(loss_function=nn.CrossEntropyLoss()),\n",
        "        fully_concat=True,\n",
        "        multiple_losses=True,\n",
        "        auxilary_loss_weight = 0.5\n",
        "    ):\n",
        "        super(TripleModel_grd, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch_grd = model_grd\n",
        "        self.branch_grd_seg = model_grd_seg\n",
        "        self.branch_sat = model_sat\n",
        "\n",
        "        # check output dimension\n",
        "        grd_output_dim = self.branch_grd.output_dim + self.branch_grd_seg.output_dim\n",
        "        if self.branch_sat.output_dim != grd_output_dim:\n",
        "            raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = self.branch_sat.output_dim\n",
        "\n",
        "        self.fully_concat = fully_concat\n",
        "        self.multiple_losses = multiple_losses\n",
        "\n",
        "        # to make concatenation learnable\n",
        "        if fully_concat:\n",
        "            self.fc = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        self.auxilary_loss_weight = auxilary_loss_weight #used for multiple losses only\n",
        "\n",
        "        self.loss = loss\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_imgs, grd_seg, _, sat_imgs, _ = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_rgb_emb = self.branch_grd(grd_imgs['imgs'])\n",
        "        grd_seg_emb = self.branch_grd_seg(grd_seg['imgs'])\n",
        "        sat_emb = self.branch_sat(sat_imgs['imgs'])\n",
        "\n",
        "        # compute the total embeddings\n",
        "        grd_emb = torch.cat((grd_rgb_emb, grd_seg_emb), dim=1)\n",
        "        if self.fully_concat:\n",
        "            grd_emb = self.fc(grd_emb)\n",
        "\n",
        "        return (grd_rgb_emb, grd_seg_emb, sat_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        if self.multiple_losses:\n",
        "            loss += self.auxilary_loss_weight*self.loss(embeddings[0], embeddings[1])\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"TripleModel_grd(model_grd={self.branch_grd}, model_sat={self.branch_grd_seg},\n",
        "        model_sat_seg={self.branch_sat}, loss={self.loss}, fully_concat={self.fully_concat},\n",
        "        multiple_losses={self.multiple_losses}, auxilary_loss_weight = {self.auxilary_loss_weight})\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gG7fYg6cSrji"
      },
      "outputs": [],
      "source": [
        "# @title Create Model\n",
        "grd_model = ResNetBranch(output_dim = 512, resnet_version = 50) #LOOKS PROMISING!!!\n",
        "sat_model = ResNetBranch(output_dim = 256,  resnet_version = 50)\n",
        "seg_model = ResNetBranch(output_dim = 256,  resnet_version = 50)\n",
        "\n",
        "\"\"\"grd_model = SAIGBranch(\n",
        "        data_module.size['grd'],\n",
        "        embed_dim= 384,\n",
        "        num_heads = 8,\n",
        "        depth = 4,\n",
        "        smd_dim = 12\n",
        "    )\n",
        "\n",
        "sat_model = SAIGBranch(\n",
        "        data_module.size['sat'],\n",
        "        embed_dim= 384,\n",
        "        num_heads = 8,\n",
        "        depth = 4,\n",
        "        smd_dim = 6\n",
        "    )\n",
        "\n",
        "seg_model = SAIGBranch(\n",
        "        data_module.size['sat_seg'],\n",
        "        embed_dim= 384,\n",
        "        num_heads = 8,\n",
        "        depth = 4,\n",
        "        smd_dim = 6\n",
        "    )\n",
        "\"\"\"\n",
        "\n",
        "model = TripleModel_sat(model_grd=grd_model, model_sat=sat_model, model_sat_seg=seg_model, loss = InfoNCE(loss_function = nn.CrossEntropyLoss(), logit_scale = 5.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iHcC07AUUhtD"
      },
      "outputs": [],
      "source": [
        "#@title Create Trainer\n",
        "logger = TensorBoardLogger(\"/content/drive/MyDrive/CV Project/tb_logs\", name=\"triple_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='triple_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs = 100,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=44, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OWIVZLEGUlJf"
      },
      "outputs": [],
      "source": [
        "# @title Train\n",
        "trainer.fit(\n",
        "    model = model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7-d3ympNUoWu"
      },
      "outputs": [],
      "source": [
        "#@title Test\n",
        "trainer.test(\n",
        "    dataloaders = val_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9YaK9QS2VN5"
      },
      "source": [
        "# Quadruple Model (RGB + SEG Grd | RGB + SEG Sat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrjyQvVzB791"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1kza9BhLzN6aHlbb-YhIUzlCD07Vv0hBW' />\n",
        "<figcaption>Quadruple Model architecture using ground and satellite segmentation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This model further extends the previous approaches, combining all the possible features we have about segmentation of both Ground and Satellite images.\n",
        "\n",
        "As for the previous models, the use of auxilary Losses improved the learning of the models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "q0pz4W3jYN95"
      },
      "outputs": [],
      "source": [
        "# @title Quadruple Model\n",
        "\n",
        "class QuadrupleModel(MultiBranchModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_grd_seg,\n",
        "        model_sat,\n",
        "        model_sat_seg,\n",
        "        loss=InfoNCE(loss_function=nn.CrossEntropyLoss()),\n",
        "        multiple_losses=True,\n",
        "        fully_concat_grd=True,\n",
        "        fully_concat_sat=True,\n",
        "        auxilary_loss_weight = 0.5\n",
        "    ):\n",
        "        super(QuadrupleModel, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch_grd = model_grd\n",
        "        self.branch_grd_seg = model_grd_seg\n",
        "        self.branch_sat = model_sat\n",
        "        self.branch_sat_seg = model_sat_seg\n",
        "\n",
        "        # check output dimension\n",
        "        grd_output_dim = self.branch_grd.output_dim + self.branch_grd_seg.output_dim\n",
        "        sat_output_dim = self.branch_sat.output_dim + self.branch_sat_seg.output_dim\n",
        "        if grd_output_dim != sat_output_dim:\n",
        "          raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = grd_output_dim\n",
        "\n",
        "        self.fully_concat_grd = fully_concat_grd\n",
        "        self.fully_concat_sat = fully_concat_sat\n",
        "\n",
        "        self.multiple_losses = multiple_losses\n",
        "\n",
        "        # To make concatenations learnable\n",
        "        if self.fully_concat_grd:\n",
        "          self.fc_grd = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        if self.fully_concat_sat:\n",
        "          self.fc_sat = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        self.auxilary_loss_weight = auxilary_loss_weight\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_img, grd_seg, _, sat_img, sat_seg = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_rgb_emb = self.branch_grd(grd_img['imgs'])\n",
        "        grd_seg_emb = self.branch_grd_seg(grd_seg['imgs'])\n",
        "        sat_rgb_emb = self.branch_sat(sat_img['imgs'])\n",
        "        sat_seg_emb = self.branch_sat_seg(sat_seg['imgs'])\n",
        "\n",
        "        # compute the total embeddings\n",
        "        grd_emb = torch.cat((grd_rgb_emb, grd_seg_emb), dim=1)\n",
        "        sat_emb = torch.cat((sat_rgb_emb, sat_seg_emb), dim=1)\n",
        "        if self.fully_concat_grd:\n",
        "          grd_emb = self.fc_grd(grd_emb)\n",
        "        if self.fully_concat_sat:\n",
        "          sat_emb = self.fc_sat(sat_emb)\n",
        "\n",
        "        return (grd_rgb_emb, grd_seg_emb, sat_rgb_emb, sat_seg_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        if self.multiple_losses:\n",
        "            loss += self.auxilary_loss_weight*self.loss(embeddings[0], embeddings[1])\n",
        "            loss += self.auxilary_loss_weight*self.loss(embeddings[2], embeddings[3])\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"QuadrupleModel(model_grd={self.branch_grd}, model_grd_seg={self.branch_grd_seg},\n",
        "        model_sat={self.branch_sat}, model_sat_seg={self.branch_sat_seg}, loss={self.loss},\n",
        "        multiple_losses={self.multiple_losses}, fully_concat_grd={self.fully_concat_grd},\n",
        "        fully_concat_sat={self.fully_concat_sat}, auxilary_loss_weight = {self.auxilary_loss_weight})\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YApHu9J0_Sva"
      },
      "outputs": [],
      "source": [
        "# @title Create Model\n",
        "\n",
        "grd_model = ResNetBranch(output_dim = 512, resnet_version = 101) # LOOKS PROMISING!!!\n",
        "grd_seg_model = ResNetBranch(output_dim = 512, resnet_version = 101)\n",
        "sat_model = ResNetBranch(output_dim = 512,  resnet_version = 101)\n",
        "sat_seg_model = ResNetBranch(output_dim = 512,  resnet_version = 101)\n",
        "\n",
        "\"\"\"grd_model = SAIGBranch(\n",
        "    data_module.size['grd'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "grd_seg_model = SAIGBranch(\n",
        "    data_module.size['grd_seg'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "sat_model = SAIGBranch(\n",
        "    data_module.size['sat'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "sat_seg_model = SAIGBranch(\n",
        "    data_module.size['sat_seg'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\"\"\"\n",
        "\n",
        "# model = QuadrupleModel(model_grd=grd_model, model_grd_seg=grd_seg_model, model_sat=sat_model, model_sat_seg=sat_seg_model, loss=InfoNCE(loss_function=nn.CrossEntropyLoss(), logit_scale=5.0), multiple_losses=False)\n",
        "model = QuadrupleModel(model_grd=grd_model, model_grd_seg=grd_seg_model, model_sat=sat_model, model_sat_seg=sat_seg_model, loss=TripletLoss(), multiple_losses=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alcJP_ZfA8kS"
      },
      "outputs": [],
      "source": [
        "#@title Create Trainer\n",
        "logger = TensorBoardLogger(\"/content/drive/MyDrive/CV Project/tb_logs\", name=\"quadruple_model_triplet_loss_multiple_losses\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='quadruple_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs=100,\n",
        "    devices=1,\n",
        "    callbacks=[RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=44, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz5-lu_lA-Kv"
      },
      "outputs": [],
      "source": [
        "# @title Train\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=val_loader,\n",
        "    # ckpt_path=\"/content/drive/MyDrive/CV Project/tb_logs/quadruple_model/version_3/checkpoints/quadruple_model-epoch=21-val_top1=0.07.ckpt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2OInjw71A_xp"
      },
      "outputs": [],
      "source": [
        "#@title Test\n",
        "trainer.test(\n",
        "    dataloaders=val_loader,\n",
        "    ckpt_path=\"last\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F2TfTrGIKju"
      },
      "source": [
        "# Quintuple Model (RGB + SEG + DEPTH Grd | RGB + SEG Sat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goFmBE3sHGnZ"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1TD552ZImNnnNAsVFQvzGe4oVYxkC_nBE' />\n",
        "<figcaption>Quintuple Model architecture using ground and satellite segmentation + ground depth estimation</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "The same thought process as before. However, here we include the ground depth estimation as an input in the ground sub-brances. The idea is that the depth in the ground image, can be mapped to the distances in the satellite image.\n",
        "\n",
        "Again, the use of auxillary losses, improves the learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zOK0sSntasYq"
      },
      "outputs": [],
      "source": [
        "# @title Quintuple Model\n",
        "\n",
        "class QuintupleModel(MultiBranchModel):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_grd,\n",
        "        model_grd_seg,\n",
        "        model_grd_dep,\n",
        "        model_sat,\n",
        "        model_sat_seg,\n",
        "        loss=InfoNCE(loss_function=nn.CrossEntropyLoss()),\n",
        "        multiple_losses=True,\n",
        "        fully_concat_grd=True,\n",
        "        fully_concat_sat=True,\n",
        "        auxilary_loss_weight = 0.5\n",
        "    ):\n",
        "        super(QuintupleModel, self).__init__(loss)\n",
        "\n",
        "        # create branches\n",
        "        self.branch_grd = model_grd\n",
        "        self.branch_grd_seg = model_grd_seg\n",
        "        self.branch_grd_dep = model_grd_dep\n",
        "        self.branch_sat = model_sat\n",
        "        self.branch_sat_seg = model_sat_seg\n",
        "\n",
        "        # check output dimension\n",
        "        grd_output_dim = self.branch_grd.output_dim + self.branch_grd_seg.output_dim + self.branch_grd_dep.output_dim\n",
        "        sat_output_dim = self.branch_sat.output_dim + self.branch_sat_seg.output_dim\n",
        "        if grd_output_dim != sat_output_dim:\n",
        "          raise ValueError(\"Mismatching output dimensions for the branches!\")\n",
        "        self.output_dim = grd_output_dim\n",
        "\n",
        "        self.fully_concat_grd = fully_concat_grd\n",
        "        self.fully_concat_sat = fully_concat_sat\n",
        "\n",
        "        self.multiple_losses = multiple_losses\n",
        "\n",
        "        # To make concatenations learnable\n",
        "        if self.fully_concat_grd:\n",
        "          self.fc_grd = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        if self.fully_concat_sat:\n",
        "          self.fc_sat = nn.Linear(self.output_dim, self.output_dim)\n",
        "\n",
        "        self.auxilary_loss_weight = auxilary_loss_weight\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # elaborate the batch\n",
        "        grd_img, grd_seg, grd_dep, sat_img, sat_seg = batch\n",
        "\n",
        "        # apply the models\n",
        "        grd_rgb_emb = self.branch_grd(grd_img['imgs'])\n",
        "        grd_seg_emb = self.branch_grd_seg(grd_seg['imgs'])\n",
        "        grd_dep_emb = self.branch_grd_dep(grd_dep['imgs'])\n",
        "        sat_rgb_emb = self.branch_sat(sat_img['imgs'])\n",
        "        sat_seg_emb = self.branch_sat_seg(sat_seg['imgs'])\n",
        "\n",
        "        # compute the total embeddings\n",
        "        grd_emb = torch.cat((grd_rgb_emb, grd_seg_emb, grd_dep_emb), dim=1)\n",
        "        sat_emb = torch.cat((sat_rgb_emb, sat_seg_emb), dim=1)\n",
        "        if self.fully_concat_grd:\n",
        "          grd_emb = self.fc_grd(grd_emb)\n",
        "        if self.fully_concat_sat:\n",
        "          sat_emb = self.fc_sat(sat_emb)\n",
        "\n",
        "        return (grd_rgb_emb, grd_seg_emb, grd_dep_emb, sat_rgb_emb, sat_seg_emb), grd_emb, sat_emb\n",
        "\n",
        "\n",
        "    def compute_loss(self, embeddings, grd_emb, sat_emb):\n",
        "        loss = self.loss(grd_emb, sat_emb)\n",
        "        if self.multiple_losses:\n",
        "            loss += self.auxilary_loss_weight*self.loss(embeddings[0], embeddings[1])\n",
        "            loss += self.auxilary_loss_weight*self.loss(embeddings[0], embeddings[2])\n",
        "            loss += self.auxilary_loss_weight*self.loss(embeddings[1], embeddings[2])\n",
        "            loss += self.auxilary_loss_weight*self.loss(embeddings[3], embeddings[4])\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"\"\"QuintupleModel(model_grd={self.branch_grd}, model_grd_seg={self.branch_grd_seg},\n",
        "        model_grd_dep={self.branch_grd_dep}, model_sat={self.branch_sat}, model_sat_seg={self.branch_sat_seg},\n",
        "        loss={self.loss}, multiple_losses={self.multiple_losses},\n",
        "        fully_concat_grd={self.fully_concat_grd}, fully_concat_sat={self.fully_concat_sat}, auxilary_loss_weight = {self.auxilary_loss_weight})\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSkz4jbRLsJM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Create Model\n",
        "\n",
        "grd_model = ResNetBranch(output_dim = 256, resnet_version = 50, finetuning = 'freeze')\n",
        "grd_seg_model = ResNetBranch(output_dim = 256, resnet_version = 50)\n",
        "grd_dep_model = ResNetBranch(output_dim = 256, resnet_version = 50)\n",
        "sat_model = ResNetBranch(output_dim = 384,  resnet_version = 50, finetuning = 'freeze')\n",
        "sat_seg_model = ResNetBranch(output_dim = 384,  resnet_version = 50)\n",
        "\n",
        "\"\"\"\n",
        "grd_model = SAIGBranch(\n",
        "    data_module.size['grd'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "grd_seg_model = SAIGBranch(\n",
        "    data_module.size['grd_seg'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "grd_dep_model = SAIGBranch(\n",
        "    data_module.size['grd_dep'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 6\n",
        ")\n",
        "\n",
        "sat_model = SAIGBranch(\n",
        "    data_module.size['sat'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 9\n",
        ")\n",
        "\n",
        "sat_seg_model = SAIGBranch(\n",
        "    data_module.size['sat_seg'],\n",
        "    embed_dim= 384,\n",
        "    num_heads = 8,\n",
        "    depth = 4,\n",
        "    smd_dim = 9\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "model = QuintupleModel(\n",
        "    model_grd=grd_model,\n",
        "    model_grd_seg=grd_seg_model,\n",
        "    model_grd_dep=grd_dep_model,\n",
        "    model_sat=sat_model,\n",
        "    model_sat_seg=sat_seg_model,\n",
        "    loss=InfoNCE(loss_function=nn.CrossEntropyLoss(), logit_scale=5.0),\n",
        "    auxilary_loss_weight = 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCvovcR0MU3t",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Create Trainer\n",
        "logger = TensorBoardLogger(\"/content/drive/MyDrive/CV Project/tb_logs\", name=\"quintuple_model\")\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    filename='quintuple_model-{epoch}-{val_top1:.2f}',\n",
        "    mode=\"max\",\n",
        "    every_n_epochs=1,\n",
        "    save_top_k=1,\n",
        "    config=repr(model)\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=logger,\n",
        "    max_epochs=100,\n",
        "    devices=1,\n",
        "    callbacks=[RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=44, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps=3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFqhLQjxMWLa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Train\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=val_loader,\n",
        "    ckpt_path=\"/content/drive/MyDrive/CV Project/tb_logs/quintuple_model/version_1/checkpoints/quintuple_model-epoch=32-val_top1=0.27.ckpt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(\n",
        "    logger = logger,\n",
        "    max_epochs = 100,\n",
        "    devices = 1,\n",
        "    callbacks = [RichProgressBar(), EarlyStopping(monitor=\"val_top1\", patience=44, mode=\"max\"), checkpoint_callback],\n",
        "    log_every_n_steps = 3\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    model=model,\n",
        "    train_dataloaders = train_loader,\n",
        "    val_dataloaders = val_loader,\n",
        "    ckpt_path = \"/content/drive/MyDrive/CV Project/tb_logs/quintuple_model/version_13/checkpoints/quintuple_model-epoch=74-val_top1=0.45.ckpt\"\n",
        ")"
      ],
      "metadata": {
        "id": "GlchFCWYjng5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36YgM7dBMXli"
      },
      "outputs": [],
      "source": [
        "# @title Test\n",
        "\n",
        "trainer.test(\n",
        "    dataloaders = val_loader,\n",
        "    ckpt_path = \"/content/drive/MyDrive/CV Project/tb_logs/quintuple_model/version_13/checkpoints/quintuple_model-epoch=74-val_top1=0.45.ckpt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJAKm9G3-Kwd"
      },
      "source": [
        "# Visualization Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9JYT-4Jtod7"
      },
      "source": [
        "Here we implement two functions to visualize the heatmap and the ranked results from the model's predictions.\n",
        "\n",
        "The **create_activation_maps** function uses the branches of the model (in an appropiate mode) to compute the output of the last convolutional layer, obtaining a processed but still image-like tensor. Taking the value of each \"pixel\" across the channels we can compute the importance the model has given to that region of the image.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <figure>\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1z6g5ZnxOWPsLW_-Pu90nhHm-EUWemAvc' alt='Visualization of Activations of Ground Branch' style=\"width: 300px; height: 300px;\">\n",
        "        <figcaption>Visualization of Ground Branch</figcaption>\n",
        "      </figure>\n",
        "    </td>\n",
        "    <td>\n",
        "      <figure>\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1OTBQx-uhoKMY6QlA-Bjih_5bdyauunBa' alt='Visualization of Activations of Ground Segmentation Branch' style=\"width: 300px; height: 300px;\">\n",
        "        <figcaption>Visualization Ground Segmentation Branch</figcaption>\n",
        "      </figure>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "The **visualize_ranked_results** function uses the model to perform a complete evaluation step and then show the top satellite matches for each ground image showing if they are correct and what are they distances from the ground image.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td>\n",
        "      <figure>\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1T1FIvJKkanSgZtkJeVLBBKo1N0m8iqKu' alt='Additional Image 1' style=\"width: 300px; height: 300px;\">\n",
        "        <figcaption>Visualization Rank: img1</figcaption>\n",
        "      </figure>\n",
        "    </td>\n",
        "    <td>\n",
        "      <figure>\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1MF4uJ1RbS71lq0BdseP20TLcHG8UpPu7' alt='Additional Image 2' style=\"width: 300px; height: 300px;\">\n",
        "        <figcaption>Visualization Rank: img2</figcaption>\n",
        "      </figure>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36uMaOGy-NFa"
      },
      "outputs": [],
      "source": [
        "# @title Visualize Heatmap\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def create_activation_maps(\n",
        "    model,\n",
        "    data_module,\n",
        "    split,\n",
        "    save_dir,\n",
        "    use_gpu = True\n",
        "):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    spacing = 10\n",
        "    fading = 0.5\n",
        "\n",
        "    if split == 'train':\n",
        "        data_loader = data_module.train_dataloader()\n",
        "    elif split == 'test':\n",
        "        data_loader = data_module.test_dataloader()\n",
        "    elif split == 'val':\n",
        "        data_loader = data_module.val_dataloader()\n",
        "    else:\n",
        "        raise ValueError('split should be \"train\", \"test\" or \"val\"')\n",
        "\n",
        "    grd_mean, grd_std = data_module.mean['grd'], data_module.std['grd']\n",
        "    grd_seg_mean, grd_seg_std = data_module.mean['grd_seg'], data_module.std['grd_seg']\n",
        "    grd_dep_mean, grd_dep_std = data_module.mean['grd_dep'], data_module.std['grd_dep']\n",
        "    sat_mean, sat_std = data_module.mean['sat'], data_module.std['sat']\n",
        "    sat_seg_mean, sat_seg_std = data_module.mean['sat_seg'], data_module.std['sat_seg']\n",
        "\n",
        "    grd_height, grd_width = data_module.size['grd']\n",
        "    grd_seg_height, grd_seg_width = data_module.size['grd_seg']\n",
        "    grd_dep_height, grd_dep_width = data_module.size['grd_dep']\n",
        "    sat_height, sat_width = data_module.size['sat']\n",
        "    sat_seg_height, sat_seg_width = data_module.size['sat_seg']\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        grd_imgs = batch[0]['imgs']\n",
        "        sat_imgs = batch[3]['imgs']\n",
        "\n",
        "        grd_ids = batch[0]['imgs_id']\n",
        "        sat_ids = batch[3]['imgs_id']\n",
        "\n",
        "        if use_gpu:\n",
        "            grd_imgs = grd_imgs.cuda()\n",
        "            sat_imgs = sat_imgs.cuda()\n",
        "\n",
        "        grd_output = model.branch_grd(grd_imgs, featuremaps=True)\n",
        "        sat_output = model.branch_sat(sat_imgs, featuremaps=True)\n",
        "\n",
        "        # compute activation maps for streetview (try adding square root?)\n",
        "        grd_output = (grd_output**2).sum(1)\n",
        "        b, h, w = grd_output.size()\n",
        "        grd_output = grd_output.view(b, h * w)\n",
        "        grd_output = nn.functional.normalize(grd_output, p=2, dim=1)\n",
        "        grd_output = grd_output.view(b, h, w)\n",
        "\n",
        "        # compute activation maps for satmap\n",
        "        sat_output = (sat_output**2).sum(1)\n",
        "        b, h, w = sat_output.size()\n",
        "        sat_output = sat_output.view(b, h * w)\n",
        "        sat_output = nn.functional.normalize(sat_output, p=2, dim=1)\n",
        "        sat_output = sat_output.view(b, h, w)\n",
        "\n",
        "        if use_gpu:\n",
        "            grd_imgs, grd_output = grd_imgs.cpu(), grd_output.cpu()\n",
        "            sat_imgs, sat_output = sat_imgs.cpu(), sat_output.cpu()\n",
        "\n",
        "\n",
        "        for index in range(grd_output.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            img_id = str(int(grd_ids[index])).zfill(7)\n",
        "\n",
        "            # RGB image (from the normalized input image)\n",
        "            input_img = grd_imgs[index, ...]\n",
        "            for img, mean, std in zip(input_img, grd_mean, grd_std):\n",
        "                img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "            input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "            input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "            # activation map (from the output image)\n",
        "            act_map = grd_output[index, ...].numpy()\n",
        "            act_map = cv2.resize(act_map, (grd_width, grd_height))\n",
        "            act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "            act_map = np.uint8(np.floor(act_map))\n",
        "            act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapping between the two images\n",
        "            overlapped_img = input_img*(1-fading) + act_map*fading\n",
        "            overlapped_img[overlapped_img > 255] = 255\n",
        "            overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            output_img = 255 * np.ones((3*grd_height + 2*spacing, grd_width, 3), dtype=np.uint8)\n",
        "            output_img[:grd_height, ...] = input_img[..., ::-1]\n",
        "            output_img[grd_height + spacing:2*grd_height + spacing, ...] = act_map\n",
        "            output_img[2*grd_height + 2*spacing:, ...] = overlapped_img\n",
        "            os. makedirs(os.path.join(save_dir, img_id))\n",
        "            cv2.imwrite(os.path.join(save_dir, img_id + '/grd.jpg'), output_img)\n",
        "\n",
        "\n",
        "        for index in range(sat_output.size(0)):\n",
        "\n",
        "            # get image name\n",
        "            img_id = str(int(sat_ids[index])).zfill(7)\n",
        "\n",
        "            # RGB image (input image)\n",
        "            input_img = sat_imgs[index, ...]\n",
        "            for img, mean, std in zip(input_img, sat_mean, sat_std):\n",
        "                img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "            input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "            input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "            # activation map\n",
        "            act_map = sat_output[index, ...].numpy()\n",
        "            act_map = cv2.resize(act_map, (sat_width, sat_height))\n",
        "            act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "            act_map = np.uint8(np.floor(act_map))\n",
        "            act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "            # overlapped image\n",
        "            overlapped_img = input_img*(1-fading) + act_map*(fading)\n",
        "            overlapped_img[overlapped_img > 255] = 255\n",
        "            overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "            # save images in a single figure (add white spacing between images)\n",
        "            output_img = 255 * np.ones((3*sat_height + 2*spacing, sat_width, 3), dtype=np.uint8)\n",
        "            output_img[:sat_height, ...] = input_img[..., ::-1]\n",
        "            output_img[sat_height + spacing:2*sat_height + spacing, ...] = act_map\n",
        "            output_img[2*sat_height + 2*spacing:, ...] = overlapped_img\n",
        "            cv2.imwrite(os.path.join(save_dir, img_id + '/sat.jpg'), output_img)\n",
        "\n",
        "\n",
        "        if model.branch_grd_seg != None:\n",
        "            grd_seg_imgs = batch[1]['imgs']\n",
        "            if use_gpu:\n",
        "                grd_seg_imgs = grd_seg_imgs.cuda()\n",
        "\n",
        "            grd_seg_output = model.branch_grd_seg(grd_seg_imgs, featuremaps=True)\n",
        "\n",
        "            grd_seg_output = (grd_seg_output**2).sum(1)\n",
        "            b, h, w = grd_seg_output.size()\n",
        "            grd_seg_output = grd_seg_output.view(b, h * w)\n",
        "            grd_seg_output = nn.functional.normalize(grd_seg_output, p=2, dim=1)\n",
        "            grd_seg_output = grd_seg_output.view(b, h, w)\n",
        "\n",
        "            if use_gpu:\n",
        "                grd_seg_imgs, grd_seg_output = grd_seg_imgs.cpu(), grd_seg_output.cpu()\n",
        "\n",
        "            for index in range(grd_seg_output.size(0)):\n",
        "\n",
        "                # get image name\n",
        "                img_id = str(int(grd_ids[index])).zfill(7)\n",
        "\n",
        "                # RGB image (from the normalized input image)\n",
        "                input_img = grd_seg_imgs[index, ...]\n",
        "                for img, mean, std in zip(input_img, grd_seg_mean, grd_seg_std):\n",
        "                    img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "                input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "                input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "                # activation map (from the output image)\n",
        "                act_map = grd_seg_output[index, ...].numpy()\n",
        "                act_map = cv2.resize(act_map, (grd_seg_width, grd_seg_height))\n",
        "                act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "                act_map = np.uint8(np.floor(act_map))\n",
        "                act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "                # overlapping between the two images\n",
        "                overlapped_img = input_img*(1-fading) + act_map*fading\n",
        "                overlapped_img[overlapped_img > 255] = 255\n",
        "                overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "                # save images in a single figure (add white spacing between images)\n",
        "                output_img = 255 * np.ones((3*grd_seg_height + 2*spacing, grd_seg_width, 3), dtype=np.uint8)\n",
        "                output_img[:grd_seg_height, ...] = input_img[..., ::-1]\n",
        "                output_img[grd_seg_height + spacing:2*grd_seg_height + spacing, ...] = act_map\n",
        "                output_img[2*grd_seg_height + 2*spacing:, ...] = overlapped_img\n",
        "                cv2.imwrite(os.path.join(save_dir, img_id + '/grd_seg.jpg'), output_img)\n",
        "\n",
        "\n",
        "        if model.branch_grd_dep != None:\n",
        "            grd_dep_imgs = batch[2]['imgs']\n",
        "            if use_gpu:\n",
        "                grd_dep_imgs = grd_dep_imgs.cuda()\n",
        "\n",
        "            grd_dep_output = model.branch_grd_dep(grd_dep_imgs, featuremaps=True)\n",
        "\n",
        "            grd_dep_output = (grd_dep_output**2).sum(1)\n",
        "            b, h, w = grd_dep_output.size()\n",
        "            grd_dep_output = grd_dep_output.view(b, h * w)\n",
        "            grd_dep_output = nn.functional.normalize(grd_dep_output, p=2, dim=1)\n",
        "            grd_dep_output = grd_dep_output.view(b, h, w)\n",
        "\n",
        "            if use_gpu:\n",
        "                grd_dep_imgs, grd_dep_output = grd_dep_imgs.cpu(), grd_dep_output.cpu()\n",
        "\n",
        "            for index in range(grd_dep_output.size(0)):\n",
        "\n",
        "                # get image name\n",
        "                img_id = str(int(grd_ids[index])).zfill(7)\n",
        "\n",
        "                # RGB image (from the normalized input image)\n",
        "                input_img = grd_dep_imgs[index, ...]\n",
        "                for img, mean, std in zip(input_img, grd_dep_mean, grd_dep_std):\n",
        "                    img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "                input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "                input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "                # activation map (from the output image)\n",
        "                act_map = grd_dep_output[index, ...].numpy()\n",
        "                act_map = cv2.resize(act_map, (grd_dep_width, grd_dep_height))\n",
        "                act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "                act_map = np.uint8(np.floor(act_map))\n",
        "                act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "                # overlapping between the two images\n",
        "                overlapped_img = input_img*(1-fading) + act_map*fading\n",
        "                overlapped_img[overlapped_img > 255] = 255\n",
        "                overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "                # save images in a single figure (add white spacing between images)\n",
        "                output_img = 255 * np.ones((3*grd_dep_height + 2*spacing, grd_dep_width, 3), dtype=np.uint8)\n",
        "                output_img[:grd_dep_height, ...] = input_img[..., ::-1]\n",
        "                output_img[grd_dep_height + spacing:2*grd_dep_height + spacing, ...] = act_map\n",
        "                output_img[2*grd_dep_height + 2*spacing:, ...] = overlapped_img\n",
        "                cv2.imwrite(os.path.join(save_dir, img_id + '/grd_dep.jpg'), output_img)\n",
        "\n",
        "\n",
        "        if model.branch_sat_seg != None:\n",
        "            sat_seg_imgs = batch[4]['imgs']\n",
        "            if use_gpu:\n",
        "                sat_seg_imgs = sat_seg_imgs.cuda()\n",
        "\n",
        "            sat_seg_output = model.branch_sat_seg(sat_seg_imgs, featuremaps=True)\n",
        "\n",
        "            sat_seg_output = (sat_seg_output**2).sum(1)\n",
        "            b, h, w = sat_seg_output.size()\n",
        "            sat_seg_output = sat_seg_output.view(b, h * w)\n",
        "            sat_seg_output = nn.functional.normalize(sat_seg_output, p=2, dim=1)\n",
        "            sat_seg_output = sat_seg_output.view(b, h, w)\n",
        "\n",
        "            if use_gpu:\n",
        "                sat_seg_imgs, sat_seg_output = sat_seg_imgs.cpu(), sat_seg_output.cpu()\n",
        "\n",
        "            for index in range(sat_seg_output.size(0)):\n",
        "\n",
        "                # get image name\n",
        "                img_id = str(int(sat_ids[index])).zfill(7)\n",
        "\n",
        "                # RGB image (input image)\n",
        "                input_img = sat_seg_imgs[index, ...]\n",
        "                for img, mean, std in zip(input_img, sat_seg_mean, sat_seg_std):\n",
        "                    img.mul_(std).add_(mean).clamp_(0, 1)\n",
        "                input_img = np.uint8(np.floor(input_img.numpy() * 255))\n",
        "                input_img = input_img.transpose((1, 2, 0))\n",
        "\n",
        "                # activation map\n",
        "                act_map = sat_seg_output[index, ...].numpy()\n",
        "                act_map = cv2.resize(act_map, (sat_seg_width, sat_seg_height))\n",
        "                act_map = 255 * (act_map - np.min(act_map)) / (np.max(act_map) - np.min(act_map) + 1e-12)\n",
        "                act_map = np.uint8(np.floor(act_map))\n",
        "                act_map = cv2.applyColorMap(act_map, cv2.COLORMAP_JET)\n",
        "\n",
        "                # overlapped image\n",
        "                overlapped_img = input_img*(1-fading) + act_map*(fading)\n",
        "                overlapped_img[overlapped_img > 255] = 255\n",
        "                overlapped_img = overlapped_img.astype(np.uint8)\n",
        "\n",
        "                # save images in a single figure (add white spacing between images)\n",
        "                output_img = 255 * np.ones((3*sat_seg_height + 2*spacing, sat_seg_width, 3), dtype=np.uint8)\n",
        "                output_img[:sat_seg_height, ...] = input_img[..., ::-1]\n",
        "                output_img[sat_seg_height + spacing:2*sat_seg_height + spacing, ...] = act_map\n",
        "                output_img[2*sat_seg_height + 2*spacing:, ...] = overlapped_img\n",
        "                cv2.imwrite(os.path.join(save_dir, img_id + '/sat_seg.jpg'), output_img)\n",
        "\n",
        "\n",
        "        # Restore the original mode of the model\n",
        "        if was_training:\n",
        "          model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRMabKiQQffN"
      },
      "outputs": [],
      "source": [
        "# @title Visualize Ranked Results\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_ranked_results(\n",
        "    model,\n",
        "    data_module,\n",
        "    split,\n",
        "    save_dir,\n",
        "    top_k = 5,\n",
        "    inverse = False,\n",
        "    use_gpu = True\n",
        "):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    spacing = 10\n",
        "    query_spacing = 30\n",
        "    border = 5\n",
        "    text_space = 30\n",
        "\n",
        "    # select dataloader\n",
        "    if split == 'train':\n",
        "        data_loader = data_module.train_dataloader()\n",
        "    elif split == 'test':\n",
        "        data_loader = data_module.test_dataloader()\n",
        "    elif split == 'val':\n",
        "        data_loader = data_module.val_dataloader()\n",
        "    else:\n",
        "        raise ValueError('split should be \"train\", \"test\" or \"val\"')\n",
        "\n",
        "    # (using data module dimensions)\n",
        "    grd_height, grd_width = data_module.original_size['grd']\n",
        "    sat_height, sat_width = data_module.original_size['sat']\n",
        "\n",
        "    grd_ids = np.empty((0))\n",
        "    sat_ids = np.empty((0))\n",
        "\n",
        "    model.grd_features_train.clear()\n",
        "    model.sat_features_train.clear()\n",
        "\n",
        "    model.grd_features_val.clear()\n",
        "    model.sat_features_val.clear()\n",
        "\n",
        "    model.grd_features_test.clear()\n",
        "    model.sat_features_test.clear()\n",
        "\n",
        "    # compute features for each image\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "\n",
        "        grd_ids = np.concatenate((grd_ids, batch[0]['imgs_id']))\n",
        "        sat_ids = np.concatenate((sat_ids, batch[0]['imgs_id']))\n",
        "\n",
        "        if use_gpu:\n",
        "            batch[0]['imgs'] = batch[0]['imgs'].cuda()\n",
        "            batch[1]['imgs'] = batch[1]['imgs'].cuda()\n",
        "            batch[2]['imgs'] = batch[2]['imgs'].cuda()\n",
        "            batch[3]['imgs'] = batch[3]['imgs'].cuda()\n",
        "            batch[4]['imgs'] = batch[4]['imgs'].cuda()\n",
        "\n",
        "        model.test_step(batch, batch_idx)\n",
        "\n",
        "        if use_gpu:\n",
        "            batch[0]['imgs'] = batch[0]['imgs'].cpu()\n",
        "            batch[1]['imgs'] = batch[1]['imgs'].cpu()\n",
        "            batch[2]['imgs'] = batch[2]['imgs'].cpu()\n",
        "            batch[3]['imgs'] = batch[3]['imgs'].cuda()\n",
        "            batch[4]['imgs'] = batch[4]['imgs'].cuda()\n",
        "\n",
        "    grd_features = torch.cat(model.grd_features_test, dim = 0)\n",
        "    sat_features = torch.cat(model.sat_features_test, dim = 0)\n",
        "\n",
        "    if use_gpu:\n",
        "        grd_features = grd_features.cpu()\n",
        "        sat_features = sat_features.cpu()\n",
        "\n",
        "    num_samples = grd_features.shape[0]\n",
        "\n",
        "    model.grd_features_test.clear()\n",
        "    model.sat_features_test.clear()\n",
        "\n",
        "    # compute distance matrix\n",
        "    grd_features = F.normalize(grd_features, dim=-1)\n",
        "    sat_features = F.normalize(sat_features, dim=-1)\n",
        "\n",
        "\n",
        "    if not inverse:\n",
        "\n",
        "        dist_matrix = 1 - grd_features @ sat_features.T\n",
        "        indices = np.argsort(dist_matrix, axis=1)\n",
        "\n",
        "        for grd_index in range(num_samples):\n",
        "\n",
        "            # create empty output image\n",
        "            output_height = grd_height + query_spacing + sat_height + text_space\n",
        "            output_width = max(grd_width, top_k*sat_width + (top_k-1)*spacing)\n",
        "            output_img = 255 * np.ones((output_height, output_width, 3), dtype=np.uint8)\n",
        "\n",
        "            # create query image with black border\n",
        "            grd_id = str(int(grd_ids[grd_index])).zfill(7)\n",
        "            grd_path = data_module.data_dir + '/streetview/' + grd_id + '.jpg'\n",
        "            color = (0,0,0)\n",
        "            grd_img = cv2.imread(grd_path)\n",
        "            grd_img = cv2.resize(grd_img, (grd_width, grd_height))\n",
        "            grd_img = cv2.copyMakeBorder(grd_img, border, border, border, border, cv2.BORDER_CONSTANT, value = color)\n",
        "            grd_img = cv2.resize(grd_img, (grd_width, grd_height))\n",
        "\n",
        "            # add query image to the output\n",
        "            start_width = (output_width - grd_width) // 2\n",
        "            end_width = start_width + grd_width\n",
        "            output_img[:grd_height, start_width:end_width, :] = grd_img\n",
        "\n",
        "            rank = 1\n",
        "            for sat_index in indices[grd_index, :]:\n",
        "\n",
        "                # create ranked image with red or green border\n",
        "                sat_id = str(int(sat_ids[sat_index])).zfill(7)\n",
        "                folder = 'polarmap/normal' if data_module.polar else 'bingmap'\n",
        "                sat_path = data_module.data_dir + '/' + folder + '/input' + sat_id + '.png'\n",
        "                color = (0, 255, 0) if (grd_id == sat_id) else (0, 0, 255)\n",
        "                sat_img = cv2.imread(sat_path)\n",
        "                sat_img = cv2.resize(sat_img, (sat_width, sat_height))\n",
        "                sat_img = cv2.copyMakeBorder(sat_img, border, border, border, border, cv2.BORDER_CONSTANT, value = color)\n",
        "                sat_img = cv2.resize(sat_img, (sat_width, sat_height))\n",
        "\n",
        "                # add ranked image to the output\n",
        "                start_height = grd_height+query_spacing\n",
        "                end_height = start_height + sat_height\n",
        "                start_width = (rank-1) * (sat_width + spacing)\n",
        "                end_width = start_width + sat_width\n",
        "                output_img[start_height:end_height, start_width:end_width, :] = sat_img\n",
        "\n",
        "                # add text about the ranked image\n",
        "                text = \"Rank: {} Distance:{:.4f}\".format(rank, dist_matrix[grd_index, sat_index])\n",
        "                bottom_left = (start_width + 10, output_height - 5)\n",
        "                cv2.putText(output_img, text, bottom_left, cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 1, 2)\n",
        "\n",
        "                rank += 1\n",
        "                if rank > top_k:\n",
        "                    break\n",
        "\n",
        "            # create output image file\n",
        "            cv2.imwrite(os.path.join(save_dir, grd_id + '_visrank.jpg'), output_img)\n",
        "\n",
        "            if was_training:\n",
        "              model.train()\n",
        "\n",
        "\n",
        "    if inverse:\n",
        "\n",
        "        dist_matrix = 1 - sat_features @ grd_features.T\n",
        "        indices = np.argsort(dist_matrix, axis=1)\n",
        "\n",
        "        for sat_index in range(num_samples):\n",
        "\n",
        "            # create empty output image\n",
        "            output_height = sat_height + query_spacing + top_k*(grd_height + text_space+spacing)\n",
        "            output_width = max(sat_width, grd_width)\n",
        "            output_img = 255 * np.ones((output_height, output_width, 3), dtype=np.uint8)\n",
        "\n",
        "            # create query image with black border\n",
        "            sat_id = str(int(sat_ids[sat_index])).zfill(7)\n",
        "            folder = 'polarmap/normal' if data_module.polar else 'bingmap'\n",
        "            sat_path = data_module.data_dir + '/' + folder + '/input' + sat_id + '.png'\n",
        "            color = (0,0,0)\n",
        "            sat_img = cv2.imread(sat_path)\n",
        "            sat_img = cv2.resize(sat_img, (sat_width, sat_height))\n",
        "            sat_img = cv2.copyMakeBorder(sat_img, border, border, border, border, cv2.BORDER_CONSTANT, value = color)\n",
        "            sat_img = cv2.resize(sat_img, (sat_width, sat_height))\n",
        "\n",
        "            # add query image to the output\n",
        "            start_width = (output_width - sat_width) // 2\n",
        "            end_width = start_width + sat_width\n",
        "            output_img[:sat_height, start_width:end_width, :] = sat_img\n",
        "\n",
        "            rank = 1\n",
        "            for grd_index in indices[sat_index, :]:\n",
        "\n",
        "                # create ranked image with red or green border\n",
        "                grd_id = str(int(grd_ids[grd_index])).zfill(7)\n",
        "                grd_path = data_module.data_dir + '/streetview/' + grd_id + '.jpg'\n",
        "                color = (0, 255, 0) if (sat_id == grd_id) else (0, 0, 255)\n",
        "                grd_img = cv2.imread(grd_path)\n",
        "                grd_img = cv2.resize(grd_img, (grd_width, grd_height))\n",
        "                grd_img = cv2.copyMakeBorder(grd_img, border, border, border, border, cv2.BORDER_CONSTANT, value = color)\n",
        "                grd_img = cv2.resize(grd_img, (grd_width, grd_height))\n",
        "\n",
        "                # add ranked image to the output\n",
        "                start_height = sat_height + query_spacing + (rank-1) * (grd_height+text_space+spacing)\n",
        "                end_height = start_height + grd_height\n",
        "                start_width = (output_width - grd_width) // 2\n",
        "                end_width = start_width + grd_width\n",
        "                output_img[start_height:end_height, start_width:end_width, :] = grd_img\n",
        "\n",
        "                # add text about the ranked image\n",
        "                text = \"Rank: {} Distance:{:.4f}\".format(rank, dist_matrix[sat_index, grd_index])\n",
        "                bottom_left = (start_width + 10, end_height + 25)\n",
        "                cv2.putText(output_img, text, bottom_left, cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 1, 2)\n",
        "\n",
        "                rank += 1\n",
        "                if rank > top_k:\n",
        "                    break\n",
        "\n",
        "            # create output image file\n",
        "            cv2.imwrite(os.path.join(save_dir, grd_id + '_inverse_visrank.jpg'), output_img)\n",
        "\n",
        "            if was_training:\n",
        "              model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dSp6VMiKNAa"
      },
      "outputs": [],
      "source": [
        "!rm -f -r '/content/output/log/activation_maps'\n",
        "!mkdir '/content/output/log/activation_maps'\n",
        "\n",
        "create_activation_maps(\n",
        "    model = model.to(device),\n",
        "    data_module = data_module,\n",
        "    split = 'val',\n",
        "    save_dir = '/content/output/log/activation_maps',\n",
        "    use_gpu = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buh9sdrgGU-B"
      },
      "outputs": [],
      "source": [
        "!rm -f -r '/content/output/log/ranked_results'\n",
        "!mkdir '/content/output/log/ranked_results'\n",
        "\n",
        "visualize_ranked_results(\n",
        "    model = model.to(device),\n",
        "    data_module = data_module,\n",
        "    split = 'val',\n",
        "    save_dir = '/content/output/log/ranked_results',\n",
        "    top_k = 5,\n",
        "    inverse = False,\n",
        "    use_gpu = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvaU8HmOln47"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pi5yzhh_RW7B",
        "y1FRSut8m-Mv",
        "5VmODtjKnRzH",
        "rXEBXiWchDAq",
        "wNh1Gez5-j8h",
        "RouQUwE2nWua",
        "-eQpKOJZarI6",
        "ZGDT8x2WPpbC",
        "h9YaK9QS2VN5"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}